 1/1: hotels.shape()[0]
 1/2: import pandas as pd
 1/3: import pandas as pd
 1/4: print("Hello World")
 1/5: !pip install pandas
 1/6: import pandas as pd
 1/7: hotels = pd.read_csv("hotel_booking_data.csv")
 1/8: hotels.head()
 1/9: hotels.shape()[0]
1/10: hotels.shape[0]
1/11: hotels.isna()
1/12: hotels.isna().sum().sum()
1/13: hotels.isna().sum()
1/14:
hotels.drop(axis=1, labels="company")
hotels.head()
1/15: hotels.groupby(by="country").agg("count")
1/16: hotels.groupby(by="country").agg({"hotel": "count"}).sort_values("hotel", ascending=False).iloc[:5]
1/17: hotels.columns()
1/18: hotels.columns
1/19: hotels.sort_values('adr', ascending=False).iloc[0]
1/20: hotels.sort_values('adr', ascending=False).iloc[0]["name", "adr"]
1/21: hotels.sort_values('adr', ascending=False).iloc[0].loc["name", "adr"]
1/22: hotels.sort_values('adr', ascending=False).iloc[0].loc["name"]
1/23: hotels.sort_values('adr', ascending=False).iloc[0].loc[["name", "adr"]]
1/24: hotels["adr"].mean()
1/25: round(hotels["adr"].mean(), 2)
1/26: round((hotels["stays_in_week_nights"] + hotels["stays_in_weekend_nights"]).mean(), 2)
1/27: round((hotels["adr"]*(hotels["stays_in_week_nights"] +hotels["stays_in_weekend_nights"]).mean(), 2)
1/28: round((hotels["adr"]*(hotels["stays_in_week_nights"] +hotels["stays_in_weekend_nights"])).mean(), 2)
1/29: hotels["name"][hotels["total_of_special_requests"] == 5]
1/30: hotels["is_repeated_guest"]
1/31: hotels["is_repeated_guest"][hotels["is_repeated_guest"] != 0]
1/32: hotels["is_repeated_guest"][hotels["is_repeated_guest"] != 0].sort_values{ascending=False}
1/33: hotels["is_repeated_guest"][hotels["is_repeated_guest"] != 0].sort_values(ascending=False)
1/34: hotels["is_repeated_guest"][hotels["is_repeated_guest"] != 0].count()/hotels["is_repeated_guest"].count()
1/35: round(hotels["is_repeated_guest"][hotels["is_repeated_guest"] != 0].count()/hotels["is_repeated_guest"].count()*100, 2)
1/36:
hotels["last_name"] = hotels["name"].str.split()[-1]
hotels["last_name"]
1/37:
hotels["last_name"] = hotels["name"].str.split()([-1])
hotels["last_name"]
1/38:
hotels["last_name"] = hotels["name"].str.split().str[-1]
hotels["last_name"]
1/39:
hotels["last_name"] = hotels["name"].str.split().str[-1]
hotels.groupby(by="last_name").agg("count")
1/40:
hotels["last_name"] = hotels["name"].str.split().str[-1]
hotels.groupby(by="last_name").agg({"name": "count"})
1/41:
hotels["last_name"] = hotels["name"].str.split().str[-1]
hotels.groupby(by="last_name").agg({"name": "count"}).sort_values(ascending=False)
1/42:
hotels["last_name"] = hotels["name"].str.split().str[-1]
hotels.groupby(by="last_name").agg({"name": "count"}).sort_values(by="name", ascending=False)
1/43:
hotels = hotels[hotels["is_cancelled"] == 1]
hotels
1/44:
hotels = hotels[hotels["is_canceled"] == 1]
hotels
1/45:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
non_canceled_bookings.sort_values("children", ascending=False).iloc[:5]["name"]
1/46:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
non_canceled_bookings.sort_values("children", ascending=False)["name"]
1/47:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
non_canceled_bookings.sort_values("children", ascending=False)
1/48:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
#non_canceled_bookings.sort_values("children", ascending=False)
non_canceled_bookings
1/49:
#non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
#non_canceled_bookings.sort_values("children", ascending=False)
#non_canceled_bookings
hotels["is_canceled"].describe()
1/50:
#non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
#non_canceled_bookings.sort_values("children", ascending=False)
#non_canceled_bookings
hotels
1/51: hotels = pd.read_csv("hotel_booking_data.csv")
1/52:
#non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index()
#non_canceled_bookings.sort_values("children", ascending=False)
#non_canceled_bookings
hotels
1/53:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].reset_index().sort_values("children", ascending=False)
non_canceled_bookings
1/54:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)["children"]
non_canceled_bookings
1/55:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:3].loc[["name", "children"]]
1/56:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:3]
1/57:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5]["name", "children"]
1/58:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5]
1/59:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5]["children"]
1/60:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5]["name"]
1/61:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5].loc["name", "children"]
1/62:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5].loc["name"]
1/63:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5]
1/64:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5][["name", "email"]]
1/65:
non_canceled_bookings = hotels[hotels["is_canceled"] == 0].sort_values("children", ascending=False)
non_canceled_bookings.iloc[:5][["name", "children"]]
1/66: hotels[hotels['name'] == "Jamie Ramirez"]
1/67: hotels[hotels['name'] == "Jamie Ramirez"]["children"]
1/68: hotels[hotels['name'] == "Jamie Ramirez"]["is_canceled"]
1/69:
hotels["area_codes"] = hotels["phone_number"].str[:3]
hotels["area_codes"]
1/70:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels["area_codes"]
1/71:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name", "count"}).sort_values("name", ascending=False).iloc[:10]
1/72:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels["area_codes"]
#hotels.groupby(by="area_codes").agg({"name", "count"}).sort_values("name", ascending=False).iloc[:10]
1/73:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name", "count"}).sort_values("name", ascending=False).iloc[:10]
1/74:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name", "count"}).sort_values(ascending=False).iloc[:10]
1/75:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name", "count"})
1/76:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes")
1/77:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg("count")
1/78:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name": "count")
1/79:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name": "count"})
1/80:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name": "count"}).sort_values(ascending=False)
1/81:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name": "count"}).sort_values(by="name", ascending=False)
1/82:
hotels["area_codes"] = hotels["phone-number"].str[:3]
hotels.groupby(by="area_codes").agg({"name": "count"}).sort_values(by="name", ascending=False).iloc[:10]
1/83: hotels.tail()
1/84: hotels[pd.to_datetime(hotels["arrival_date_day_of_month"]).day != 0)
1/85: hotels[pd.to_datetime(hotels["arrival_date_day_of_month"]).day != 0]
1/86: hotels.describe()
1/87: hotels.dtypes()
1/88: hotels.dtypes
1/89: hotels[hotels["arrival_date_day_of_month"] <= 15].count()
1/90: hotels[hotels["arrival_date_day_of_month"] <= 15]["name"].count()
1/91: hotels[["arrival_date_year", "arrival_date_month"]]
1/92: hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]]
1/93: pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]]
1/94: pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day")]
1/95: pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day")])
1/96: pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day")]])
1/97: pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}]])
1/98: pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/99:
hotels["arrival_date_month"] = hotels["arrival_date_month"].dt.strptime("%B").dt.month
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/100:
hotels["arrival_date_month"] = pd.to_date_time(hotels["arrival_date_month"]).dt.strptime("%B").astype(datetime).dt.month
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/101:
hotels["arrival_date_month"] = pd.to_datetime(hotels["arrival_date_month"]).dt.strptime("%B").astype(datetime).dt.month
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/102:
hotels["arrival_date_month"] = hotels["arrival_date_month"].strptime("%B").astype(datetime).dt.month
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/103:
hotels["arrival_date_month"] = hotels["arrival_date_month"].strftime("%B").dt.strptime("%B").astype(datetime).dt.month
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/104:
hotels["arrival_date_month"] = hotels["arrival_date_month"].dt.strftime("%B").dt.strptime("%B").astype(datetime).dt.month
#pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/105:
hotels["arrival_date_month"] = hotels["arrival_date_month"].str.strftime("%B").dt.strptime("%B").astype(datetime).dt.month
#pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/106:
import pandas as pd
from datetime import datetime
1/107:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B") for month in hotels["arrival_date_month"]]
#pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/108:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B") for month in hotels["arrival_date_month"]]
hotels["arrival_date_month"]
#pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/109:
hotels["arrival_date_month"] = [month.strftime("%m") for month in hotels["arrival_date_month"]]
hotels["arrival_date_month"]
#pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/110:
hotels["arrival_date_month"] = [month.strftime("%m") for month in hotels["arrival_date_month"]]
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/111:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B").strftime("%m") for month in hotels["arrival_date_month"]]
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/112: hotels = pd.read_csv("hotel_booking_data.csv")
1/113:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B").strftime("%m") for month in hotels["arrival_date_month"]]
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/114:
import pandas as pd
from datetime import datetime
1/115: hotels = pd.read_csv("hotel_booking_data.csv")
1/116:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B").strftime("%m") for month in hotels["arrival_date_month"]]
pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"}))
1/117: hotels = pd.read_csv("hotel_booking_data.csv")
1/118:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B").strftime("%m") for month in hotels["arrival_date_month"]]
hotels["day_of_the_week"] = pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"})).strftime("%a")
1/119:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B").strftime("%m") for month in hotels["arrival_date_month"]]
hotels["day_of_the_week"] = pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"})).dt.strftime("%a")
1/120: hotels = pd.read_csv("hotel_booking_data.csv")
1/121:
hotels["arrival_date_month"] = [datetime.strptime(month, "%B").strftime("%m") for month in hotels["arrival_date_month"]]
hotels["day_of_the_week"] = pd.to_datetime(hotels[["arrival_date_year", "arrival_date_month", "arrival_date_day_of_month"]].rename(columns={"arrival_date_year": "year", "arrival_date_month": "month", "arrival_date_day_of_month": "day"})).dt.strftime("%a")
1/122: hotels.grouby("day_of_the_week").agg({"name": "count"})
1/123: hotels.groupby("day_of_the_week").agg({"name": "count"})
 2/1:
# COMMON MISTAKE!
# DON'T FORGET THE .PYPLOT part

import matplotlib.pyplot as plt
 2/2: !pip install matplotlib
 2/3:
# COMMON MISTAKE!
# DON'T FORGET THE .PYPLOT part

import matplotlib.pyplot as plt
 7/1:
# Question 1 - Counting DNA Nucleotides
string = "AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC"
print(list(string).count('A'), list(string).count("C"), list(string).count('G'), list(string).count('T'))
 7/2:
# # Question 1 - Counting DNA Nucleotides
# string = "AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC"
# print(list(string).count('A'), list(string).count("C"), list(string).count('G'), list(string).count('T'))
 7/3:
# Question 2 - Transcribing DNA into RNA
string = "CGGCAAAGTGCATAAACCAGCATATTAGCTTTACCTCGGCACACCTTGCACGGCATTCGGCAATTCTTATTTGTTCGCGCCCGTGGTATATTGTAGTTAGTGCTGTGCCTAAGTATCGGGATGCGGCGCGTTGCCTATCGCCCTGGGAGATTTAAGCTGAGGTGTGCCGCTAAAGTAGCGTAACGTCTATCCTGACTAGGGTCATGGCTAAAGGTTCCTGGCGGATGATTTAACATTGTAATTGATTGATCTGGGGCACGTGGTAGGAGGCGTATGTGTCGAAACCGATTTCACAGGTAAGTCTCCTTACGTGGTCGCCCCGTGTGAACATAAATAGCCCATATAACACGTGGTCCGATGCTTTTCCTTGAGTGTCGTAGGGTGAGTCGCCCCCGGAATCCATGTTGCACGGGAAGTTACGAGTGCCTCATGAGTACTCTCTCCGTAGCCGAGGTCCTGCCCGCTTAATAATCGTCAACACAACGCCGTATGTCGACCATGTCGAGGTCCGCCACAGTATTAACGTAATTTCACAGGACCCGAGATACTTACGCGTACTTCATAAGGTCGATTGTGTCAAGCAGGTGCGTACCTGAATTATCCCACCTTTATTGCAATCGCATTGGCGATACGAATTCCTTCAAGAAGTCAGTGTTTAATAGGACGTCGCACTAGGTACGCGGGAAGACGCGTCCCAAACCTCATCATACATTAATTCAGGGGATCAACGATGACGGGTGTCGTGCCGGCCCCTGTCCCTCTAAACTATTCGACCATCGCTTCAACTCAGCAATCTTGGACCCATGGAGTCACCGCTGTGGGCAGCAGAAGTCGGGTAGTGAAGTGAAGTCGAGCACTACGTCTCGCCGGGCAGCAAGCTGATACATAAACCCGATGTAGTCATCTGCGCCAAGCCGCGATCGGTCTCAGCCTAGTTAGATTCCGT"
print(string.replace("T", "U"))
 7/4:
# Question 5 - Computing GC Content
# https://rosalind.info/problems/gc/
with open("data_strings", "r") as file:
    string = "".join([line.strip() for line in file.readlines()])
dictionary_of_strands = {strand: strand.split("_")[1][4:] for strand in string.split(">")[1:]}
list_of_strands = [(strand.count("G") + strand.count("C"))/len(strand) for data, strand in dictionary_of_strands.items()]
index = list_of_strands.index(max(list_of_strands))
print(list(dictionary_of_strands.keys())[index][:13])
print(100*max(list_of_strands))
 7/5:
# Question 5 - Computing GC Content
# https://rosalind.info/problems/gc/
with open("data_strings", "r") as file:
    string = "".join([line.strip() for line in file.readlines()])
dictionary_of_strands = {strand: strand.split("_")[1][4:] for strand in string.split(">")[1:]}
list_of_strands = [(strand.count("G") + strand.count("C"))/len(strand) for data, strand in dictionary_of_strands.items()]
index = list_of_strands.index(max(list_of_strands))
print(list(dictionary_of_strands.keys())[index][:13])
print(100*max(list_of_strands))
 7/6:
# Question 5 - Computing GC Content
# https://rosalind.info/problems/gc/
with open("data_strings", "r") as file:
    string = "".join([line.strip() for line in file.readlines()])
dictionary_of_strands = {strand: strand.split("_")[1][4:] for strand in string.split(">")[1:]}
list_of_strands = [(strand.count("G") + strand.count("C"))/len(strand) for data, strand in dictionary_of_strands.items()]
index = list_of_strands.index(max(list_of_strands))
print(list(dictionary_of_strands.keys())[index][:13])
print(100*max(list_of_strands))
 7/7:
# Question 10 - Consensus Strings and Profiling
# https://rosalind.info/problems/cons/
import pandas as pd

with open("data_strings", "r") as file:
    string = "".join(line.strip() for line in file.readlines())

string_list = string.split(">")[1:]
strand_length = len(string_list[0]) - 10
df = pd.DataFrame([list(strand[-strand_length:]) for strand in string_list])
new_df = pd.DataFrame({base: [df[column].to_list().count(base) for column in df.columns] for base in ["A", "C", "G", "T"]})
new_df = new_df.transpose()
print("".join([new_df[column].idxmax() for column in new_df.columns][3:]))
for row in list(new_df.index):
    print(f"{row}: " + " ".join([str(df[column].to_list().count(row)) for column in df.columns[3:]]))
10/1:
# Question 54 - Weighted Newick Format
# https://rosalind.info/problems/nkew/
with open("data_strings", "r") as file:
    data = [line.strip() for line in file.readlines()]
indices = [-1] + [i for i in range(len(data)) if data[i] == ""]
indices += [len(data)]
trees = [data[indices[i] + 1:indices[i+1]] for i in range(0, len(indices) - 1)]
path_list = []

for tree in trees:
    xk, yk = tuple(tree[-1].split())
    if xk == yk:
        print(0)
        continue
    tree_data = tree[0]
    x_index = tree_data.find(xk) + len(xk)
    i = 1
    x_value_string = ""
    while tree_data[x_index + i].isdigit():
        x_value_string += tree_data[x_index + i]
        i += 1
    x_value = int(x_value_string)
    y_index = tree_data.find(yk) + len(yk)
    i = 1
    y_value_string = ""
    while tree_data[y_index + i].isdigit():
        y_value_string += tree_data[y_index + i]
        i += 1
    y_value = int(y_value_string)
    path = x_value + y_value
    for index in [index for index in range(max(x_index, y_index)) if tree_data[index] == "("]:
        closing_index = min([i for i in range(index + 1, len(tree_data)) if
                             tree_data[i] == ")" and tree_data[index:i + 1].count("(") == tree_data[index:i + 1].count(
                                 ")")])
        if (index < x_index < closing_index) and not (index < y_index < closing_index):
            index_value_string = ""
            i = closing_index + 1
            while not tree_data[i].isdigit():
                i += 1
            while tree_data[i].isdigit():
                index_value_string += tree_data[i]
                i += 1
            path += int(index_value_string)
        elif (index < y_index < closing_index) and not (index < x_index < closing_index):
            index_value_string = ""
            i = closing_index + 1
            while not tree_data[i].isdigit():
                i += 1
            while tree_data[i].isdigit():
                index_value_string += tree_data[i]
                i += 1
            path += int(index_value_string)
    path_list.append(path)

print(" ".join([str(path) for path in path_list]))
14/1:
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*(c^2)
m, E
14/2:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*(c^2)
m, E
14/3:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*(c^2)
m
14/4:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*(c^2)
print("[ " + ",".join([str(int(element)) for element in m]) + " ]")
14/5:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*(c^2)
print("[ " + ", ".join([str(int(element)) for element in m]) + " ]")
14/6:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*(c^2)
print("[ " + ", ".join([str(int(element)) for element in m]) + " ]")
print("[ " + ", ".join([str(int(element)) for element in E]) + " ]")
14/7:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10^8)
E = m*c
print("[ " + ", ".join([str(int(element)) for element in m]) + " ]")
print("[ " + ", ".join([str(int(element)) for element in E]) + " ]")
14/8:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10**8)
E = m*(c**2)
print("[ " + ", ".join([str(int(element)) for element in m]) + " ]")
print("[ " + ", ".join([str(int(element)) for element in E]) + " ]")
14/9:
import numpy as np
m = np.linspace(0, 10, 11)
c = 3*(10**8)
E = m*(c**2)
print("[ " + ", ".join([str(int(element)) for element in m]) + " ]")
E
14/10: from matplotlib import pyplot as plt
14/11:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E)
fig.show()
14/12:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E)
# fig.show()
15/1: import numpy as np
15/2: x = np.arange(0,10)
15/3: y = 2*x
15/4: x
15/5: y
15/6:
plt.plot(x, y) 
plt.xlabel('X Axis Title Here')
plt.ylabel('Y Axis Title Here')
plt.title('String Title Here')
plt.show() # Required for non-jupyter users , but also removes Out[] info
15/7:
from matplotlib import pyplot as plt
plt.plot(x, y) 
plt.xlabel('X Axis Title Here')
plt.ylabel('Y Axis Title Here')
plt.title('String Title Here')
plt.show() # Required for non-jupyter users , but also removes Out[] info
14/13:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E)
fig.show()
15/8:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.xlabel('X Axis Title Here')
ax.ylabel('Y Axis Title Here')
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
15/9:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.set_xlabel('X Axis Title Here')
ax.ylabel('Y Axis Title Here')
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
15/10:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.set_xlabel('X Axis Title Here')
ax.set_ylabel('Y Axis Title Here')
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
15/11:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.set_xlabel('X Axis Title Here')
ax.set_ylabel('Y Axis Title Here')
ax.show()
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
15/12:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.set_xlabel('X Axis Title Here')
ax.set_ylabel('Y Axis Title Here')
fig.show()
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
15/13:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.set_xlabel('X Axis Title Here')
ax.set_ylabel('Y Axis Title Here')
plt.show()
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
15/14:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.plot(x, y)
ax.set_xlabel('X Axis Title Here')
ax.set_ylabel('Y Axis Title Here')
ax.set_title('Title here')
plt.show()
# plt.plot(x, y) 
# plt.xlabel('X Axis Title Here')
# plt.ylabel('Y Axis Title Here')
# plt.title('String Title Here')
# plt.show() # Required for non-jupyter users , but also removes Out[] info
14/14:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E)
plt.show()
14/15:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E)
ax.set_xlabel('Mass in grams')
ax.set_ylabel('Energy in joules')
plt.show()
14/16:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in joules')
ax.set_title('E=mc^2')
plt.show()
14/17:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=2.0, color='red')
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in joules')
ax.set_title('E=mc^2')
plt.
plt.show()
14/18:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=2.0, color='red')
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in joules')
ax.set_title('E=mc^2')

plt.show()
14/19:
from matplotlib import pyplot as plt
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5.0, color='red')
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in joules')
ax.set_title('E=mc^2')
ax.set_xlim(0, 10)
plt.show()
14/20:
# CODE HERE
fig, ax = plt.subplots(1,1)
14/21:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plt(m, E, lw=5, color='red')
14/22:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.yscale('log')
14/23:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
14/24:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(tight=True, nbins=4)
14/25:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(tight=True)
14/26:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(tight=True, nbins= 10)
14/27:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 10)
ax.set_xlabel('Mass in Grams')
14/28:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
plt.show()
14/29:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in Joules')
plt.grid(color='b',ls='-', axis='y')
plt.show()
14/30:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in Joules')
plt.grid(ls='.', axis='y')
plt.show()
14/31:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in Joules')
plt.grid(ls='-.', axis='y')
plt.show()
14/32:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in Joules')
plt.grid(ls='-.', axis='y', which='both')
plt.show()
14/33:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in Joules')
plt.grid(ls='-', axis='y', which='both')
plt.show()
14/34:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(m, E, lw=5, color='red')
ax.set_xlim(0, 10)
ax.set_yscale('log')
ax.locator_params(axis='x', tight=True, nbins= 5)
ax.set_xlabel('Mass in Grams')
ax.set_ylabel('Energy in Joules')
plt.grid(ls='-', axis='y', which='both')
ax.set_title('E=mc^2')
plt.show()
14/35:
labels = ['1 Mo','3 Mo','6 Mo','1 Yr','2 Yr','3 Yr','5 Yr','7 Yr','10 Yr','20 Yr','30 Yr']

july16_2007 =[4.75,4.98,5.08,5.01,4.89,4.89,4.95,4.99,5.05,5.21,5.14]
july16_2020 = [0.12,0.11,0.13,0.14,0.16,0.17,0.28,0.46,0.62,1.09,1.31]
14/36:
labels = ['1 Mo','3 Mo','6 Mo','1 Yr','2 Yr','3 Yr','5 Yr','7 Yr','10 Yr','20 Yr','30 Yr']

july16_2007 =[4.75,4.98,5.08,5.01,4.89,4.89,4.95,4.99,5.05,5.21,5.14]
july16_2020 = [0.12,0.11,0.13,0.14,0.16,0.17,0.28,0.46,0.62,1.09,1.31]
14/37:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(labels, july16_2007)
ax.plot(labels, july16_2020)
14/38:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(labels, july16_2007)
ax.plot(labels, july16_2020)
plt.show()
14/39:
# CODE HERE
fig, ax = plt.subplots(1,1)
ax.plot(labels, july16_2007)
ax.plot(labels, july16_2020)
ax.legend()
plt.show()
14/40:
# CODE HERE
fig, ax = plt.subplots(1,1)
line1, = ax.plot(labels, july16_2007)
line2, = ax.plot(labels, july16_2020)
ax.legend([line1, line2], ['july16_2007', 'july16_2020'])
plt.show()
14/41:
# CODE HERE
fig, ax = plt.subplots(1,1)
line1, = ax.plot(labels, july16_2007)
line2, = ax.plot(labels, july16_2020)
ax.legend([line1, line2], ['july16_2007', 'july16_2020'], loc=7)
plt.show()
14/42:
# CODE HERE
fig, ax = plt.subplots(1,1)
line1, = ax.plot(labels, july16_2007)
line2, = ax.plot(labels, july16_2020)
ax.legend([line1, line2], ['july16_2007', 'july16_2020'], loc=6)
plt.show()
14/43:
# CODE HERE
fig, ax = plt.subplots(1,1)
line1, = ax.plot(labels, july16_2007)
line2, = ax.plot(labels, july16_2020)
ax.legend([line1, line2], ['july16_2007', 'july16_2020'], loc=(1.2, 0.5))
plt.show()
14/44:
# CODE HERE
fig, ax = plt.subplots(1,1)
line1, = ax.plot(labels, july16_2007)
line2, = ax.plot(labels, july16_2020)
ax.legend([line1, line2], ['july16_2007', 'july16_2020'], loc=(1.01, 0.5))
plt.show()
14/45:
# CODE HERE
fig, ax = plt.subplots(1,1)
line1, = ax.plot(labels, july16_2007)
line2, = ax.plot(labels, july16_2020)
ax.legend([line1, line2], ['july16_2007', 'july16_2020'], loc=(1.15, 0.5))
plt.show()
14/46:
# CODE HERE
fig, ax = plt.subplots(2,1)
plt.show()
14/47:
# CODE HERE
fig, ax = plt.subplots(2,1)
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
plt.show()
14/48:
# CODE HERE
fig, ax = plt.subplots(2,1)
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
plt.show()
14/49:
# CODE HERE
fig, ax = plt.subplots(2,1)
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=5)
plt.show()
14/50:
# CODE HERE
fig, ax = plt.subplots(2,1)
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/51:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(10, 100))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/52:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(100, 10))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/53:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(600, 40))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/54:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(60, 40))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/55:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(60, 40))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/56:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(40, 40))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/57:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(40, 10))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/58:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(40, 20))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/59:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(20, 15))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/60:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(20, 7.5))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/61:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(5, 7.5))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/62:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(10, 7.5))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/63:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(7.5, 7.5))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/64:
# CODE HERE
fig, ax = plt.subplots(2,1, figsize=(7.5, 5))
line1, = ax[0].plot(labels, july16_2007)
line2, = ax[1].plot(labels, july16_2007)
ax[1].set_ylim(0.75*min(july16_2007), 1.2*max(july16_2007))
ax[1].set_title('July 16, 2007')
ax[0].set_title('July 16, 2020')
fig.tight_layout(pad=2)
plt.show()
14/65:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007)
ax2.plot(labels, july16_2020)
14/66:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax2.plot(labels, july16_2020, color='red')
14/67:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007')
ax2.plot(labels, july16_2020, color='red')
ax2.set_xlabel('2020')
plt.show()
14/68:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020')
plt.show()
14/69:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color]'darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
plt.show()
14/70:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
plt.show()
14/71:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
ax.spines['left'].set_color('darkblue')
plt.show()
14/72:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
ax.spines['bottom'].set_color('darkblue')
plt.show()
14/73:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')

plt.show()
14/74:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
ax.spines['top'].set_visible(False)
plt.show()
14/75:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
ax.spines['top'].set_visible(False)
ax2.spines['top'].set_visible(False)
plt.show()
14/76:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    ax.spines['left'].set_color('darkblue')
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
14/77:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    axis.spines['left'].set_color('darkblue')
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
14/78:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    axis.spines['left'].set_color('darkblue')
    axis.spines['right'].set_color('red')
    axis.spines['left'].set_linewidth(3)
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
14/79:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    axis.spines['left'].set_color('darkblue')
    axis.spines['right'].set_color('red')
    for spine in axis.spines.itervalues():
        spine.set_linewidth(2)
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
14/80:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    axis.spines['left'].set_color('darkblue')
    axis.spines['right'].set_color('red')
    for spine in axis.spines:
        spine.set_linewidth(2)
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
14/81:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    axis.spines['left'].set_color('darkblue')
    axis.spines['right'].set_color('red')
    for spine in axis.spines:
        spine.set_linewidth(2)
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
14/82:
# CODE HERE
fig, ax = plt.subplots()
ax2 = ax.twinx()
ax.plot(labels, july16_2007, color='darkblue')
ax.set_ylabel('2007', color='darkblue')
ax2.plot(labels, july16_2020, color='red')
ax2.set_ylabel('2020', color='red')
for axis in [ax, ax2]:
    axis.spines['left'].set_color('darkblue')
    axis.spines['right'].set_color('red')
    for key, spine in axis.spines.items():
        spine.set_linewidth(2)
# ax.spines['top'].set_visible(False)
# ax2.spines['top'].set_visible(False)
plt.show()
16/1:
# Basically, we want to figure out how to create this line
ax = sns.regplot(x='total_spend',y='sales',data=df)
16/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
16/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
16/4:
pip install seaborn
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
16/5:
pip install seaborn
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
16/6:
!pip install seaborn
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
16/7:
python3 -m pip install --upgrade pip
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
16/8:
!python3 -m pip install --upgrade pip
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
16/9:
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
16/10:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
16/11:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
16/12: df = pd.read_csv("Advertising.csv")
16/13: df.head()
16/14: df['total_spend'] = df['TV'] + df['radio'] + df['newspaper']
16/15: sns.scatterplot(x='total_spend',y='sales',data=df)
16/16:
# Basically, we want to figure out how to create this line
ax = sns.regplot(x='total_spend',y='sales',data=df)
16/17:
# Basically, we want to figure out how to create this line
ax = sns.regplot(x='total_spend',y='sales',data=df)
ax.get_lines()[0].get_xdata()
16/18:
# Basically, we want to figure out how to create this line
ax = sns.regplot(x='total_spend',y='sales',data=df)
slope = (ax.get_lines()[0].get_ydata()[1] - ax.get_lines()[0].get_ydata()[0])/(ax.get_lines()[0].get_xdata()[1] - ax.get_lines()[0].get_xdata()[0])
intercept = ax.get_lines()[0].get_ydata()[0] - slope*ax.get_lines()[0].get_xdata()[0]
slope = round(slope, 3)
intercept = round(intercept, 3)
slope, intercept
17/1:
import random
import numpy as np
17/2:
import random
import numpy as np
from matplotlib import pyplot as plt
17/3: means = np.random.normal(0, 1, (10,0))
17/4: means
17/5:
for mean in means:
    print(mean)
17/6: means = np.random.normal(0, 1, 10)
17/7: means
17/8:
probs = [np.random.normal(mean,1,1000) for mean in means]
probs
17/9:
import random
import numpy as np
import seaborn as sns
17/10: sns.boxplot(data=probs)
17/11: sns.violinplot(data=probs)
17/12: sns.violinplot(data=probs, inner-"quart")
17/13: sns.violinplot(data=probs, inner="quart")
17/14:
sns.violinplot(data=probs, inner="quart")
plt.axhline(0)
17/15:
sns.violinplot(data=probs, inner="quart")
plt.axhline(0, alpha=0.5, ls="--")
17/16:
class Model():
    def __init__(self):
        pass
17/17:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print("Result:\n",res)
17/18:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print(res)
17/19:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print(res.size())
17/20:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print(res.dimensions())
17/21:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print(res.shape())
17/22:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print(type(res))
17/23:
# average_results = [[], []]
# for n in range(1000):
# Import numpy
import numpy as np

# Creating numpy array
arr = np.array([7, 6, 5, 7, 6, 7, 6, 6, 6, 4, 5, 6])

# Display original array
print("Original array:\n",arr,"\n")

# Return all the positions of max value
res = np.argwhere(arr == np.amax(arr))

# Display result
print(res.shape)
17/24:
class Model():
    def __init__(self):
        self.averages = np.zeros(10)
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        weights = [(1-self.epsilon)/len(res) if [i] in res else epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(self.averages, p=weights)
17/25:
model = Model()
print(model.chosen_bandit)
17/26:
model = Model()
print(model.chosen_bandit())
17/27:
class Model():
    def __init__(self):
        self.averages = np.zeros(10)
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        weights = [(1-self.epsilon)/len(res) if [i] in res else epsilon/(len(self.averages) - len(res)) for i in range(10)]
        print(weights)
        self.n += 1
        return np.random.choice(self.averages, p=weights)
17/28:
model = Model()
print(model.chosen_bandit())
17/29:
class Model():
    def __init__(self):
        self.averages = np.zeros(10)
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        weights = [(1-self.epsilon)/len(res) if [i] in res else epsilon/(len(self.averages) - len(res)) for i in range(10)]
        print(weights)
        self.n += 1
        return 0
        # return np.random.choice(self.averages, p=weights)
17/30:
model = Model()
print(model.chosen_bandit())
17/31:
model = Model()
print(model.chosen_bandit())
17/32:
class Model():
    def __init__(self):
        self.averages = np.zeros(10)
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/33:
model = Model()
print(model.chosen_bandit())
17/34:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/35:
model = Model()
print(model.chosen_bandit())
17/36:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/37:
model = Model()
print(model.chosen_bandit())
17/38:
model = Model()
print(model.chosen_bandit())
17/39:
model = Model()
print(model.chosen_bandit())
17/40:
model = Model()
print(model.chosen_bandit())
17/41:
model = Model()
print(model.chosen_bandit())
17/42:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/43:
model = Model()
print(model.chosen_bandit())
17/44:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/45:
model = Model()
print(model.chosen_bandit())
17/46:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/47:
model = Model()
print(model.chosen_bandit())
17/48:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/49:
model = Model()
print(model.chosen_bandit())
17/50:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/51:
model = Model()
print(model.chosen_bandit())
17/52:
class Model():
    def __init__(self):
        # self.averages = np.zeros(10)
        self.averages = np.array([10, 9, 9, 6, 7, 8, 2, 3, 4, 8])
        self.epsilon = 0.1
        self.n = 1

    def chosen_bandit(self):
        res = np.argwhere(self.averages == np.amax(self.averages))
        if len(res) == len(self.averages):
            return np.random.choice(range(10))
        weights = [(1-self.epsilon)/len(res) if [i] in res else self.epsilon/(len(self.averages) - len(res)) for i in range(10)]
        self.n += 1
        return np.random.choice(range(10), p=weights)
17/53:
model = Model()
print(model.chosen_bandit())
18/1:
# bw_mdp = {state: {action: [(transition_probability p(s' | s, a), s', r, )]
bw_mdp = {

    0 : {
        "Right" : [(1, 0, 0, True)],
        "Left" : [(1, 0, 0, True)]
    },

    1 : {
        "Right" : [(1, 2, 1, True)],
        "Left" : [(1, 0, 0, True)]
    },

    2 : {
        "Right" : [(1, 2, 0, True)],
        "Left" : [(1, 2, 0, True)]
    }
    
}
18/2: import json
18/3:
actions = ["Right", "Left"]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state not in terminal_states
    else {
        action: [(1/2, state+1, 0, True), (1/3, state, 0, True), (1/6, state-1, 0, True)]
    }
        for state in range(7)
}
bsw_mdp
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/4:
actions = ["Right", "Left"]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state not in terminal_states
    else {
        action: [(1/2, state+1, 0, True), (1/3, state, 0, True), (1/6, state-1, 0, True)] for action in actions
    }
        for state in range(7)
}
bsw_mdp
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/5:
actions = ["Right", "Left"]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        action: [(1/2, state+1, 0, True), (1/3, state, 0, True), (1/6, state-1, 0, True)] for action in actions
    }
        for state in range(7)
}
bsw_mdp
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/6:
actions = ["Right", "Left"]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        action: [(1/2, state+1, 0, True), (1/3, state, 0, True), (1/6, state-1, 0, True)] for action in actions
    }
        for state in range(7)
}
json.dumps(bsw_mdp, indent=2)
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/7:
actions = ["Right", "Left"]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        action: [(1/2, state+1, 0, True), (1/3, state, 0, True), (1/6, state-1, 0, True)] for action in actions
    }
        for state in range(7)
}
print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/8:
actions = ["Right", "Left"]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        action: [(1/2, state+1, 0, True), (1/3, state, 0, True), (1/6, state-1, 0, True)] for action in actions
    }
        for state in range(7)
}
bsw_mdp
#print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/9:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, True), (1/3, state, 0, True), (1/6, state-dir[i], 0, True)] for i in range(len(actions))
    }
        for state in range(7)
}
for key, value in bsw_mdp.items():
    if key in terminal_states:
        continue
bsw_mdp    
#print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/10:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, True), (1/3, state, 0, True), (1/6, state-dir[i], 0, True)] for i in range(len(actions))
    }
        for state in range(7)
}
for state, values in bsw_mdp.items():
    if state in terminal_states:
        continue
    if values["Right"][0][1] == n-1:
        print("HERE")
        values["Right"][0][2] = 1
#print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/11:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, True), (1/3, state, 0, True), (1/6, state-dir[i], 0, True)] for i in range(len(actions))
    }
        for state in range(7)
}
for state, values in bsw_mdp.items():
    if state in terminal_states:
        continue
    if values["Right"][0][1] == n-1:
        print("HERE")
        values["Right"][0] = (values["Right"][0][0], n-1, 1, True)
    if values["Left"][2][1] == n-1:
        print("HERE2")
        values["Left"][2] = (values["Left"][2][0], n-1, 1, True)
#print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/12:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, True), (1/3, state, 0, True), (1/6, state-dir[i], 0, True)] for i in range(len(actions))
    }
        for state in range(7)
}

bsw_mdp[n-2]["Right"][0] = (bsw_mdp[n-2]["Right"][0][0], n-1, 1, True)
bsw_mdp[n-2]["Left"][2] = (bsw_mdp[n-2]["Left"][2][0], n-1, 1, True)

print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/13:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, False), (1/3, state, 0, False), (1/6, state-dir[i], 0, False)] for i in range(len(actions))
    }
        for state in range(7)
}

bsw_mdp[n-2]["Right"][0] = (bsw_mdp[n-2]["Right"][0][0], n-1, 1, True)
bsw_mdp[n-2]["Left"][2] = (bsw_mdp[n-2]["Left"][2][0], n-1, 1, True)
bsw_mdp[1]["Left"][0] = (bsw_mdp[1]["Left"][0][0], 0, 0, True)
bsw_mdp[1]["Right"][2] = (bsw_mdp[1]["Right"][2][0], 0, 0, True)

print(json.dumps(bsw_mdp, indent=2))
# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/14:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, False), (1/3, state, 0, False), (1/6, state-dir[i], 0, False)] for i in range(len(actions))
    }
        for state in range(7)
}

bsw_mdp[n-2]["Right"][0] = (bsw_mdp[n-2]["Right"][0][0], n-1, 1, True)
bsw_mdp[n-2]["Left"][2] = (bsw_mdp[n-2]["Left"][2][0], n-1, 1, True)
bsw_mdp[1]["Left"][0] = (bsw_mdp[1]["Left"][0][0], 0, 0, True)
bsw_mdp[1]["Right"][2] = (bsw_mdp[1]["Right"][2][0], 0, 0, True)

print(json.dumps(bsw_mdp, indent=2))

# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/15:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 0, state+dir[i] in terminal_states), (1/3, state, 0, False), (1/6, state-dir[i], 0, state-dir[i] in terminal_states)] for i in range(len(actions))
    }
        for state in range(7)
}

bsw_mdp[n-2]["Right"][0] = (bsw_mdp[n-2]["Right"][0][0], n-1, 1, True)
bsw_mdp[n-2]["Left"][2] = (bsw_mdp[n-2]["Left"][2][0], n-1, 1, True)
# bsw_mdp[1]["Left"][0] = (bsw_mdp[1]["Left"][0][0], 0, 0, True)
# bsw_mdp[1]["Right"][2] = (bsw_mdp[1]["Right"][2][0], 0, 0, True)

print(json.dumps(bsw_mdp, indent=2))

# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/16:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        actions[i]: [(1/2, state+dir[i], 1*(state + dir[i] == n-1), state+dir[i] in terminal_states), (1/3, state, 0, False), (1/6, state-dir[i], 1*(state - dir[i] == n-1), state-dir[i] in terminal_states)] for i in range(len(actions))
    }
        for state in range(7)
}

# bsw_mdp[n-2]["Right"][0] = (bsw_mdp[n-2]["Right"][0][0], n-1, 1, True)
# bsw_mdp[n-2]["Left"][2] = (bsw_mdp[n-2]["Left"][2][0], n-1, 1, True)
# bsw_mdp[1]["Left"][0] = (bsw_mdp[1]["Left"][0][0], 0, 0, True)
# bsw_mdp[1]["Right"][2] = (bsw_mdp[1]["Right"][2][0], 0, 0, True)

print(json.dumps(bsw_mdp, indent=2))

# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/17:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        action: [(1/2, state-1 + 2*(action == "Right"), 1*(state + 2*(action == "Right") == n), (state-1 + 2*(action == "Right")) in terminal_states), (1/3, state, 0, False), (1/6, state+1-2*(action=="Right"), 1*(state+1-2*(action=="Right") == n-1), state+1-2*(action=="Right") in terminal_states)] for action in actions
    }
        for state in range(7)
}

# bsw_mdp[n-2]["Right"][0] = (bsw_mdp[n-2]["Right"][0][0], n-1, 1, True)
# bsw_mdp[n-2]["Left"][2] = (bsw_mdp[n-2]["Left"][2][0], n-1, 1, True)
# bsw_mdp[1]["Left"][0] = (bsw_mdp[1]["Left"][0][0], 0, 0, True)
# bsw_mdp[1]["Right"][2] = (bsw_mdp[1]["Right"][2][0], 0, 0, True)

print(json.dumps(bsw_mdp, indent=2))

# bsw_mdp = {

#     0 : {
#         "Right" : [(1, 0, 0, True)],
#         "Left" : [(1, 0, 0, True)],
#     },

#     1 : {
#         "Right" : [
#             (1/2, 2, 0, False),
#             (1/3, 1, 0, False),
#             (1/6, 0, 0, True),
#         ],
#         "Left" : [
#             # to be added
#             None
#         ]
#     }, 

#     # to be added 
#     2 : None,
#     3 : None,
#     4 : None,
#     5 : None,
#     6 : None,
    
# }
18/18: terminal_states = [5, 7, 11, 12, 15]
18/19:
n = 16
goal = 15
terminal_states = [5, 7, 11, 12, 15]
actions = ["Up", "Left", "Down", "Right"]
18/20:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2)
18/21:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/22:
n = 16
goal = 15
terminal_states = [5, 7, 11, 12, 15]
actions = ["Up", "Left", "Down", "Right"]
dirs = [-4, -1, +4, +1]
18/23:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/24:
n = 16
goal = 15
terminal_states = [0, 5, 7, 11, 12, 15]
actions = ["Up", "Left", "Down", "Right"]
dirs = [-4, -1, +4, +1]
18/25:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/26:
n = 16
goal = 15
terminal_states = [0, 5, 7, 11, 12, 15]
actions = ["Up", "Down", "Left", "Right"]
dirs = [-4, -1, +4, +1]
18/27:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/28:
n = 16
goal = 15
terminal_states = [0, 5, 7, 11, 12, 15]
actions = ["Up", "Down", "Left", "Right"]
dirs = [-4, +4, -1, +1]
18/29:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/30:
n = 16
goal = 15
terminal_states = [5, 7, 11, 12, 15]
actions = ["Up", "Down", "Left", "Right"]
dirs = [-4, +4, -1, +1]
18/31:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/32:
import gym
P = gym.make('FrozenLake-v1').env.P
18/33:
import gym
P = gym.make('FrozenLake-v1').env.P
18/34:
import gym
P = gym.make('FrozenLake-v1').env.P
18/35:
!pip install gym
# import gym
# P = gym.make('FrozenLake-v1').env.P
18/36:
#!pip install gym
import gym
P = gym.make('FrozenLake-v1').env.P
18/37:
# using the pretty print module

import pprint
pprint.pprint(P)
actions_dict = {actions[i]: i for i in range(4)}
error = False

for key, actions in f1_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print("Error")
            error = True
            break
    if error:
        break
18/38:
# using the pretty print module

import pprint
pprint.pprint(P)
actions_dict = {actions[i]: i for i in range(4)}
error = False

for key, actions in f1_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print("Error")
            error = True
            break
    if error:
        break
18/39:
# using the pretty print module

import pprint
pprint.pprint(P)
actions_dict = {actions[i]: i for i in range(4)}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print("Error")
            error = True
            break
    if error:
        break
18/40:
# using the pretty print module

import pprint
pprint.pprint(P)
actions_dict = {actions[i]: i for i in range(4)}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print(key, action)
            error = True
            break
    if error:
        break
18/41:
n = 16
goal = 15
terminal_states = [5, 7, 11, 12, 15]
actions = ["Up", "Down", "Left", "Right"]
dirs = [-4, +4, -1, +1]
18/42:
fl_mdp = {
    state: {
        action: [(1, state, 0, True)] for action in actions
    }
    if state in terminal_states
    else {
         actions[i]: [(1/3, state+dirs[j]*(0 <= state+dirs[j] <= 15), 1*(state+dirs[j]*(0 <= state+dirs[j] <= 15) == goal), state+dirs[j]*(0 <= state+dirs[j] <= 15) in terminal_states) for j in range(4) if int(j/2) != int(i/2) or j%2 == i%2] for i in range(4)  
    }
    for state in range(16)
}
print(json.dumps(fl_mdp, indent=2))
18/43:
# using the pretty print module

import pprint
pprint.pprint(P)
actions_dict = {"Up": 0, "Left": 1, "Down": 2, "Right": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print(key, action)
            error = True
            break
    if error:
        break
18/44:
# using the pretty print module

import pprint
pprint.pprint(P)
actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print(key, action)
            error = True
            break
    if error:
        break
18/45:
import pprint
pprint.pprint(P)
18/46:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        if values != P[key][actions_dict.get(action)]:
            print(values, P[key][actions_dict.get(action)])
            error = True
            break
    if error:
        break
18/47:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        new_list = P[key][actions_dict.get(action)]
        for i in range(3):
            if values[3-i] != new_list[i]
                print(values, P[key][actions_dict.get(action)])
                error = True
                break
    if error:
        break
18/48:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        new_list = P[key][actions_dict.get(action)]
        for i in range(3):
            if values[3-i] != new_list[i]:
                print(values, P[key][actions_dict.get(action)])
                error = True
                break
    if error:
        break
18/49:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        new_list = P[key][actions_dict.get(action)]
        for i in range(3):
            if values[2-i] != new_list[i]:
                print(values, P[key][actions_dict.get(action)])
                error = True
                break
    if error:
        break
18/50:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        print(action)
        new_list = P[key][actions_dict.get(action)]
        for i in range(3):
            if values[2-i] != new_list[i]:
                print(values, P[key][actions_dict.get(action)])
                error = True
                break
    if error:
        break
18/51:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        new_list = P[key][actions_dict.get(action)]
        for i in range(3):
            if values[i] != new_list[i]:
                print(values, P[key][actions_dict.get(action)])
                error = True
                break
    if error:
        break
18/52:
# using the pretty print module

actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
error = False

for key, actions in fl_mdp.items():
    for action, values in actions.items():
        print(action)
        new_list = P[key][actions_dict.get(action)]
        for i in range(3):
            if values[i] != new_list[i]:
                print(values, P[key][actions_dict.get(action)])
                error = True
                break
    if error:
        break
18/53:
# # using the pretty print module

# actions_dict = {"Left": 0, "Right": 1, "Down": 2, "Up": 3}
# error = False

# for key, actions in fl_mdp.items():
#     for action, values in actions.items():
#         print(action)
#         new_list = P[key][actions_dict.get(action)]
#         for i in range(3):
#             if values[i] != new_list[i]:
#                 print(values, P[key][actions_dict.get(action)])
#                 error = True
#                 break
#     if error:
#         break
18/54:
actions = ["Right", "Left"]
dir = [1, -1]
n = 7
terminal_states = [0, n-1]
bsw_mdp = {
    state: {
        action: [(1,state,0,True)] for action in actions 
    }
    if state in terminal_states
    else {
        action: [(1/2, state-1 + 2*(action == "Right"), 1*(state + 2*(action == "Right") == n), (state-1 + 2*(action == "Right")) in terminal_states), (1/3, state, 0, False), (1/6, state+1-2*(action=="Right"), 1*(state+1-2*(action=="Right") == n-1), state+1-2*(action=="Right") in terminal_states)] for action in actions
    }
        for state in range(7)
}

print(json.dumps(bsw_mdp, indent=2))
17/54:
# Dynamic Programming and Value Iteration
terminal_states = [0, 15]
states = list(range(16))
actions = [+4, -4, -1, +1]
values = [0]*16
mdp = {
    state: {
        action: {0.25, state, 0} for action in actions
    }
    if state in terminal_states
    else {
        action: (0.25, state + action*(0 <= state + action <= 15), 1*(state + action in terminal_states)) for action in actions
    }
    for state in states
}
mdp
17/55:
# Dynamic Programming and Value Iteration
terminal_states = [0, 15]
states = list(range(16))
actions = [+4, -4, -1, +1]
values = [0]*16
mdp = {
    state: {
        action: (0.25, state, 0) for action in actions
    }
    if state in terminal_states
    else {
        action: (0.25, state + action*(0 <= state + action <= 15), 1*(state + action in terminal_states)) for action in actions
    }
    for state in states
}
mdp
17/56:
# Dynamic Programming and Value Iteration
terminal_states = [0, 15]
states = list(range(16))
actions = [+4, -4, -1, +1]
values = [0]*16
mdp = {
    state: {
        action: (0.25, state, 0) for action in actions
    }
    if state in terminal_states
    else {
        action: (0.25, state + action*(0 <= state + action <= 15), -1*(state + action not in terminal_states)) for action in actions
    }
    for state in states
}
epsilon = 10^(-2)
mdp
17/57:
delta = 1000
while delta > epsilon:
    delta = 0
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
17/58:
# Dynamic Programming and Value Iteration
terminal_states = [0, 15]
states = list(range(16))
actions = [+4, -4, -1, +1]
values = [0]*16
mdp = {
    state: {
        action: (0.25, state, 0) for action in actions
    }
    if state in terminal_states
    else {
        action: (0.25, state + action*(0 <= state + action <= 15), -1*(state + action not in terminal_states)) for action in actions
    }
    for state in states
}
epsilon = 10^(-2)
gamma = 0.9
mdp
17/59:
def display_matrix(array):
    for i in range(4):
        print(array[4*i: 4*(i+1)]
17/60:
def display_matrix(array):
    for i in range(4):
        print(array[4*i: 4*(i+1)])
17/61:
delta = 1000
for i in range(1):
    delta = 0
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    display_matrix(values)
17/62:
delta = 1000
for i in range(6):
    delta = 0
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    display_matrix(values)
17/63:
delta = 1000
for i in range(1):
    delta = 0
    for i in range(16):
        prev = values[i]
        print([action[2] + values[action[1]] for key, action in mdp[i].items()])
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    display_matrix(values)
17/64:
delta = 1000
for i in range(1):
    delta = 0
    for i in range(16):
        prev = values[i]
        print([action[2] + values[action[1]] for key, action in mdp[i].items()])
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    #display_matrix(values)
17/65:
delta = 1000
for i in range(1):
    delta = 0
    for i in range(16):
        prev = values[i]
        print([action[2] for key, action in mdp[i].items()])
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    #display_matrix(values)
17/66:
delta = 1000
for i in range(1):
    delta = 0
    for i in range(16):
        prev = values[i]
        print(values)
        print([action[2] for key, action in mdp[i].items()])
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    #display_matrix(values)
17/67:
# Dynamic Programming and Value Iteration
terminal_states = [0, 15]
states = list(range(16))
actions = [+4, -4, -1, +1]
values = [0]*16
mdp = {
    state: {
        action: (0.25, state, 0) for action in actions
    }
    if state in terminal_states
    else {
        action: (0.25, state + action*(0 <= state + action <= 15), -1) for action in actions
    }
    for state in states
}
epsilon = 10^(-2)
gamma = 0.9
mdp
17/68:
def display_matrix(array):
    for i in range(4):
        print(array[4*i: 4*(i+1)])
17/69:
delta = 1000
for i in range(1):
    delta = 0
    for i in range(16):
        prev = values[i]
        print(values)
        print([action[2] for key, action in mdp[i].items()])
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    #display_matrix(values)
17/70:
delta = 1000
for i in range(6):
    delta = 0
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    #display_matrix(values)
17/71: display_matrix(values)
17/72:
delta = 1000
for i in range(6):
    delta = 0
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        delta = max(delta, abs(values[i] - prev))
    display_matrix(values)
17/73:
is_static = True
while is_static:
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        if values[i] != prev:
            is_static = False
    display_matrix(values)
17/74:
is_static = False
while not is_static:
    is_static = True
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        if values[i] != prev:
            is_static = False
    display_matrix(values)
17/75:
is_static = False
while not is_static:
    print("HERE")
    is_static = True
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        if values[i] != prev:
            is_static = False
    display_matrix(values)
17/76:
is_static = False
while not is_static:
    is_static = True
    for i in range(16):
        prev = values[i]
        values[i] = max([action[2] + values[action[1]] for key, action in mdp[i].items()])
        if values[i] != prev:
            print("HERE")
            is_static = False
    display_matrix(values)
17/77:
is_static = False
values = [0]*16
while not is_static:
    is_static = True
    for i in range(16):
        prev = values[i]
        values[i] = max([(action[2] + values[action[1]) for key, action in mdp[i].items()])
        if values[i] != prev:
            is_static = False
    display_matrix(values)
17/78:
is_static = False
values = [0]*16
while not is_static:
    is_static = True
    for i in range(16):
        prev = values[i]
        values[i] = max([(action[2] + values[action[1]]) for key, action in mdp[i].items()])
        if values[i] != prev:
            is_static = False
    display_matrix(values)
17/79:
# Dynamic Programming and Value Iteration
terminal_states = [0, 15]
states = list(range(16))
actions = [+4, -4, -1, +1]
values = [0]*16
mdp = {
    state: {
        action: (0.25, state, 0) for action in actions
    }
    if state in terminal_states
    else {
        action: (0.25, state + action*(0 <= state + action <= 15), -1) for action in actions
    }
    for state in states
}
epsilon = 10^(-2)
gamma = 0.9
mdp
17/80:
def display_matrix(array):
    for i in range(4):
        print(array[4*i: 4*(i+1)])
17/81:
is_static = False
while not is_static:
    is_static = True
    for i in range(16):
        prev = values[i]
        values[i] = max([(action[2] + values[action[1]]) for key, action in mdp[i].items()])
        if values[i] != prev:
            is_static = False
17/82: display_matrix(values)
17/83:
# Car Rental Problem
states = [(i,j) for i, j in range(1, 21)]
17/84:
# Car Rental Problem
states = []
for i in range(1, 21):
    for j in range(1, 21):
        states.append((i,j))
17/85: states
17/86: actions = list(range(-5, 6))
17/87: actions
17/88:
actions = list(range(-5, 6))
import math
17/89:
actions = list(range(-5, 6))
from math import exp
17/90:
def fill_customer_probs(n, expected):
    result = []
    total_prob = 0
    i_factorial = 1
    lambda_power = 1
    e_power = exp(-expected)
    for i in range(n+1):
        probability = lambda_power*e_power/i_factorial
        total_prob += probability
        result.append(probability)
        lambda_power *= expected
        i_factorial *= (i+1)
    result.append(1-total_prob)
17/91:
customer_probs_1 = fill_customer_probs(20, 3)
customer_probs_2 = fill_customer_probs(20, 4)
customer_probs_1
17/92:
customer_probs_1 = fill_customer_probs(20, 3)
customer_probs_2 = fill_customer_probs(20, 4)
customer_probs_1
17/93:
customer_probs_1 = []
customer_probs_2 = []
return_probs_1 = []
return_probs_2 = []
17/94:
def fill_customer_probs(n, expected):
    result = []
    total_prob = 0
    i_factorial = 1
    lambda_power = 1
    e_power = exp(-expected)
    for i in range(n+1):
        probability = lambda_power*e_power/i_factorial
        total_prob += probability
        result.append(probability)
        lambda_power *= expected
        i_factorial *= (i+1)
    result.append(1-total_prob)
17/95:
def fill_customer_probs(n, expected):
    result = []
    total_prob = 0
    i_factorial = 1
    lambda_power = 1
    e_power = exp(-expected)
    for i in range(n+1):
        probability = lambda_power*e_power/i_factorial
        total_prob += probability
        result.append(probability)
        lambda_power *= expected
        i_factorial *= (i+1)
    result.append(1-total_prob)
    return result
17/96:
customer_probs_1 = fill_customer_probs(20, 3)
customer_probs_2 = fill_customer_probs(20, 4)
customer_probs_1
17/97:
def fill_probs(n, expected):
    result = []
    total_prob = 0
    i_factorial = 1
    lambda_power = 1
    e_power = exp(-expected)
    for i in range(n+1):
        probability = lambda_power*e_power/i_factorial
        total_prob += probability
        result.append(probability)
        lambda_power *= expected
        i_factorial *= (i+1)
    result.append(1-total_prob)
    return result
17/98:
customer_probs_1 = fill_probs(20, 3)
customer_probs_2 = fill_probs(20, 4)
return_probs_1 = fill_probs(19, 3)
return_probs_2 = fill_probs(19, 2)
customer_probs_1
17/99:
mdp = {}
lambda1 = 3
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 1 <= state[0] - action <= 20 && 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
17/100:
mdp = {}
lambda1 = 3
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 1 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
17/101:
for i in range(1, len(customer_probs_1)):
    customer_probs_1[i] += customer_probs_1[i-1]
    customer_probs_2[i] += customer_probs_2[i-1]
customer_probs_1
17/102:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 1 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        for ret_1 in range(0, 20-init_state_1):
            for ret_2 in range(0, 20-init_state_2):
                prob = return_probs_1[ret1]*return_probs_2[ret2]
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/103:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 1 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        for ret1 in range(0, 20-init_state_1):
            for ret2 in range(0, 20-init_state_2):
                prob = return_probs_1[ret1]*return_probs_2[ret2]
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/104: mdp
17/105:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 1 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        for ret1 in range(0, 21-init_state_1):
            for ret2 in range(0, 21-init_state_2):
                prob = return_probs_1[ret1]*return_probs_2[ret2]
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/106: mdp
17/107:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max(sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/108:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/109:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/110:
# Car Rental Problem
states = []
for i in range(0, 21):
    for j in range(0, 21):
        states.append((i,j))
17/111:
actions = list(range(-5, 6))
from math import exp
17/112:
customer_probs_1 = []
customer_probs_2 = []
return_probs_1 = []
return_probs_2 = []
17/113:
def fill_probs(n, expected):
    result = []
    total_prob = 0
    i_factorial = 1
    lambda_power = 1
    e_power = exp(-expected)
    for i in range(n+1):
        probability = lambda_power*e_power/i_factorial
        total_prob += probability
        result.append(probability)
        lambda_power *= expected
        i_factorial *= (i+1)
    result.append(1-total_prob)
    return result
17/114:
customer_probs_1 = fill_probs(20, 3)
customer_probs_2 = fill_probs(20, 4)
return_probs_1 = fill_probs(19, 3)
return_probs_2 = fill_probs(19, 2)
customer_probs_1
17/115:
for i in range(1, len(customer_probs_1)):
    customer_probs_1[i] += customer_probs_1[i-1]
    customer_probs_2[i] += customer_probs_2[i-1]
customer_probs_1
17/116:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 1 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        for ret1 in range(0, 21-init_state_1):
            for ret2 in range(0, 21-init_state_2):
                prob = return_probs_1[ret1]*return_probs_2[ret2]
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/117: mdp
17/118:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/119: mdp.drop((0,0))
17/120:
del mdp[(0,0)]
mdp
17/121:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/122:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 0 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        for ret1 in range(0, 21-init_state_1):
            for ret2 in range(0, 21-init_state_2):
                prob = return_probs_1[ret1]*return_probs_2[ret2]
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/123: mdp
17/124:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/125:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 0 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        total_return_prob_1 = 1
        for ret1 in range(0, 21-init_state_1):
            p1 = return_probs_1[ret1]
            if ret1 + init_state_1 == 20:
                p1 = total_return_prob_1
            else:
                total_return_prob_1 -= p1
            total_return_prob_2 = 1
            for ret2 in range(0, 20-init_state_2):
                p2 = return_probs_2[ret2]
                if ret2 + init_state_2 == 20:
                    p2 = total_return_prob_2
                else:
                    total_return_prob_2 -= p2
                prob = p1*p2
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/126: mdp
17/127:
mdp = {}
lambda1 = 3
lambda2 = 4
ret_lambda1 = 3
ret_lambda2  = 2
for state in states:
    mdp[state] = {}
    possible_actions = [action for action in actions if 0 <= state[0] - action <= 20 and 0 <= state[1] + action <= 20]
    for action in possible_actions:
        mdp[state][action] = []
        init_state_1 = state[0] - action
        init_state_2 = state[1] + action
        total_return_prob_1 = 1
        for ret1 in range(0, 21-init_state_1):
            p1 = return_probs_1[ret1]
            if ret1 + init_state_1 == 20:
                p1 = total_return_prob_1
            else:
                total_return_prob_1 -= p1
            total_return_prob_2 = 1
            for ret2 in range(0, 21-init_state_2):
                p2 = return_probs_2[ret2]
                if ret2 + init_state_2 == 20:
                    p2 = total_return_prob_2
                else:
                    total_return_prob_2 -= p2
                prob = p1*p2
                final_state = (init_state_1 + ret1, init_state_2 + ret2)
                reward = -2*abs(action)
                if final_state[0] != 0:
                    reward += 10*lambda1*customer_probs_1[final_state[0] - 1] 
                if final_state[1] != 0:
                    reward += 10*lambda2*customer_probs_2[final_state[1] - 1]
                mdp[state][action].append((prob, final_state, reward))
17/128: mdp
17/129:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/130:
values = [0]*len(states)
gamma = 0.9
for iteration in range(1):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/131:
values = [0]*len(states)
gamma = 0.9
for iteration in range(16):
    for i in range(len(states)):
        values[i] = max([sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()])
values
17/132:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
for iteration in range(6):
    for i in range(len(states)):
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        best_actions[i] = mdp[states[i]].values()[np.argmax(possibilities)]
values
17/133:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
for iteration in range(6):
    for i in range(len(states)):
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        best_actions[i] = mdp[states[i]].keys()[np.argmax(possibilities)]
values
17/134:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
for iteration in range(6):
    for i in range(len(states)):
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
values
17/135: best_actions
17/136:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
for iteration in range(16):
    for i in range(len(states)):
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
values
17/137: best_actions
17/138:
import pandas as pd
import seaborn as sns
17/139:
def display_values(array):
    for i in range(21):
        print(array[21*i: 21*(i+1)])
17/140: display_values(best_actions)
17/141:
#best values after 16 iterations
display_values(best_actions)
17/142:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
for iteration in range(100):
    for i in range(len(states)):
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
17/143:
#best values after 100 iterations
display_values(best_actions)
17/144:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
i = 0
epsilon = 10**(-2)
delta = 10
while delta > epsilon:
    i += 1
    print("Iteration Number:", i)
    delta = 0
    for i in range(len(states)):
        prev = values[i]
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        delta = max(delta, abs(values[i] - prev))
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
17/145:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
i = 0
epsilon = 10**(-2)
delta = 10
while delta > epsilon:
    i += 1
    print("Iteration Number:", i)
    print(delta)
    delta = 0
    for i in range(len(states)):
        prev = values[i]
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        delta = max(delta, abs(values[i] - prev))
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
17/146:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
index = 0
epsilon = 10**(-2)
delta = 10
while delta > epsilon:
    index += 1
    print("Iteration Number:", index)
    print(delta)
    delta = 0
    for i in range(len(states)):
        prev = values[i]
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        delta = max(delta, abs(values[i] - prev))
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
17/147: display_values(best_actions)
17/148:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
index = 0
epsilon = 10**(-2)
delta = 10
for iteration in range(400):
    index += 1
    print("Iteration Number:", index)
    print(delta)
    delta = 0
    for i in range(len(states)):
        prev = values[i]
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        delta = max(delta, abs(values[i] - prev))
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
17/149: display_values(best_actions)
17/150:
values = [0]*len(states)
best_actions = [0]*len(states)
gamma = 0.9
index = 0
epsilon = 10**(-2)
delta = 10
while delta > epsilon:
    index += 1
    print("Iteration Number:", index)
    print(delta)
    delta = 0
    for i in range(len(states)):
        prev = values[i]
        possibilities = [sum([result[0]*(result[2] + gamma*values[21*result[1][0] + result[1][1]]) for result in action_list]) for action, action_list in mdp[states[i]].items()]
        values[i] = max(possibilities)
        delta = max(delta, abs(values[i] - prev))
        best_actions[i] = list(mdp[states[i]].keys())[np.argmax(possibilities)]
17/151: # BlackJack Game
17/152:
states = []
for bool in [True, False]:
    for sum in range(1, 22):
        for dealer_card in range(1, 11):
            states.append(bool, sum, dealer_card)
states
17/153:
states = []
for bool in [True, False]:
    for sum in range(1, 22):
        for dealer_card in range(1, 11):
            states.append((bool, sum, dealer_card))
states
17/154: actions = [1, 2]
17/155:
def add_to_sum(sum):
    new_card = np.random.choice(list(range(1,11)), p=[1/13 for i in range(1, 11) if i < 10 else 4/13])
17/156:
def add_to_sum(sum):
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 for i in range(1, 11) if i < 10 else 4/13])
17/157:
def add_to_sum(sum):
    new_card = np.random.choice(a=list(range(1,11)))
17/158:
def add_to_sum(sum):
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
17/159:
states = []
for bool in [True, False]:
    if bool:
        for sum in range(1, 12):
            for dealer_card in range(1, 11):
                states.append((bool, sum, dealer_card))
        continue   
    for sum in range(1, 22):
        for dealer_card in range(1, 11):
            states.append((bool, sum, dealer_card))
states
17/160:
states = []
for bool in [True, False]:
    if bool:
        for sum in range(1, 12):
            for dealer_card in range(1, 11):
                states.append((bool, sum, dealer_card))
        continue   
    for sum in range(1, 22):
        for dealer_card in range(1, 11):
            states.append([bool, sum, dealer_card])
states
17/161:
states = []
for bool in [True, False]:
    if bool:
        for sum in range(1, 12):
            for dealer_card in range(1, 11):
                states.append([bool, sum, dealer_card])
        continue   
    for sum in range(1, 22):
        for dealer_card in range(1, 11):
            states.append([bool, sum, dealer_card])
states
17/162:
def add_to_sum(i):
    global states
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
    if state[0]:
        state[1] += new_card 
        if state[1] > 11:
            state[0] = False
    else:
        state[1] += new_card
    if state[1] > 21:
        return -10
    else:
        return 0
17/163:
def add_to_sum(i):
    global states
    state = states[i]
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
    if state[0]:
        state[1] += new_card 
        if state[1] > 11:
            state[0] = False
    else:
        state[1] += new_card
    if state[1] > 21:
        return -10
    else:
        return 0
17/164:
def check_victory(i):
    global states
    state = states[i]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return 10
17/165:
states = []
for bool in [True, False]:
    if bool:
        for sum in range(12, 22):
            for dealer_card in range(1, 11):
                states.append([bool, sum, dealer_card])
        continue   
    for sum in range(1, 22):
        for dealer_card in range(1, 11):
            states.append([bool, sum, dealer_card])
states
17/166: policy = [1 if state[1] < 17 else 2 for state in states]
17/167:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return (state, +10)
    else:
        if state[2] > state[1]:
            return (state, -10)
        if state[2] == state[1]:
            return (state, 0)
        else:
            return (state, +10)
17/168:
epsilon = 0.1
policy = []
for state in states:
    if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/169: Q = [0 for state, action in states, actions]
17/170: Q = [0]*(len(states)*2)
17/171:
Q = [0]*(len(states)*2)
Returns = [[]]*(len(Q))
17/172:
def states_index(state):
    if state[0]:
        return 10*state[1] - 121 + state[2]
    else:
        return 89 + 10*state[1] + state[2]
17/173:
def add_to_sum(i):
    global states
    state = [val for val in states[i]]
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
    if state[0]:
        state[1] += new_card 
        if state[1] > 21:
            state[1] -= 10
            state[0] = False
    else:
        state[1] += new_card
    if state[1] > 21:
        return (state, -10)
    else:
        return (state, 0)
17/174:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return (state, +10)
    else:
        if state[2] > state[1]:
            return (state, -10)
        if state[2] == state[1]:
            return (state, 0)
        else:
            return (state, +10)
17/175:
Q = [0]*(len(states)*2)
Returns = [[0,0]]*(len(Q))
17/176:
def game():
    global policy, Q, Returns, states
    episode_sequence = [np.random.choice(states)]
    index = states_index(episode_sequence[0])
    first_action = np.random.choice(policy[2*index: 2*index+2], p=[policy[2*index][0], policy[2*index+1][0]])
17/177:
Q = [0]*(len(states)*2)
Returns = [[0,0]]*(len(Q))
gamma = 0.9
17/178:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice(policy[2*index: 2*index+2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action[1] != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice(policy[2*index: 2*index+2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
17/179:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice(policy[2*index: 2*index+2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action[1] != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice(policy[2*index: 2*index+2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            # A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
17/180: game()
17/181:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action[1] != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            # A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
17/182: game()
17/183:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            # A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
17/184: game()
17/185:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/186:
epsilon = 0.1
policy = []
for state in states:
    if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/187:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/188:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            # A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
17/189: game()
17/190:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            # A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
17/191: game()
17/192: Q
17/193: game()
17/194: Q
17/195: game()
17/196: Q[2*269+1]
17/197: game()
17/198:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            # A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
    return episode_sequence
17/199:
ep_seq = game()
ep_seq
17/200: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/201:
ep_seq = game()
ep_seq
17/202: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/203:
ep_seq = game()
ep_seq
17/204: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/205:
ep_seq = game()
ep_seq
17/206: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/207:
ep_seq = game()
ep_seq
17/208: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/209:
ep_seq = game()
ep_seq
17/210: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/211:
ep_seq = game()
ep_seq
17/212: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/213:
ep_seq = game()
ep_seq
17/214: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/215:
ep_seq = game()
ep_seq
17/216: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/217:
ep_seq = game()
ep_seq
17/218: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/219:
ep_seq = game()
ep_seq
17/220: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/221:
ep_seq = game()
ep_seq
17/222: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/223:
ep_seq = game()
ep_seq
17/224: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/225:
ep_seq = game()
ep_seq
17/226: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/227:
ep_seq = game()
ep_seq
17/228: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/229:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*Q_index + A][0] = 1 - epsilon/2
            policy[2*Q_index + 1 - A] = epsilon/2
    return episode_sequence
17/230:
ep_seq = game()
ep_seq
17/231: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/232:
ep_seq = game()
ep_seq
17/233: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/234:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [Q[Q_index][0]*Q[Q_index][1] + G, Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] = 1 - epsilon/2
            policy[2*episode_sequence[-i][0] + 1 - A] = epsilon/2
    return episode_sequence
17/235:
ep_seq = game()
ep_seq
17/236: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/237:
ep_seq = game()
ep_seq
17/238: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/239:
ep_seq = game()
ep_seq
17/240: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/241:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/242:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] = 1 - epsilon/2
            policy[2*episode_sequence[-i][0] + 1 - A] = epsilon/2
    return episode_sequence
17/243:
ep_seq = game()
ep_seq
17/244: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/245:
ep_seq = game()
ep_seq
17/246: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/247:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/248:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] = 1 - epsilon/2
            policy[2*episode_sequence[-i][0] + 1 - A][0] = epsilon/2
    return episode_sequence
17/249:
ep_seq = game()
ep_seq
17/250: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/251:
ep_seq = game()
ep_seq
17/252: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/253:
ep_seq = game()
ep_seq
17/254: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/255:
ep_seq = game()
ep_seq
17/256: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/257:
ep_seq = game()
ep_seq
17/258: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/259:
ep_seq = game()
ep_seq
17/260: Q[2*ep_seq[-1][0]+ep_seq[-1][1]-1]
17/261:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/262:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] -= policy[2*episode_sequence[-i][0] + 1 - A][0]*0.1
    return episode_sequence
17/263:
for i in range(1000):
    game()
17/264:
epsilon = 0.1
policy = []
for state in states:
    if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/265:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/266:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] -= policy[2*episode_sequence[-i][0] + 1 - A][0]*0.1
    return episode_sequence
17/267:
for i in range(1000):
    game()
17/268:
for i in range(1000):
    game()
17/269: Q
17/270:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] -= policy[2*episode_sequence[-i][0] + 1 - A][0]*0.1
    return reward
17/271:
for i in range(1000):
    reward = game()
    print(reward)
17/272:
for i in range(1000):
    reward = game()
    results.append(reward)
17/273: results = []
17/274:
for i in range(1000):
    reward = game()
    results.append(reward)
17/275:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/276: results
17/277:
for i in range(10000):
    reward = game()
    results.append(reward)
17/278:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/279:
#Display results after 10000 iterations
results
17/280:
#Display results after 10000 iterations
for i in range(1, 1001):
    print(results[-i])
17/281: policy
17/282: len(policy)
17/283: len(states)
17/284: results = []
17/285:
for i in range(10000):
    reward = game()
    results.append(reward)
17/286:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/287:
#Display results after 10000 iterations
for i in range(1, 1001):
    print(results[-i])
17/288:
for i in range(1, 1001):
    print(results[-i])
17/289: results = []
17/290:
for i in range(1000):
    reward = game()
    results.append(reward)
17/291:
for i in range(1, 1001):
    print(results[-i])
17/292:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/293:
#Display results after 10000 iterations
for i in range(1, 1001):
    print(results[-i])
17/294:
for i in range(10000):
    reward = game()
    results.append(reward)
17/295:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/296:
#Display results after 10000 iterations
for i in range(1, 1001):
    print(results[-i])
17/297:
for i in range(100, 310):
    print(1 + (policy[2*i][0] < policy[2*i + 1][0]))
17/298:
for i in range(100, 310):
    print(int((i-99)/10) + 1, 1 + (policy[2*i][0] < policy[2*i + 1][0]))
17/299:
for i in range(100, 310):
    print(int((i-99)/10) + 1, (i-99)%10 + 10*((i-99)%10 == 0), 1 + (policy[2*i][0] < policy[2*i + 1][0]))
17/300:
for line in range(100, 310, 10):
    print([1 + (policy[2*i][0] < policy[2*i + 1][0]) for i in range(line, line+10)])
17/301:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/302:
for i in range(200000):
    reward = game()
    results.append(reward)
17/303:
for i in range(1, 1001):
    print(results[-i])
17/304:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/305:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/306:
#Display results after 10000 iterations
for i in range(1, 1001):
    print(results[-i])
17/307: len(results)
17/308:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/309: results = []
17/310:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][0] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] -= policy[2*episode_sequence[-i][0] + 1 - A][0]*0.1
    return reward
17/311: results = []
17/312:
for i in range(10000):
    reward = game()
    results.append(reward)
17/313:
for i in range(1, 1001):
    print(results[-i])
17/314:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/315:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/316:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/317:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/318:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][0] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] -= policy[2*episode_sequence[-i][0] + 1 - A][0]*0.1
    return reward
17/319: results = []
17/320:
for i in range(10000):
    reward = game()
    results.append(reward)
17/321: Q
17/322: results = []
17/323:
for i in range(200000):
    reward = game()
    results.append(reward)
17/324: Q
17/325:
for i in range(1, 1001):
    print(results[-i])
17/326:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/327:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/328: policy
17/329:
def add_to_sum(i):
    global states
    state = [val for val in states[i]]
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
    if state[0]:
        state[1] += new_card 
        if state[1] > 21:
            state[1] -= 10
            state[0] = False
    elif new_card == 1:
        state[1] += 11
        state[0] = True
        if state[1] > 21:
            state[1] -= 10
            state[0] = False        
    else:
        state[1] += new_card
    if state[1] > 21:
        return (state, -10)
    else:
        return (state, 0)
17/330:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/331:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/332:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/333:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][0] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] -= policy[2*episode_sequence[-i][0] + 1 - A][0]*0.1
    return reward
17/334: results = []
17/335:
for i in range(200000):
    reward = game()
    results.append(reward)
17/336:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/337:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/338: Q
17/339: policy
17/340:
for i in range(100, 310):
    print(int((i-99)/10) + 1, (i-99)%10 + 10*((i-99)%10 == 0), 1 + (policy[2*i][0] < policy[2*i + 1][0]))
17/341:
for i in range(1, 1001):
    print(results[-i])
17/342: Q[209:221]
17/343: Q[2*209:2*221]
17/344: policy[2*209:2*221]
17/345: len(Q)
17/346:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][0] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] += 0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
    return reward
17/347:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/348: stimulate_game(12, 5)
17/349:
epsilon = 0.1
new_policy = []
for state in states:
    if state[1] <= 11:
        new_policy.append([1, 1])
        new_policy.append([0,2])
    if state[1] < 17:
        new_policy.append([1-epsilon, 1])
        new_policy.append([epsilon, 2])
    else:
        new_policy.append([epsilon, 1])
        new_policy.append([1-epsilon, 2])
17/350:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/351: stimulate_game(12, 5)
17/352:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/353: stimulate_game(12, 5)
17/354: stimulate_game(12, np.random.choice(range(1,11)))
17/355: stimulate_game(12, np.random.choice(range(1,11)))
17/356: stimulate_game(12, np.random.choice(range(1,11)))
17/357: stimulate_game(12, np.random.choice(range(1,11)))
17/358: stimulate_game(12, np.random.choice(range(1,11)))
17/359: stimulate_game(12, np.random.choice(range(1,11)))
17/360:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/361: stimulate_game(12, np.random.choice(range(1,11)))
17/362: stimulate_game(12, np.random.choice(range(1,11)))
17/363: stimulate_game(12, np.random.choice(range(1,11)))
17/364: stimulate_game(12, np.random.choice(range(1,11)))
17/365: stimulate_game(12, np.random.choice(range(1,11)))
17/366: stimulate_game(12, np.random.choice(range(1,11)))
17/367:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([new_policy[2*index][0], new_policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([new_policy[2*index][0], new_policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/368:
epsilon = 0.1
new_policy = []
for state in states:
    if state[1] <= 11:
        new_policy.append([1, 1])
        new_policy.append([0,2])
    if state[1] < 17:
        new_policy.append([1-epsilon, 1])
        new_policy.append([epsilon, 2])
    else:
        new_policy.append([epsilon, 1])
        new_policy.append([1-epsilon, 2])
17/369:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([new_policy[2*index][0], new_policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([new_policy[2*index][0], new_policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/370: stimulate_game(12, np.random.choice(range(1,11)))
17/371: states
17/372:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    else if state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/373:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/374:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/375:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][0] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] += 0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
    return reward
17/376: results = []
17/377:
for i in range(200000):
    reward = game()
    results.append(reward)
17/378:
for i in range(1, 1001):
    print(results[-i])
17/379:
for i in range(1, 1001):
    print(results[-i])
17/380:
sum = 0
for i in range(1, len(results)+1):
    sum += results[-i]
    results[-i] = sum/i
17/381:
#Display results after 10000 iterations
for i in range(1, 1001):
    print(results[-i])
17/382:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/383:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/384:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/385:
epsilon = 0.1
new_policy = []
for state in states:
    if state[1] <= 11:
        new_policy.append([1, 1])
        new_policy.append([0,2])
    else if state[1] < 17:
        new_policy.append([1-epsilon, 1])
        new_policy.append([epsilon, 2])
    else:
        new_policy.append([epsilon, 1])
        new_policy.append([1-epsilon, 2])
17/386:
epsilon = 0.1
new_policy = []
for state in states:
    if state[1] <= 11:
        new_policy.append([1, 1])
        new_policy.append([0,2])
    elif state[1] < 17:
        new_policy.append([1-epsilon, 1])
        new_policy.append([epsilon, 2])
    else:
        new_policy.append([epsilon, 1])
        new_policy.append([1-epsilon, 2])
17/387:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([new_policy[2*index][0], new_policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([new_policy[2*index][0], new_policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/388: stimulate_game(12, np.random.choice(range(1,11)))
17/389:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([new_policy[2*index][0], new_policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([new_policy[2*index][0], new_policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[new_policy[2*index][0], new_policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/390: stimulate_game(12, np.random.choice(range(1,11)))
17/391: stimulate_game(12, np.random.choice(range(1,11)))
17/392: stimulate_game(12, np.random.choice(range(1,11)))
17/393: stimulate_game(12, np.random.choice(range(1,11)))
17/394: stimulate_game(12, np.random.choice(range(1,11)))
17/395: stimulate_game(12, np.random.choice(range(1,11)))
17/396: stimulate_game(12, np.random.choice(range(1,11)))
17/397: stimulate_game(12, np.random.choice(range(1,11)))
17/398: stimulate_game(12, np.random.choice(range(1,11)))
17/399: stimulate_game(12, np.random.choice(range(1,11)))
17/400: stimulate_game(12, np.random.choice(range(1,11)))
17/401:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
    return reward
17/402: actions = [1, 2]
17/403:
def states_index(state):
    if state[0]:
        return 10*state[1] - 121 + state[2]
    else:
        return 89 + 10*state[1] + state[2]
17/404:
def add_to_sum(i):
    global states
    state = [val for val in states[i]]
    new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
    if state[0]:
        state[1] += new_card 
        if state[1] > 21:
            state[1] -= 10
            state[0] = False
    elif new_card == 1:
        state[1] += 11
        state[0] = True
        if state[1] > 21:
            state[1] -= 10
            state[0] = False        
    else:
        state[1] += new_card
    if state[1] > 21:
        return (state, -10)
    else:
        return (state, 0)
17/405:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/406:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/407:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/408:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
    return reward
17/409: results = []
17/410:
for i in range(200000):
    reward = game()
    results.append(reward)
17/411:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] += 0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
    return reward
17/412: results = []
17/413:
for i in range(200000):
    reward = game()
    results.append(reward)
17/414:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/415:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/416:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] += 0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] = 1 - policy[2*episode_sequence[-i][0] + A][0]
    return reward
17/417: results = []
17/418:
for i in range(200000):
    reward = game()
    results.append(reward)
17/419:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/420:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/421:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[-1][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[-1].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence.append([index])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[-1].append(action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i] not in episode_sequence[:-i]:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] += 0.1
            policy[2*episode_sequence[-i][0] + 1 - A][0] = 1 - policy[2*episode_sequence[-i][0] + A][0]
    return reward
17/422: results = []
17/423:
for i in range(200000):
    reward = game()
    results.append(reward)
17/424:
for i in range(1, 1001):
    print(results[-i])
17/425:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/426:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/427:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/428:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
        policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
        policy[2*episode_sequence[-i][0] + 1 - A][0] 
    return reward
17/429: results = []
17/430:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/431:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/432:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
        policy[2*episode_sequence[-i][0] + A][0] += (1 - policy[2*episode_sequence[-i][0] + A][0])*0.1
        policy[2*episode_sequence[-i][0] + 1 - A][0] 
    return reward
17/433: results = []
17/434:
for i in range(200000):
    reward = game()
    results.append(reward)
17/435:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
        policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
        policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/436: results = []
17/437:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11:
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/438:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/439:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
        policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
        policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/440: results = []
17/441:
for i in range(200000):
    reward = game()
    results.append(reward)
17/442:
for i in range(1, 1001):
    print(results[-i])
17/443:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/444:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/445:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([policy[2*index][0], policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([policy[2*index][0], policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/446: stimulate_game(12, np.random.choice(range(1,11)))
17/447: stimulate_game(12, np.random.choice(range(1,11)))
17/448: stimulate_game(12, np.random.choice(range(1,11)))
17/449: stimulate_game(12, np.random.choice(range(1,11)))
17/450: stimulate_game(12, np.random.choice(range(1,11)))
17/451: stimulate_game(12, np.random.choice(range(1,11)))
17/452:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < 17:
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/453: stimulate_game(12, 10))
17/454: stimulate_game(12, dealer_card=10)
17/455: stimulate_game(12, dealer_card=10)
17/456: stimulate_game(12, dealer_card=10)
17/457: stimulate_game(12, dealer_card=10)
17/458: stimulate_game(12, dealer_card=10)
17/459: stimulate_game(12, dealer_card=10)
17/460: stimulate_game(12, dealer_card=10)
17/461:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/462: policy
17/463:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/464: stimulate_game(12, dealer_card=10)
17/465: stimulate_game(12, dealer_card=10)
17/466: stimulate_game(12, dealer_card=10)
17/467:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < min(17, state[1]):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/468:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([policy[2*index][0], policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([policy[2*index][0], policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/469: stimulate_game(12, dealer_card=10)
17/470: stimulate_game(12, dealer_card=10)
17/471: stimulate_game(12, dealer_card=10)
17/472: stimulate_game(12, dealer_card=10)
17/473: stimulate_game(12, dealer_card=10)
17/474:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace <= min(16, state[1]):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/475: stimulate_game(12, dealer_card=10)
17/476: stimulate_game(12, dealer_card=10)
17/477: stimulate_game(12, dealer_card=10)
17/478: stimulate_game(12, dealer_card=10)
17/479: stimulate_game(12, dealer_card=10)
17/480: stimulate_game(12, dealer_card=10)
17/481: stimulate_game(12, dealer_card=10)
17/482: stimulate_game(12, dealer_card=10)
17/483: stimulate_game(12, dealer_card=10)
17/484: stimulate_game(12, dealer_card=10)
17/485: stimulate_game(12, dealer_card=10)
17/486: stimulate_game(12, dealer_card=10)
17/487: stimulate_game(12, dealer_card=10)
17/488: stimulate_game(12, dealer_card=10)
17/489:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/490:
certain_indices = []
for i in range(len(states)):
    state = states[i]
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        certain_indices.append(i)
17/491:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/492:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/493:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
        policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
        policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/494: results = []
17/495:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace <= min(16, state[1]):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/496:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/497:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/498:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
        policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
        policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/499: results = []
17/500:
for i in range(200000):
    reward = game()
    results.append(reward)
17/501:
def check_victory(i):
    global states
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace <= min(16, state[1]):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/502:
def check_victory(i):
    global states, policy
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    index = states_index([dealer_has_ace, state[2] + 10*de])
    action = np.random.choice([1, 2], p=[policy[2*states_]])
    while state[2] + 10*dealer_has_ace <= min(16, state[1]):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        #print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/503:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i][0] not in certain_indices:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/504: results = []
17/505:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/506:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/507:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i][0] not in certain_indices:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/508: results = []
17/509:
for i in range(200000):
    reward = game()
    results.append(reward)
17/510:
def check_victory(i):
    global states, policy
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace <= min(16, state[1]):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        #print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/511:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/512:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/513:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i][0] not in certain_indices:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/514: results = []
17/515:
for i in range(200000):
    reward = game()
    results.append(reward)
17/516:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/517:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/518:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([policy[2*index][0], policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([policy[2*index][0], policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    print(reward)
17/519: stimulate_game(12, dealer_card=10)
17/520: stimulate_game(12, dealer_card=10)
17/521: stimulate_game(15, dealer_card=10)
17/522:
def check_victory(i):
    global states, policy
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < state[1] or (state[2] + 10*dealer_has_ace == state[1] and state[1] < 17):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        #print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/523:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/524:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/525:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i][0] not in certain_indices:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/526: results = []
17/527:
for i in range(200000):
    reward = game()
    results.append(reward)
17/528:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/529:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/530:
def check_victory(i):
    global states, policy
    state = [val for val in states[i]]
    dealer_has_ace = (state[2] == 1)
    while state[2] + 10*dealer_has_ace < state[1] or (state[2] + 10*dealer_has_ace == state[1] and state[1] <= 14):
        new_card = np.random.choice(a=list(range(1,11)), p=[1/13 if i < 10 else 4/13 for i in range(1, 11)])
        #print(new_card)
        if new_card == 1:
            dealer_has_ace = True
        state[2] += new_card
    if dealer_has_ace and state[2] <= 11:
        state[2] += 10
    if state[2] > 21:
        return +10
    else:
        if state[2] > state[1]:
            return -10
        if state[2] == state[1]:
            return 0
        else:
            return +10
17/531:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/532:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/533:
def game():
    global policy, Q, states, gamma
    episode_sequence = [[np.random.choice(list(range(len(states))))]]
    index = episode_sequence[0][0]
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    episode_sequence[0].append(action)
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        episode_sequence[0][0] = index
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
        episode_sequence[0][1] = action
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    G = reward/gamma
    for i in range(1, len(episode_sequence)+1):
        G = gamma*G
        if episode_sequence[-i][0] not in certain_indices:
            Q_index = 2*episode_sequence[-i][0] + episode_sequence[-i][1] - 1
            Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + G)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
            A = np.argmax([val[0] for val in Q[Q_index - Q_index%2: Q_index + 2 - Q_index%2]])
            policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
            policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/534: results = []
17/535:
for i in range(200000):
    reward = game()
    results.append(reward)
17/536:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/537:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/538:
certain_indices = []
for i in range(len(states)):
    state = states[i]
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1) or state[1] == 21:
        certain_indices.append(i)
17/539:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] == 21:
        policy.append([0, 1])
        policy.append([1, 2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/540:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1]
        policy[2*episode_sequence[-i][0] + 1 - A][0] *= 0.9
        policy[2*episode_sequence[-i][0] + A][0] = 1 - policy[2*episode_sequence[-i][0] + 1 - A][0]
    return reward
17/541:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            if index < 100:
                for j in range(2*index + 1, 0, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 200, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        elif reward == 10:
            if index < 100:
                for j in range(2*index + 1, 200, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +2):
                policy[j + 1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1]0]
        else:
            for j in range(2*index, 620, +2):
                policy[j+1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/542:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            if index < 100:
                for j in range(2*index + 1, 0, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 200, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        elif reward == 10:
            if index < 100:
                for j in range(2*index + 1, 200, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +2):
                policy[j + 1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        else:
            for j in range(2*index, 620, +2):
                policy[j+1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/543:
for i in range(200000):
    reward = game()
    results.append(reward)
17/544:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            if index < 100:
                for j in range(2*index + 1, 0, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 200, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        elif reward == 10:
            if index < 100:
                for j in range(2*index + 1, 200, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +2):
                policy[j + 1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        else:
            for j in range(2*index, 620, +2):
                policy[j+1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/545: results = []
17/546:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] == 21:
        policy.append([0, 1])
        policy.append([1, 2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/547:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/548:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            if index < 100:
                for j in range(2*index + 1, 0, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 200, -2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        elif reward == 10:
            if index < 100:
                for j in range(2*index + 1, 200, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +2):
                policy[j + 1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        else:
            for j in range(2*index, 620, +2):
                policy[j+1][0] *= 0.9
                policy[j][0] = 1 - policy[j+1][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/549: results = []
17/550:
for i in range(200000):
    reward = game()
    results.append(reward)
17/551:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/552:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/553:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            # Waiting has failed. For all lower numbers, reduce the probability of waiting - odd num
            if index < 100:
                for j in range(2*index + 1, 0, -2):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
            else:
                for j in range(2*index + 1, 200, -2):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
        elif reward == 10:
            # Waiting has worked. For all higher numbers, increase the probability of waiting - even num
            if index < 100:
                for j in range(2*index + 1, 200, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        # Hitting has failed. For all larger values, reduce prob of hitting. Hitting is the even member
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +2):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        else:
            for j in range(2*index, 620, +2):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/554:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] == 21:
        policy.append([0, 1])
        policy.append([1, 2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/555:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/556:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            # Waiting has failed. For all lower numbers, reduce the probability of waiting - odd num
            if index < 100:
                for j in range(2*index + 1, 0, -2):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
            else:
                for j in range(2*index + 1, 200, -2):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
        elif reward == 10:
            # Waiting has worked. For all higher numbers, increase the probability of waiting - even num
            if index < 100:
                for j in range(2*index + 1, 200, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +2):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        # Hitting has failed. For all larger values, reduce prob of hitting. Hitting is the even member
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +2):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        else:
            for j in range(2*index, 620, +2):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/557: results = []
17/558:
for i in range(200000):
    reward = game()
    results.append(reward)
17/559:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/560:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/561:
epsilon = 0.1
policy = []
for state in states:
    if state[1] <= 11 or state[1] < state[2] + 10*(state[2] == 1):
        policy.append([1, 1])
        policy.append([0,2])
    elif state[1] == 21:
        policy.append([0, 1])
        policy.append([1, 2])
    elif state[1] < 17:
        policy.append([1-epsilon, 1])
        policy.append([epsilon, 2])
    else:
        policy.append([epsilon, 1])
        policy.append([1-epsilon, 2])
17/562:
Q = [[0, 0]]*(len(states)*2)
gamma = 0.9
17/563:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if reward == -10:
            # Waiting has failed. For all lower numbers, reduce the probability of waiting - odd num
            if index < 100:
                for j in range(2*index + 1, 0, -20):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
            else:
                for j in range(2*index + 1, 200, -20):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
        elif reward == 10:
            # Waiting has worked. For all higher numbers, increase the probability of waiting - even num
            if index < 100:
                for j in range(2*index + 1, 200, +20):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +20):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        # Hitting has failed. For all larger values, reduce prob of hitting. Hitting is the even member
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        if index < 100:
            for j in range(2*index, 200, +20):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        else:
            for j in range(2*index, 620, +20):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/564: results = []
17/565:
for i in range(200000):
    reward = game()
    results.append(reward)
17/566:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/567:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/568:
for i in range(200000):
    reward = game()
17/569:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/570:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/571:
for i in range(200000):
    reward = game()
17/572:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/573:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/574:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        if reward == -10:
            # Waiting has failed. For all lower numbers, reduce the probability of waiting - odd num
            if index < 100:
                for j in range(2*index + 1, 0, -20):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
            else:
                for j in range(2*index + 1, 200, -20):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
        elif reward == 10:
            # Waiting has worked. For all higher numbers, increase the probability of waiting - even num
            if index < 100:
                for j in range(2*index + 1, 200, +20):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +20):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        # Hitting has failed. For all larger values, reduce prob of hitting. Hitting is the even member
        if index < 100:
            for j in range(2*index, 200, +20):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        else:
            for j in range(2*index, 620, +20):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/575:
for i in range(2000000):
    reward = game()
17/576:
for line in range(100, 310, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/577:
for line in range(0, 100, 10):
    print([" " if 1 + (policy[2*i][0] < policy[2*i + 1][0]) == 1 else "*" for i in range(line, line+10)])
17/578:
def game():
    global policy, Q, states, gamma
    index = np.random.choice(list(range(len(states))))
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    #print(episode_sequence)
    A = action - 1
    if reward != -10:
        reward = check_victory(index)
        if reward == -10:
            if index < 100:
                for j in range(2*index + 1, 0, -20):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
            else:
                for j in range(2*index + 1, 200, -20):
                    policy[j][0] *= 0.9
                    policy[j-1][0] = 1 - policy[j][0]
        elif reward == 10:
            if index < 100:
                for j in range(2*index + 1, 200, +20):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
            else:
                for j in range(2*index + 1, 620, +20):
                    policy[j-1][0] *= 0.9
                    policy[j][0] = 1 - policy[j-1][0]
        return reward
    else:
        if index < 100:
            for j in range(2*index, 200, +20):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        else:
            for j in range(2*index, 620, +20):
                policy[j][0] *= 0.9
                policy[j+1][0] = 1 - policy[j][0]
        return -10
    A = action - 1
    if index not in certain_indices:
        Q_index = 2*index + A
        Q[Q_index] = [(Q[Q_index][0]*Q[Q_index][1] + reward)/(Q[Q_index][1] + 1), Q[Q_index][1] + 1] 
        policy[2*index + 1 - A][0] *= 0.9
        policy[2*index + A][0] = 1 - policy[2*index + 1 - A][0]
    return reward
17/579: stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
17/580: stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
17/581: stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
17/582: stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
17/583:
prob = 0
for i in range(1000):
    prob = (prob*i + reward)/(i+1)
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
17/584:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    #print([policy[2*index][0], policy[2*index+1][0]])
    action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
    reward = 0
    #print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
      #  print([policy[2*index][0], policy[2*index+1][0]])
        action = np.random.choice([1,2], p=[policy[2*index][0], policy[2*index+1][0]])
     #   print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    return reward
17/585:
prob = 0
for i in range(1000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/586: prob
17/587:
epsilon = 0.1
new_policy = []
for index in range(len(states)):
    if policy[2*index] > policy[2*index+1]:
        new_policy.append(1)
    else:
        new_policy.append(2)
17/588:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    #print([policy[2*index][0], policy[2*index+1][0]])
    action = new_policy[index]
    reward = 0
    #print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
      #  print([policy[2*index][0], policy[2*index+1][0]])
        action = new_policy[index]
     #   print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    return reward
17/589:
prob = 0
for i in range(1000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/590: prob
17/591:
prob = 0
for i in range(1000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/592: prob
17/593:
prob = 0
for i in range(1000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/594: prob
17/595:
prob = 0
for i in range(1000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/596: prob
17/597:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/598: prob
17/599:
policies = [
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', '*', ' ', ' ', ' ', ' '],
[' ', '*', '*', '*', '*', '*', '*', '*', ' ', ' '],
[' ', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', '*', '*', ' ', ' ', ' ', ' ', '*', ' ', ' '],
[' ', '*', '*', '*', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', '*', '*', '*', ' ', '*', ' ', ' ', ' '],
['*', ' ', '*', ' ', '*', '*', '*', ' ', ' ', ' '],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
]
17/600: len(policies)
17/601:
epsilon = 0.1
new_policy = []
for index in range(len(states)):
    if policies[int(index/10)][index%10] == ' ':
        new_policy.append(1)
    else:
        new_policy.append(2)
17/602:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    #print([policy[2*index][0], policy[2*index+1][0]])
    action = new_policy[index]
    reward = 0
    #print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
      #  print([policy[2*index][0], policy[2*index+1][0]])
        action = new_policy[index]
     #   print(state, action)
    #print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    return reward
17/603:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/604: prob
17/605: prob
17/606: prob
17/607:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/608: prob
17/609:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/610: prob
17/611:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/612: prob
17/613:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/614: prob
17/615:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/616: prob
17/617:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/618: prob
17/619:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/620: prob
17/621: prob
17/622:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/623: prob
17/624:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/625: prob
17/626:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([policy[2*index][0], policy[2*index+1][0]])
    action = new_policy[index]
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([policy[2*index][0], policy[2*index+1][0]])
        action = new_policy[index]
        print(state, action)
    print(episode_sequence)
    if reward != -10:
        reward = check_victory(index)
    return reward
17/627:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/628:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([policy[2*index][0], policy[2*index+1][0]])
    action = new_policy[index]
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([policy[2*index][0], policy[2*index+1][0]])
        action = new_policy[index]
        print(state, action)
    if reward != -10:
        reward = check_victory(index)
    return reward
17/629:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/630:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/631:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/632:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/633:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/634:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/635:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/636:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/637:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/638:
policies = [
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', '*', '*', ' '],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', ' ', ' ', '*', ' ', ' ', ' '],
[' ', ' ', ' ', ' ', '*', ' ', '*', ' ', ' ', ' '],
[' ', ' ', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
['*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],
]
17/639:
epsilon = 0.1
new_policy = []
for index in range(len(states)):
    if policies[int(index/10)][index%10] == ' ':
        new_policy.append(1)
    else:
        new_policy.append(2)
17/640:
def stimulate_game(sum, dealer_card=10, has_ace = False):
    global states, new_policy, policy
    index = states_index([has_ace, sum, dealer_card])
    state = states[index]
    print([policy[2*index][0], policy[2*index+1][0]])
    action = new_policy[index]
    reward = 0
    print(state, action)
    while action != 2:
        state, result = add_to_sum(index)
        if result == -10:
            reward = -10
            break
        index = states_index(state)
        print([policy[2*index][0], policy[2*index+1][0]])
        action = new_policy[index]
        print(state, action)
    if reward != -10:
        reward = check_victory(index)
    return reward
17/641:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/642:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/643:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/644:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/645:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/646:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/647:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/648:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/649:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/650:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/651:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/652:
prob = 0
for i in range(10000):
    reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
    prob = (prob*i + reward)/(i+1)
17/653: prob
17/654:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/655:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/656:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
17/657:
reward = stimulate_game(np.random.choice(range(12, 22)), dealer_card=np.random.choice(range(1,11)), has_ace = np.random.choice([True, False]))
reward
19/1: # Coding a solution to Tic Tac Toe
19/2:
def is_winning(a, b, c):
    if a[0] == a[1] == a[2]:
        if a[0] == 'X':
            return 1
        else:
            return 2
    if b[0] == b[1] == b[2]:
        if b[0] == 'X':
            return 1
        else:
            return 2
    if c[0] == c[1] == c[2]:
        if c[0] == 'X':
            return 1
        else:
            return 2
    if a[0] == b[0] == c[0]:
        if a[0] == 'X':
            return 1
        else:
            return 2
    if a[1] == b[1] == c[1]:
        if a[1] == 'X':
            return 1
        else:
            return 2
    if a[2] == b[2] == c[2]:
        if a[2] == 'X':
            return 1
        else:
            return 2
    if a[0] == b[1] == c[2]:
        if a[0] == 'X':
            return 1
        else:
            return 2
    if c[0] == b[1] == a[2]:
        if c[0] == 'X':
            return 1
        else:
            return 2
    return 0
19/3:
def is_winning(a, b, c):
    if a[0] == a[1] == a[2]:
        if a[0] == 'X':
            return 1
        elif a[0] == 'O':
            return 2
    if b[0] == b[1] == b[2]:
        if b[0] == 'X':
            return 1
        elif b[0] == 'O':
            return 2
    if c[0] == c[1] == c[2]:
        if c[0] == 'X':
            return 1
        elif c[0] == 'O':
            return 2
    if a[0] == b[0] == c[0]:
        if a[0] == 'X':
            return 1
        elif a[0] == 'O':
            return 2
    if a[1] == b[1] == c[1]:
        if a[1] == 'X':
            return 1
        elif a[1] == 'O':
            return 2
    if a[2] == b[2] == c[2]:
        if a[2] == 'X':
            return 1
        elif a[2] == 'O':
            return 2
    if a[0] == b[1] == c[2]:
        if a[0] == 'X':
            return 1
        elif a[0] == 'O':
            return 2
    if c[0] == b[1] == a[2]:
        if c[0] == 'X':
            return 1
        elif c[0] == '0':
            return 2
    return 0
19/4:
def is_draw(a, b, c):
    if not (any([char == 'X' for char in a]) and any([char == 'O' for char in a])):
        return False
    if not (any([char == 'X' for char in b]) and any([char == 'O' for char in b])):
        return False
    if not (any([char == 'X' for char in c]) and any([char == 'O' for char in c])):
        return False
    if not (any([char[0] == 'X' for char in [a, b, c]]) and any([char[0] == 'O' for char in [a, b, c]])):
        return False
    if not (any([char[1] == 'X' for char in [a, b, c]]) and any([char[1] == 'O' for char in [a, b, c]])):
        return False
    if not (any([char[2] == 'X' for char in [a, b, c]]) and any([char[2] == 'O' for char in [a, b, c]])):
        return False
    if not (any([a[0] == 'X', b[1] == 'X', c[2] == 'X']) and any([a[0] == 'O', b[1] == 'O', c[2] == 'O'])):
        return False
    if not (any([a[2] == 'X', b[1] == 'X', c[0] == 'X']) and any([a[2] == 'O', b[1] == 'O', c[0] == 'O'])):
        return False
    return True
19/5: sequences = [[0,1,2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]
19/6:
def is_winning(given_string):
    global sequences
    for sequence in sequences:
        if given_string[sequence[0]] == given_string[sequence[1]] == given_string[sequence[2]]:
            if given_string[sequence[0]] == 'X':
                return 1
            elif given_string[sequence[0]] == 'O':
                return 2
    return 0
19/7: is_winning("XXX......")
19/8: is_winning("XXO..O..O")
19/9:
def is_draw(given_string):
    global sequences:
    for sequence in sequences:
        if not (any([given_string[index] == 'X' for index in sequence]) and any([given_string[index] == 'O' for index in sequence])):
            return False
    return True
19/10:
def is_draw(given_string):
    global sequences
    for sequence in sequences:
        if not (any([given_string[index] == 'X' for index in sequence]) and any([given_string[index] == 'O' for index in sequence])):
            return False
    return True
19/11: is_draw(XXOOOXXOO)
19/12: is_draw("XXOOOXXOO")
19/13: is_draw("...XOXOXO")
19/14:
def evaluate_pos(given_string, player_num):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], []. []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
19/15:
def evaluate_pos(given_string, player_num):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
19/16:
def evaluate_pos(given_string, player_num):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos
19/17: evaluate_pos('XX.O.O..X)
19/18: evaluate_pos('XX.O.O..X')
19/19:
def evaluate_pos(given_string):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos
19/20: evaluate_pos('XX.O.O..X')
19/21: evaluate_pos('.........')
19/22:
def evaluate_pos(given_string):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    other_pos = [index for index in range(9) if given_string[index] == '.']
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), [],  []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos, [index for index in other_pos if index not in any([p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos])]
19/23: evaluate_pos('.........')
19/24:
def evaluate_pos(given_string):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    other_pos = [index for index in range(9) if given_string[index] == '.']
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), [],  []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos, [index for index in other_pos if not any([index in lis for lis in [p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos]])]
19/25: evaluate_pos('.........')
19/26: states = []
19/27:
def fill_states(n):
    global states
    if n == 1:
        return ["X", "O", "."] 
    res = fill_states(n-1)
    return ["X" + short_string for short_string in res] + ["O" + short_string for short_string in res] + ["." + short_string for short_string in res]
19/28:
def fill_states(n):
    global states
    if n == 1:
        return ["X", "O", "."] 
    res = fill_states(n-1)
    return ["X" + short_string for short_string in res] + ["O" + short_string for short_string in res] + ["." + short_string for short_string in res]
19/29: fill_states(2)
19/30: fill_states(3)
19/31: states = fill_states(9)
19/32: states
19/33: states = [state for state in state if state.count('O') <= state.count('X') <= state.count('O') + 1]
19/34: states = [state for state in states if state.count('O') <= state.count('X') <= state.count('O') + 1]
19/35: states = [state for state in states if not is_winning(state) and if not is_draw(state)]
19/36: states = [state for state in states if not is_winning(state) and not is_draw(state)]
19/37: states
19/38: len(states)
19/39: states
19/40:
rewards = {
    state:
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 1 + 1*(state.count('X') > state.count('O')))
        for action in range(9) if state[action] == "."
    }
    for state in states
} =
19/41:
rewards = {
    state:
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 1 + 1*(state.count('X') > state.count('O')))
        for action in range(9) if state[action] == "."
    }
    for state in states
}
19/42: rewards
19/43:
state = 'XXOXXOOO'
action = 8
state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]
19/44:
rewards = {
    state:
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        for action in range(9) if state[action] == "."
    }
    for state in states
}
19/45: rewards
19/46: rewards
19/47: rewards['XXOX.O...'][8]
19/48:
state = 'XXOX.O...'
action = 8
-5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
19/49:
state = 'XXOX.O...'
action = 8
state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]
-5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
19/50:
state = 'XXOX.O...'
action = 8
print(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:])
-5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
19/51:
state = 'XXOX.O...'
action = 8
print(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]))
-5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
19/52:
policy = {
    state:
    {
        action: 1/len([i for i in range(9) if state[index] == '.'])
        for action in range(9) if state[index] == '.'
    }
    for state in states
}
19/53:
policy = {
    state:
    {
        action: 1/len([i for i in range(9) if state[i] == '.'])
        for action in range(9) if state[action] == '.'
    }
    for state in states
}
19/54: policy
19/55:
import numpy as np
from numpy.random import choice
19/56:
policy = {
    state:
    {
        [1/state.count('.')]*state.count('.')
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    }
    for state in states
}
19/57:
policy = {
    state:
        [1/state.count('.')]*state.count('.')
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/58: gamma = 0.9
19/59:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/60:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else action: -50
        for action in range(9)
    }
    for state in states
}
19/61:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/62: rewards
19/63:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/64:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    episode_sequence = [['.........']]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = state[:action] + 'X' + state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'X'
        else:
            player_num = 1
            player = 'Y'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        reward *= gamma
19/65:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    episode_sequence = [['.........']]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = state[:action] + 'X' + state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'X'
        else:
            player_num = 1
            player = 'Y'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        reward *= gamma
19/66:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    episode_sequence = [['.........']]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = state[:action] + 'X' + state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'X'
        else:
            player_num = 1
            player = 'Y'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/67: game()
19/68:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = [:action] + 'X' + state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'X'
        else:
            player_num = 1
            player = 'Y'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/69:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'X'
        else:
            player_num = 1
            player = 'Y'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/70: game()
19/71:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'X'
        else:
            player_num = 1
            player = 'Y'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/72: game()
19/73: policy
19/74:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/75: game()
19/76:
import numpy as np
from numpy.random import choice
from math import ceil
19/77:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else 1/num*ceil(act) for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0])
        policy[state] = [0.9 if act == A else (1/num)*(act != 0) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/78: game()
19/79: policy
19/80:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/81:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/82:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/83:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/84: policy
19/85:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        policy[state] = [0.9 if act == A else (1/num)*(act != '0') for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        policy[state] = [0.9 if act == A else (1/num)*(act != 0) for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/86: game()
19/87:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        if num:
            policy[state] = [0.9 if act == A else (1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [0.9 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        if num:
            policy[state] = [0.9 if act == A else (1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [0.9 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/88: game()
19/89:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/90:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/91:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/92: policy
19/93: gamma = 0.9
19/94:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        print(state, action, reward)
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [0.9 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        print(state, action, reward)
        action = q_value[1]
        state = q_value[0]
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/95: game()
19/96:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    reward = G
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [0.9 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([act for act in policy[state] if act != 0]) - 1
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/97: game()
19/98:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/99:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/100:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/101: policy
19/102:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/103:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/104:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/105: policy
19/106:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(act != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/107: game()
19/108:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/109:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/110:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/111: policy
19/112:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].items())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/113: game()
19/114:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/115:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/116:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/117: policy
19/118:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].values())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].values())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/119: game()
19/120:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/121:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/122:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/123:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].values())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(rewards[state].values())
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/124: game()
19/125:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/126:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/127:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/128:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        print(policy[state])
        reward *= gamma
    print(episode_sequence)
19/129: game()
19/130:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    print(G)
19/131: game()
19/132: game()
19/133: game()
19/134:
results = []
for i in range(10000):
    results.append(game())
19/135:
results = []
for i in range(10000):
    results.append(game())
19/136:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/137:
results = []
for i in range(10000):
    results.append(game())
19/138: results[-1000:]
19/139: policy
19/140:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/141:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/142:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/143: policy
19/144:
results = []
for i in range(10000):
    results.append(game())
19/145: results[-1000:]
19/146: policy
19/147:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                print(state, action)
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]
                if (
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                print("HERE", state, action)
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/148:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                print(state, action)
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[q_value[0]]]    
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-1::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20 or reward == -5:
            summation = 1 - policy[state][action]
            if summation:
                print("HERE", state, action)
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/149:
results = []
for i in range(10000):
    results.append(game())
19/150:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/151:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/152:
policy = {
    state:
        [0 if state[index] != '.' else 1/state.count('.') for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/153:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = 1 - policy[state][action]
            if summation:
                print("HERE", state, action)
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/154:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=policy[episode_sequence[-1][0]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=policy[episode_sequence[-1][0]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = 1 - policy[state][action]
            if summation:
                policy[state][action] = 0
                policy[state] =  [val/summation for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [0.9 if act == A else (0.1/num)*(policy[state][act] != 0) for act in range(9)]
        else:
            policy[state] = [1 if act == A else 0 for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/155:
results = []
for i in range(10000):
    results.append(game())
19/156: policy
19/157:
policy = {
    state:
        [[0,1] if state[index] != '.' else [1,state.count('.')] for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/158:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[episode_sequence[-1][0]]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[episode_sequence[-1][0]]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/159:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/160:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/161:
policy = {
    state:
        [[0,1] if state[index] != '.' else [1,state.count('.')] for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/162: policy
19/163:
results = []
for i in range(10000):
    results.append(game())
19/164:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/165:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/166:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + 'X' + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        # print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/167:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/168: policy
19/169:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/170:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/171:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/172:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/173: policy['..X....O.']
19/174:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/175:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/176:
policy = {
    state:
        [[0,1] if state[index] != '.' else [1,state.count('.')] for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/177:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/178:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        #print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        #print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        #print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/179:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/180: policy
19/181:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/182:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/183: policy['...X.....']
19/184:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/185:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/186:
policy = {
    state:
        [[0,1] if state[index] != '.' else [1,state.count('.')] for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/187:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i][0] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i][0] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/188:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/189: policy['..X......']
19/190:
rewards = {
    state: 
    {
        action: -5*is_draw(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) - 20*(is_winning(state[:action] + "X"*(state.count('O') == state.count('X')) + "O"*(state.count('O') != state.count('X')) + state[action+1:]) == 2 - 1*(state.count('X') > state.count('O')))
        if state[action]== '.'
        else -50
        for action in range(9)
    }
    for state in states
}
19/191:
reward_counts = {
    state:
    {
        action: 1
        for action in range(9)
    }
    for state in states
}
19/192:
policy = {
    state:
        [[0,1] if state[index] != '.' else [1,state.count('.')] for index in range(9)]
        # action: 1/len([i for i in range(9) if state[i] == '.'])
        # for action in range(9) if state[action] == '.'
    for state in states
}
19/193:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        #print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        #print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i][0] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        #print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i][0] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
19/194:
results = []
for i in range(10000):
    print(i)
    results.append(game())
19/195: results[-1000:]
19/196: policy
19/197: policy['.........']
19/198:
results = []
num_moves = 2000000
for i in range(num_moves):
    if i + 1000 >= num_moves:
        results.append(game())
    else:
        game()
19/199:
new_policy = {
    state: np.argmax([frac[0]/frac[1] for frac in policy[state]])
    for state in states
}
19/200: new_policy
19/201:
def testing_game():
    global new_policy
    player_num = 1
    player = 'X'
    new_state = '.........'
    action = new_policy[new_state]
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        action = new_policy[new_state]
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    if not any([l for l in tup]):
        return True
    else:
        return False
19/202:
new_results = []
new_num_moves = 1000
for i in range(new_num_moves):
    new_results.append(testing_game())
19/203: new_results
19/204: testing_game(True)
19/205:
def testing_game(two_player = False):
    global new_policy
    player_num = 1
    player = 'X'
    new_state = '.........'
    if player == 'X':
        action = new_policy[new_state]
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        if two_player:
            if player == 'X':
                action = new_policy[new_state]
            else:
                print(new_state)
                action = int(input()) - 1
        else:
            action = new_policy[new_state]
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    if not any([l for l in tup]):
        return True
    else:
        return False
19/206: testing_game(True)
19/207: testing_game(True)
19/208: testing_game(True)
19/209: testing_game(True)
19/210: testing_game(True)
19/211: testing_game(True)
19/212: testing_game(True)
19/213: testing_game(True)
19/214: new_policy
19/215: import json
19/216: json.dump(new_results, '\RL from Scratch WIDS\Tic Tac Toe with RL\final_policy.json', indent=4)
19/217: json.dump(new_results, 'final_policy.json', indent=4)
19/218:
with open("RL from Scratch WIDS\Tic Tac Toe with RL\final_policy.json", "w") as file:
    json.dump(new_results, file, indent=4)
19/219:
with open("\RL from Scratch WIDS\Tic Tac Toe with RL\final_policy.json", "w") as file:
    json.dump(new_results, file, indent=4)
19/220:
with open("final_policy.json", "w") as file:
    json.dump(new_results, file, indent=4)
19/221:
with open("final_policy.json", "w") as file:
    json.dump(new_policy, file, indent=4)
19/222:
# Do not execute again. Will overwrite the existing file
new_policy = {
    state: int(action) for state, action in new_policy.keys()
}
with open("final_policy.json", "w") as file:
    json.dump(new_policy, file, indent=4)
19/223:
# Do not execute again. Will overwrite the existing file
new_policy = {
    state: int(action) for state, action in new_policy.items()
}
with open("final_policy.json", "w") as file:
    json.dump(new_policy, file, indent=4)
19/224:
## Do not execute again. Will overwrite the existing file
#new_policy = {
#   state: int(action) for state, action in new_policy.items()
#}
#with open("final_policy.json", "w") as file:
#    json.dump(new_policy, file, indent=4)
19/225:
with open('policy.json', "w") as file:
    json.dump(policy, file, indent = 4)
19/226:
#with open('policy.json', "w") as file:
#    json.dump(policy, file, indent = 4)
19/227: rewards
19/228:
with open('rewards.json', "w") as file:
    json.dump(rewards, file, indent=4)
19/229:
#with open('rewards.json', "w") as file:
#    json.dump(rewards, file, indent=4)
21/1:
import numpy as np
from numpy.random import choice
21/2: card_face_values = list(range(13, 1, -1))
21/3: card_face_values
21/4:
suits = list(range(4, 0,-1))
suits
21/5: suits = list(range(4, 0,-1))
21/6:
possible_cards = []
for suit in suits:
    for card in card_face_values:
        possible_cards.append((card, suit))
21/7: possible_cards
21/8:
states = []
for i in range(52):
    for j in range(i+1, 52):
        states.append(possible_cards[i], possible_cards[j])
21/9:
states = []
for i in range(52):
    for j in range(i+1, 52):
        states.append((possible_cards[i], possible_cards[j]))
21/10: len(possible_cards)
21/11: possible_cards
21/12: card_face_values = list(range(14, 1, -1))
21/13: suits = list(range(4, 0,-1))
21/14:
possible_cards = []
for suit in suits:
    for card in card_face_values:
        possible_cards.append((card, suit))
21/15:
states = []
for i in range(52):
    for j in range(i+1, 52):
        states.append((possible_cards[i], possible_cards[j]))
21/16:
def 7_c_5():
    for i in range(7):
        for j in range(i+1, 7):
            return [k for k in range(7) if k != i and k != j]
21/17:
def c_7_5():
    for i in range(7):
        for j in range(i+1, 7):
            return [k for k in range(7) if k != i and k != j]
21/18: combinations = c_7_5()
21/19:
def c_7_5(list):
    assert(len(list) == 7)
    for i in range(7):
        for j in range(i+1, 7):
            return [list[k] for k in range(7) if k != i and k != j]
21/20: c_7_5(range(7))
21/21:
def c_7_5(list):
    combinations = []
    assert(len(list) == 7)
    for i in range(7):
        for j in range(i+1, 7):
            combinations.append([list[k] for k in range(7) if k != i and k != j])
    return combinations
21/22: c_7_5(range(7))
21/23:
def value_of_hand(cards):
    face_values = [card[0] for card in cards]
    suits = [card[1] for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i][0] for i in range(7) if cards[i][1] == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush or straight_flush:
        return True, list(range(straight_flush, straight_flush-5, -1)
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return True, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return True, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return True, [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return True, list(range(flush, flush-5, -1)
    if straight:
        return True, list(range(straight, straight-5, -1)
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return True, values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return True, values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return True, values
21/24:
def value_of_hand(cards):
    face_values = [card[0] for card in cards]
    suits = [card[1] for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i][0] for i in range(7) if cards[i][1] == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush or straight_flush:
        return True, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return True, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return True, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return True, [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return True, list(range(flush, flush-5, -1)
    if straight:
        return True, list(range(straight, straight-5, -1)
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return True, values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return True, values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return True, values
23/1:
# Step 0 is to import stuff

import gym, gym_walk
import numpy as np
from gym.envs.toy_text.frozen_lake import generate_random_map
23/2:
# Step 0 is to import stuff
!pip install gym
# import gym, gym_walk
# import numpy as np
# from gym.envs.toy_text.frozen_lake import generate_random_map
23/3:
# Step 0 is to import stuff
import gym, gym_walk
import numpy as np
from gym.envs.toy_text.frozen_lake import generate_random_map
23/4:
# Step 0 is to import stuff
!pip install gym-walk
# import gym, gym_walk
# import numpy as np
# from gym.envs.toy_text.frozen_lake import generate_random_map
23/5:
# Step 0 is to import stuff
!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk
# import gym, gym_walk
# import numpy as np
# from gym.envs.toy_text.frozen_lake import generate_random_map
23/6:
# Step 0 is to import stuff
import gym, gym_walk
import numpy as np
from gym.envs.toy_text.frozen_lake import generate_random_map
23/7:
# Step 1 is to get the MDP

env = gym.make('SlipperyWalkFive-v0')
swf_mdp = env.P
# swf_mdp

# Note that in Gym, action "Left" is "0" and "Right" is "1"
23/8:
# Step 2 is to write the policy

pi = {
    0 : 0,
    1 : 0,
    2 : 0,
    3 : 0,
    4 : 0,
    5 : 0,
    6 : 0
}

# Or you can do it randomly
# pi = dict()
# for state in mdp:
#     pi[state] = np.random.choice(mdp[state].keys())
23/9: swf_mdp
23/10:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):
    count = 0
    diff = 10
    while (diff > epsilon):
        count++
        diff = 0
        for state in val.keys():
            initial_reward = val[state]
            val[state] = sum([sum([tup[0]*tup[2] for tup in result]) for action, result in pi[state].items()])
            diff = max(diff, abs(val[state] - initial_reward))
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
23/11:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        for state in val.keys():
            initial_reward = val[state]
            val[state] = sum([sum([tup[0]*tup[2] for tup in result]) for action, result in pi[state].items()])
            diff = max(diff, abs(val[state] - initial_reward))
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/1:
def get_new_value_fn(mdp, pi, gamma = 1.0):
    global val
    new_val = {
        state: sum([sum([tup[0]*(tup[2] + gamma*val[tup[1]]]) for tup in result]) for action, result in mdp[state].items()])
        for state in states
    }
    return new_val
24/2:
def get_new_value_fn(mdp, pi, gamma = 1.0):
    global val
    new_val = {
        state: sum([[sum([tup[0]*(tup[2] + gamma*val[tup[1]]]) for tup in result] for action, result in mdp[state].items()])
        for state in states
    }
    return new_val
24/3:
def get_new_value_fn(mdp, pi, gamma = 1.0):
    global val
    new_val = {
        state: sum([[sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in result]) for action, result in mdp[state].items()])
        for state in states
    }
    return new_val
24/4:
def get_new_value_fn(mdp, pi, gamma = 1.0):
    global val
    new_val = {
        state: sum([sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in result]) for action, result in mdp[state].items()])
        for state in states
    }
    return new_val
24/5:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        for state in val.keys():
            initial_reward = val[state]
            val = get_new_value_fn(mdp, pi, gamma)
            diff = max(diff, abs(val[state] - initial_reward))
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/6:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=1.0):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].keys()
        }
        for state in mdp.keys()
    }
    new_pi = {
        state: np.argmax(list(q[state].values())) + 1
        for state in mdp.keys()
    }
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/7:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    new_val = {
        state: sum([sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in result]) for action, result in mdp[state].items()])
        for state in mdp.keys()
    }
    return new_val
24/8:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        for state in val.keys():
            initial_reward = val[state]
            val = get_new_value_fn(val, mdp, pi, gamma)
            diff = max(diff, abs(val[state] - initial_reward))
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/9:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    pi = {state: 0 for state in mdp}
    new_pi = {state: 1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/10: pi1, val1, count1 = value_iteration(swf_mdp)
24/11:
# Step 0 is to import stuff
import gym, gym_walk
import numpy as np
from gym.envs.toy_text.frozen_lake import generate_random_map
24/12:
# Step 1 is to get the MDP

env = gym.make('SlipperyWalkFive-v0')
swf_mdp = env.P
# swf_mdp

# Note that in Gym, action "Left" is "0" and "Right" is "1"
24/13: swf_mdp
24/14: pi1, val1, count1 = value_iteration(swf_mdp)
24/15:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=1.0):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp.keys()
    }
    new_pi = {
        state: np.argmax(list(q[state].values())) + 1
        for state in mdp.keys()
    }
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/16:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/17:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    pi = {state: 0 for state in mdp}
    new_pi = {state: 1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/18: pi1, val1, count1 = value_iteration(swf_mdp)
24/19: pi1
24/20: val1
24/21: count1
24/22:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/23:
# Step 2 is to write the policy

pi = {
    0 : 0,
    1 : 0,
    2 : 0,
    3 : 0,
    4 : 0,
    5 : 0,
    6 : 0
}

# Or you can do it randomly
# pi = dict()
# for state in mdp:
#     pi[state] = np.random.choice(mdp[state].keys())
24/24:
# Step 3 is computing the value function for this envi and policy

# Let us start with a random value function

val = dict()
for state in swf_mdp:
    val[state] = np.random.random()

# Since 0 and 6 are terminal states, we know their values are 0

val[0] = 0
val[6] = 0

#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using 
# val = dict()
# for state in mdp:
#     val[state] = np.random.random()
#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state
#         val[state] = 0

#instead of doing thsi you can simply intialize the value function to 0 for all states 
# for state in swf_mdp:
#   val[state] = 0
24/25:
# Step 3 is computing the value function for this envi and policy

# Let us start with a random value function

val = dict()
for state in swf_mdp:
    val[state] = 0

# Since 0 and 6 are terminal states, we know their values are 0

val[0] = 0
val[6] = 0

#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using 
# val = dict()
# for state in mdp:
#     val[state] = np.random.random()
#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state
#         val[state] = 0

#instead of doing thsi you can simply intialize the value function to 0 for all states 
# for state in swf_mdp:
#   val[state] = 0
24/26:
# Step 2 is to write the policy

pi = {
    0 : 0,
    1 : 0,
    2 : 0,
    3 : 0,
    4 : 0,
    5 : 0,
    6 : 0
}

# Or you can do it randomly
# pi = dict()
# for state in mdp:
#     pi[state] = np.random.choice(mdp[state].keys())
24/27:
# Step 3 is computing the value function for this envi and policy

# Let us start with a random value function

val = dict()
for state in swf_mdp:
    val[state] = 0

# Since 0 and 6 are terminal states, we know their values are 0

val[0] = 0
val[6] = 0

#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using 
# val = dict()
# for state in mdp:
#     val[state] = np.random.random()
#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state
#         val[state] = 0

#instead of doing thsi you can simply intialize the value function to 0 for all states 
# for state in swf_mdp:
#   val[state] = 0
24/28:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in result]) for action, result in mdp[state].items()])
        for state in mdp.keys()
    }
    return new_val
24/29:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=0.7):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/30:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp.keys()
    }
    new_pi = {
        state: np.argmax(list(q[state].values())) + 1
        for state in mdp.keys()
    }
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/31:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/32:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, gamma=0.7, epsilon=1e-10):
    val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    pi = {state: 0 for state in mdp}
    new_pi = {state: 1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/33: pi1, val1, count1 = value_iteration(swf_mdp)
24/34: pi1
24/35: val1
24/36:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: [sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][pi[state]]]
        for state in mdp.keys()
    }
    return new_val
24/37:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][pi[state]]])
        for state in mdp.keys()
    }
    return new_val
24/38:
# Step 2 is to write the policy

pi = {
    0 : 0,
    1 : 0,
    2 : 0,
    3 : 0,
    4 : 0,
    5 : 0,
    6 : 0
}

# Or you can do it randomly
# pi = dict()
# for state in mdp:
#     pi[state] = np.random.choice(mdp[state].keys())
24/39:
# Step 3 is computing the value function for this envi and policy

# Let us start with a random value function

val = dict()
for state in swf_mdp:
    val[state] = 0

# Since 0 and 6 are terminal states, we know their values are 0

val[0] = 0
val[6] = 0

#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using 
# val = dict()
# for state in mdp:
#     val[state] = np.random.random()
#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state
#         val[state] = 0

#instead of doing thsi you can simply intialize the value function to 0 for all states 
# for state in swf_mdp:
#   val[state] = 0
24/40:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][pi[state]]])
        for state in mdp.keys()
    }
    return new_val
24/41:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=0.7):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/42:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp.keys()
    }
    new_pi = {
        state: np.argmax(list(q[state].values())) + 1
        for state in mdp.keys()
    }
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/43:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/44:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, gamma=0.7, epsilon=1e-10):
    val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    pi = {state: 0 for state in mdp}
    new_pi = {state: 1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/45: pi1, val1, count1 = value_iteration(swf_mdp)
24/46:
# Step 2 is to write the policy

pi = {
    0 : 0,
    1 : 0,
    2 : 0,
    3 : 0,
    4 : 0,
    5 : 0,
    6 : 0
}

# Or you can do it randomly
# pi = dict()
# for state in mdp:
#     pi[state] = np.random.choice(mdp[state].keys())
24/47:
# Step 3 is computing the value function for this envi and policy

# Let us start with a random value function

val = dict()
for state in swf_mdp:
    val[state] = 0

# Since 0 and 6 are terminal states, we know their values are 0

val[0] = 0
val[6] = 0

#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using 
# val = dict()
# for state in mdp:
#     val[state] = np.random.random()
#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state
#         val[state] = 0

#instead of doing thsi you can simply intialize the value function to 0 for all states 
# for state in swf_mdp:
#   val[state] = 0
24/48:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][pi[state]]])
        for state in mdp.keys()
    }
    return new_val
24/49:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=0.7):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/50:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp.keys()
    }
    new_pi = {
        state: np.argmax(list(q[state].values()))
        for state in mdp.keys()
    }
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/51:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/52:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, gamma=0.7, epsilon=1e-10):
    val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    pi = {state: 0 for state in mdp}
    new_pi = {state: 1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/53: pi1, val1, count1 = value_iteration(swf_mdp)
24/54: val1
24/55: pi1
24/56: val1
24/57: count1
24/58: pi1
24/59:
# Step 2 is to write the policy

pi = {
    0 : 0,
    1 : 0,
    2 : 0,
    3 : 0,
    4 : 0,
    5 : 0,
    6 : 0
}

# Or you can do it randomly
# pi = dict()
# for state in mdp:
#     pi[state] = np.random.choice(mdp[state].keys())
24/60:
# Step 3 is computing the value function for this envi and policy

# Let us start with a random value function

val = dict()
for state in swf_mdp:
    val[state] = 0

# Since 0 and 6 are terminal states, we know their values are 0

val[0] = 0
val[6] = 0

#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using 
# val = dict()
# for state in mdp:
#     val[state] = np.random.random()
#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state
#         val[state] = 0

#instead of doing thsi you can simply intialize the value function to 0 for all states 
# for state in swf_mdp:
#   val[state] = 0
24/61:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][pi[state]]])
        for state in mdp.keys()
    }
    return new_val
24/62:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=0.7):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    return val, count
24/63:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp.keys()
    }
    new_pi = {
        state: np.argmax(list(q[state].values()))
        for state in mdp.keys()
    }
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/64:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/65:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, gamma=0.7, epsilon=1e-10):
    val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    pi = {state: 0 for state in mdp}
    new_pi = {state: 1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        print(iteration_count)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/66: pi1, val1, count1 = value_iteration(swf_mdp)
24/67: val1
24/68: pi1
24/69: swf_mdp
24/70:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {state: 0 for state in mdp}, val={s: 0 for s in mdp} gamma=0.7, epsilon=1e-10):
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {state: -1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/71:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {state: 0 for state in mdp}, val={s: 0 for s in mdp}, gamma=0.7, epsilon=1e-10):
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {state: -1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/72:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 0 for state in mdp}
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {state: -1 for state in mdp}
    while any([pi[state] != new_pi[state] for state in mdp]):
        pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/73: pi1, val1, count1 = value_iteration(swf_mdp, pi1, val1)
24/74:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 0 for state in mdp}
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/75: pi1, val1, count1 = value_iteration(swf_mdp, pi1, val1)
24/76: val1
24/77: pi1
24/78:
env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))
mdp2 = env2.P
24/79:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/80: pi2
24/81: mdp2
24/82: count2
24/83: val2
24/84:
terminal_states = [10, 12, 15]
winning_states = [15]
dirs = {0: -1, 1: 4, 2:1, 3: -4}
mdp2 = {
    state: {
        action: [(1, state+dirs[action]*(0<=state + dirs[action]<=15), 1*(state + dirs[action] in winning_states), state+dirs[action]*(0<=state + dirs[action]<=15) in terminal_states)]
        for action in range(4)
    }
    if state not in terminal_states
    else
    {
        action: [(1, state, 0, state in terminal_states)]
    }
    for state in mdp2
}
24/85:
terminal_states = [10, 12, 15]
winning_states = [15]
dirs = {0: -1, 1: 4, 2:1, 3: -4}
mdp2 = {
    state: {
        action: [(1, state+dirs[action]*(0<=state + dirs[action]<=15), 1*(state + dirs[action] in winning_states), state+dirs[action]*(0<=state + dirs[action]<=15) in terminal_states)]
        for action in range(4)
    }
    if state not in terminal_states
    else
    {
        action: [(1, state, 0, state in terminal_states)]
        for action in range(4)
    }
    for state in mdp2
}
24/86: mdp2
24/87:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/88: pi2
24/89:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/90: pi2
24/91:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/92: pi2
24/93: val2
24/94: count2
24/95:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 0 for state in mdp}
    if not val:
        val = {s: 10 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/96:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/97: pi2
24/98: count2
24/99:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 0 for state in mdp}
    if not val:
        val = {s: 10 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        print(val)
        new_pi, q = policy_improvement(val, mdp, gamma)
        print(q)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/100: pi1, val1, count1 = value_iteration(swf_mdp, pi1, val1)
24/101: val1
24/102: pi1
24/103:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/104: pi2
24/105: count2
24/106: val2
24/107:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=0.7):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    print(count)
    return val, count
24/108:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 0 for state in mdp}
    if not val:
        val = {s: 10 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/109:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/110: val2
24/111:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = {
        state: np.argmax(list(q[state].values()))
        for state in mdp
    }
    print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/112:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/113:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 1 for state in mdp}
    if not val:
        val = {s: 10 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/114:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/115: pi2
24/116: val2
24/117:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 1 for state in mdp}
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/118:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/119: pi2
24/120: val2
24/121:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][pi[state]]])
        for state in mdp
    }
    print(new_val[11])
    return new_val
24/122:
#Use to above function to get the new value function, also print how many iterations it took to converge
def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=0.7):
    count = 0
    diff = 10
    while (diff > epsilon):
        count += 1
        diff = 0
        new_val = get_new_value_fn(val, mdp, pi, gamma)
        for state in val.keys():
            diff = max(diff, abs(val[state] - new_val[state]))
        val = {state: value for state, value in new_val.items()}
    # Complete this function to iteratively caluculate the value function until the difference between the new and old value function is less than epsilon
    # Also return the number of iterations it took to converge
    # print(count)
    return val, count
24/123:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = {
        state: np.argmax(list(q[state].values()))
        for state in mdp
    }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/124:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/125:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 1 for state in mdp}
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: action for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: action for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/126:
terminal_states = [10, 12, 15]
winning_states = [15]
dirs = {0: -1, 1: 4, 2:1, 3: -4}
mdp2 = {
    state: {
        action: [(1, state+dirs[action]*(0<=state + dirs[action]<=15), 1*(state + dirs[action] in winning_states), state+dirs[action]*(0<=state + dirs[action]<=15) in terminal_states)]
        for action in range(4)
    }
    if state not in terminal_states
    else
    {
        action: [(1, state, 0, state in terminal_states)]
        for action in range(4)
    }
    for state in mdp2
}
24/127: mdp2
24/128:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/129:
def create_epsilon_greedy_policy(mdp, epsilon=0.1):
    policy = {
        state: [1 if len(actions) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1)]
        for state, actions in mdp.items()
    }
24/130:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][np.random.choice(range(len(mdp[state])), p = pi[state])]))
        for state in mdp
    }
    print(new_val[11])
    return new_val
24/131:
def get_new_value_fn(val, mdp, pi, gamma = 0.7):
    new_val = {
        state: sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][np.random.choice(range(len(mdp[state])), p = pi[state])]])
        for state in mdp
    }
    print(new_val[11])
    return new_val
24/132:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state]).values())
        if len(q[state) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(actions) - 1) for action in actions.keys()]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/133:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state]).values())
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(actions) - 1) for action in actions.keys()]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/134:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = {state: 1 for state in mdp}
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: [val for val in action] for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: np.argmax(action) for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/135:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/136:
def create_epsilon_greedy_policy(mdp, epsilon=0.1):
    policy = {
        state: [1 if len(actions) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1)]
        for state, actions in mdp.items()
    }
    return policy
24/137:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = create_epsilon_greedy_policy(mdp)
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: [val for val in action] for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: np.argmax(action) for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/138:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/139:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state]).values())
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(actions) - 1) for action in actions.keys()]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/140:
# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge
# Create a random policy and value function to start with or use the ones defined above
def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):
    global pi, val
    count = 0
    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge
    return pi, val, count
24/141:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = create_epsilon_greedy_policy(mdp)
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: [val for val in action] for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
    pi = {state: np.argmax(action) for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/142:
def create_epsilon_greedy_policy(mdp, epsilon=0.1):
    policy = {
        state: [1 if len(actions) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    return policy
24/143:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/144:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state]).values())
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(actions) - 1) for action in actions.keys()]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/145:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state].values())
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(actions) - 1) for action in actions.keys()]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/146:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state].values()))
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(actions) - 1) for action in actions.keys()]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/147:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/148:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state].values()))
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(q[state]) - 1) for action in q[state]]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/149:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/150:
# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary
def policy_improvement(val, mdp, gamma=0.7, epsilon=0.1):
    q = {
        state: {
            action: sum([tup[0]*val[tup[1]] for tup in result])
            for action, result in mdp[state].items()
        }
        for state in mdp
    }
    new_pi = dict()
    for state in q:
        a = np.argmax(list(q[state].values()))
        if len(q[state]) == 1:
            new_pi[state] = [1]
        else:
            new_pi[state] = [1 - epsilon if action == a else epsilon/(len(q[state]) - 1) for action in q[state]]
    new_pi = {
        state: [1 if len(q[state]) == 1 else 1 - epsilon if action == list(actions.keys())[0] else epsilon/(len(actions)-1) for action in actions.keys()]
        for state, actions in mdp.items()
    }
    # new_pi = {
    #     state: np.argmax(list(q[state].values()))
    #     for state in mdp
    # }
    # print(new_pi)
    # Complete this function to get the new policy given the value function and the mdp
    return new_pi, q
24/151:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/152: pi2
24/153: val2
24/154:
#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge
def value_iteration(mdp, pi = {}, val={}, gamma=0.7, epsilon=1e-10):
    if not pi:
        pi = create_epsilon_greedy_policy(mdp)
    if not val:
        val = {s: 0 for s in mdp}
    count = 0
    q = {s: {act: 0 for act in action} for s, action in mdp.items()}
    diff = 10
    new_pi = {}
    while any([pi.get(state) != new_pi.get(state) for state in mdp]):
        if new_pi:
            pi = {state: [val for val in action] for state, action in new_pi.items()}
        count += 1
        diff = 0
        val, iteration_count = policy_evaluation(val, mdp, pi, epsilon, gamma)
        new_pi, q = policy_improvement(val, mdp, gamma)
        print(new_pi)
    pi = {state: np.argmax(action) for state, action in new_pi.items()}
    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge
    return pi, val, count
24/155:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/156:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/157: pi2
24/158:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/159: pi2
24/160:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/161: pi2
24/162:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/163: pi2
24/164:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/165: pi2
24/166:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/167: pi2
24/168:
# pi1, val1, count1 = policy_iteration(mdp2)
pi2, val2, count2 = value_iteration(mdp2)
24/169: pi2
26/1:
# Step 0 is to import stuff
import gym, gym_walk
import numpy as np
from gym.envs.toy_text.frozen_lake import generate_random_map
26/2:
# Step 1 is to get the MDP

env = gym.make('SlipperyWalkFive-v0')
swf_mdp = env.P
# swf_mdp

# Note that in Gym, action "Left" is "0" and "Right" is "1"
26/3:
terminal_states = [0, 6]
swf_mdp = {
    state: {
        action:
        [(1, state - 1*(action==0), 1*(state - 1*(action==0) == terminal_states[-1]), state - 1*(action==0) in terminal_states))]
        for action in range(2)
    }
    if state not in terminal_states
    else
        {action: [(1, state, 0, True)]
    for action in range(2)}
    for state in range(7)
}
26/4:
terminal_states = [0, 6]
swf_mdp = {
    state: {
        action:
        [(1, state - 1*(action==0), 1*(state - 1*(action==0) == terminal_states[-1]), state - 1*(action==0) in terminal_states)]
        for action in range(2)
    }
    if state not in terminal_states
    else
        {action: [(1, state, 0, True)]
    for action in range(2)}
    for state in range(7)
}
26/5: swf_mdp
26/6:
policy = {
    state: 0
    for state in swf_mdp
}
26/7:
val = {
    state: 0
    for state in swf_mdp
}
26/8:
val = {
    state: 10
    if state not in terminal_states
    else 0
    for state in swf_mdp
}
26/9:
policy = {
    state: {action: 1/len(actions) for action in actions}
    for state, actions in swf_mdp.items()
}
26/10:
values = {
    state: 10
    if state not in terminal_states
    else 0
    for state in swf_mdp
}
26/11:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    while delta > epsilon:
        delta = 0
        for state in swf_mdp:
            prev = old_val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val
26/12:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = old_val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/13:
def policy_improvement(val, mdp, gamma=1.0):
    pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
26/14:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/15:
def policy_improvement(val, mdp, gamma=1.0):
    pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action]
26/16:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*gamma*val[tup[1]] for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/17:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while count and policy != new_policy:
        count += 1
        policy = {
            state: {
                action: prob for action, prob in results.items()
            }
            for state, results in policy.items()
        }
        values, count1 = get_new_value_fn(values, swf_mdp, policy)
        new_policy, q = policy_improvement(values, swf_mdp)
    return policy, values, count
26/18: p1, val1, c1 = value_iteration(swf_mdp)
26/19: p1
26/20:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy:
        if count:
            policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/21: p1, val1, c1 = value_iteration(swf_mdp)
26/22: p1
26/23:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/24: p1, val1, c1 = value_iteration(swf_mdp)
26/25:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print("HERE")
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*gamma*val[tup[1]] for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/26:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/27: p1, val1, c1 = value_iteration(swf_mdp)
26/28:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*gamma*val[tup[1]] for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/29:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*val[tup[1]] for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/30:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/31:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/32: p1, val1, c1 = value_iteration(swf_mdp)
26/33:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/34:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/35: p1, val1, c1 = value_iteration(swf_mdp)
26/36:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        print(delta)
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/37:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/38: p1, val1, c1 = value_iteration(swf_mdp)
26/39:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        print(count)
        print(delta)
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/40:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/41: p1, val1, c1 = value_iteration(swf_mdp)
26/42: p1
26/43:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/44: p1, val1, c1 = value_iteration(swf_mdp)
26/45: p1
26/46:
terminal_states = [0, 6]
swf_mdp = {
    state: {
        action:
        [(1, state + 1 - 2*(action==0), 1*(state + 1 - 2*(action==0) == terminal_states[-1]), state + 1- 2*(action==0) in terminal_states)]
        for action in range(2)
    }
    if state not in terminal_states
    else
        {action: [(1, state, 0, True)]
    for action in range(2)}
    for state in range(7)
}
26/47: swf_mdp
26/48:
policy = {
    state: {action: 1/len(actions) for action in actions}
    for state, actions in swf_mdp.items()
}
26/49:
values = {
    state: 10
    if state not in terminal_states
    else 0
    for state in swf_mdp
}
26/50:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/51:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        print(count)
        print(delta)
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/52:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/53: p1, val1, c1 = value_iteration(swf_mdp)
26/54: p1
26/55:
# Step 1 is to get the MDP

env = gym.make('SlipperyWalkFive-v0')
swf_mdp = env.P
# swf_mdp

# Note that in Gym, action "Left" is "0" and "Right" is "1"
26/56: swf_mdp
26/57:
policy = {
    state: {action: 1/len(actions) for action in actions}
    for state, actions in swf_mdp.items()
}
26/58:
values = {
    state: 10
    if state not in terminal_states
    else 0
    for state in swf_mdp
}
26/59:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/60:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        print(count)
        print(delta)
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/61:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    return policy, values, count
26/62: p1, val1, c1 = value_iteration(swf_mdp)
26/63: p1
26/64:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    global values, policy
    new_policy = {
        state: {
            action: prob for action, prob in results.items()
        }
        for state, results in policy.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(actions.values())
        for state, actions in policy.items()
    } 
    return policy, values, count
26/65: p1, val1, c1 = value_iteration(swf_mdp)
26/66: p1
26/67: p1, val1, c1 = value_iteration(swf_mdp)
26/68: p1
26/69:
# Step 1 is to get the MDP

env = gym.make('SlipperyWalkFive-v0')
swf_mdp = env.P
# swf_mdp

# Note that in Gym, action "Left" is "0" and "Right" is "1"
26/70: swf_mdp
26/71:
policy = {
    state: {action: 1/len(actions) for action in actions}
    for state, actions in swf_mdp.items()
}
26/72:
values = {
    state: 0
    for state in swf_mdp
}
26/73:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/74:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in swf_mdp[state]}
        for state in swf_mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        print(count)
        print(delta)
        count += 1
        delta = 0
        for state in swf_mdp:
            for action in swf_mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in swf_mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/75:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(actions.values())
        for state, actions in policy.items()
    } 
    return policy, values, count
26/76: p1, val1, c1 = value_iteration(swf_mdp)
26/77: p1
26/78:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    print(policy)
    policy = {
        state: np.argmax(actions.values())
        for state, actions in policy.items()
    } 
    return policy, values, count
26/79: p1, val1, c1 = value_iteration(swf_mdp)
26/80:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(list(actions.values()))
        for state, actions in policy.items()
    } 
    return policy, values, count
26/81: p1, val1, c1 = value_iteration(swf_mdp)
26/82: p1
26/83:
env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))
mdp2 = env2.P
26/84: p2, val2, c2 = value_iteration(mdp2)
26/85: p2
26/86:
env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))
mdp2 = env2.P
mdp2
26/87: val2
26/88:
env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))
mdp2 = env2.P
mdp2
26/89: p2, val2, c2 = value_iteration(mdp2)
26/90: p2
26/91: val2
26/92:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in mdp[state]}
        for state in mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in mdp:
            for action in mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/93:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(list(actions.values()))
        for state, actions in policy.items()
    } 
    return policy, values, count
26/94: p1, val1, c1 = value_iteration(swf_mdp)
26/95: p1
26/96:
env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))
mdp2 = env2.P
mdp2
26/97: p2, val2, c2 = value_iteration(mdp2)
26/98: p2
26/99: val2
26/100:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in mdp[state]}
        for state in mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(q)
        print(count)
        print(delta)
        count += 1
        delta = 0
        for state in mdp:
            for action in mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/101:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(list(actions.values()))
        for state, actions in policy.items()
    } 
    return policy, values, count
26/102: p1, val1, c1 = value_iteration(swf_mdp)
26/103: p1
26/104: p2, val2, c2 = value_iteration(mdp2)
26/105:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in mdp[state]}
        for state in mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        print(val)
        print(q)
        print(count)
        print(delta)
        count += 1
        delta = 0
        for state in mdp:
            for action in mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/106:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(list(actions.values()))
        for state, actions in policy.items()
    } 
    return policy, values, count
26/107: p1, val1, c1 = value_iteration(swf_mdp)
26/108: p2, val2, c2 = value_iteration(mdp2)
26/109:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in mdp[state]}
        for state in mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in mdp:
            for action in mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
26/110:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print(val[state])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/111: p2, val2, c2 = value_iteration(mdp2)
26/112: p2
26/113: val2
26/114:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/115: p2, val2, c2 = value_iteration(mdp2)
26/116:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print([sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/117: p2, val2, c2 = value_iteration(mdp2)
26/118:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print([prob for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/119: p2, val2, c2 = value_iteration(mdp2)
26/120:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print([prob*sum([tup[0]*tup[2] for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/121: p2, val2, c2 = value_iteration(mdp2)
26/122:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print([prob*sum([tup[2] for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/123: p2, val2, c2 = value_iteration(mdp2)
26/124:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print(mdp[state][1][2])
            print([prob*sum([tup[2] for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/125: p2, val2, c2 = value_iteration(mdp2)
26/126:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print(mdp[state][1][0][2])
            print([prob*sum([tup[2] for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/127: p2, val2, c2 = value_iteration(mdp2)
26/128:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in swf_mdp:
            prev = val[state]
            print(mdp[state][1])
            print([prob*sum([tup[2] for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/129: p2, val2, c2 = value_iteration(mdp2)
26/130:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in mdp:
            prev = val[state]
            print([prob*sum([tup[2] for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/131:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in mdp:
            prev = val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
26/132: p1, val1, c1 = value_iteration(swf_mdp)
26/133: p1
26/134:
env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))
mdp2 = env2.P
mdp2
26/135: p2, val2, c2 = value_iteration(mdp2)
26/136: p2
26/137: val2
27/1:
# Step 0 is to import stuff
import gym, gym_walk
import numpy as np
from gym.envs.toy_text.frozen_lake import generate_random_map
27/2:
# Step 1 is to get the MDP

env = gym.make('SlipperyWalkFive-v0')
swf_mdp = env.P
# swf_mdp

# Note that in Gym, action "Left" is "0" and "Right" is "1"
27/3: new_frozen_lake_mdp = gym.make('FrozenLake-v1').env.P
27/4: new_frozen_lake_mdp
27/5:
def get_new_value_fn(val, mdp, pi, gamma = 1.0):
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in mdp:
            prev = val[state]
            val[state] = sum([prob*sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]]) for action, prob in pi[state].items()])
            delta = max(delta, abs(val[state] - prev))
    return val, count
27/6:
def policy_improvement(val, mdp, gamma=1.0):
    new_pi = dict()
    q = {
        state: {action: 0 for action in mdp[state]}
        for state in mdp
    }
    epsilon = 1e-10
    delta = 10
    count = 0
    while delta > epsilon:
        count += 1
        delta = 0
        for state in mdp:
            for action in mdp[state]:
                prev = q[state][action]
                q[state][action] = sum([tup[0]*(tup[2] + gamma*val[tup[1]]) for tup in mdp[state][action]])
                delta = max(delta, abs(q[state][action] - prev))
    epsilon = 0.1
    for state, results in mdp.items():
        if len(results) == 1:
            new_pi[state] = list(results.keys())[0]
        else:
            a = np.argmax(list(q[state].values()))
            new_pi[state] = {
                action: 1 - epsilon if action == a
                else epsilon/(len(results) - 1)
                for action in results
            }
    return new_pi, q
27/7:
def value_iteration(mdp, gamma=1.0, epsilon=1e-10):
    values = {
        state: 0
        for state in mdp
    }
    policy = {}
    new_policy = {
        state: {action: 1/len(actions) for action in actions}
        for state, actions in mdp.items()
    }
    count = 0
    while policy != new_policy or not count:
        policy = {
                state: {
                    action: prob for action, prob in results.items()
                }
                for state, results in new_policy.items()
            }
        count += 1
        values, count1 = get_new_value_fn(values, mdp, policy)
        new_policy, q = policy_improvement(values, mdp)
    policy = {
        state: np.argmax(list(actions.values()))
        for state, actions in policy.items()
    } 
    return policy, values, count
27/8: p3, val3, count3 = value_iteration(new_frozen_lake_mdp)
27/9: p3
27/10: val3
29/1:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = numpy.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        reward = np.random.binomial(1, self.probs[k-1])
        self.reward += self.optimal - reward
29/2:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = numpy.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.reward += self.optimal - reward
29/3:
# generating the underlying probability distribution

probs = np.random.random(2)
29/4:
# importing necessary stuff
import numpy as np
from pprint import pprint
from tqdm.notebook import tqdm

# if you want to use envs from Gym, import it
# import gym, gym_bandits
29/5:
# importing necessary stuff
!pip install tqdm
# import numpy as np
# from pprint import pprint
# from tqdm.notebook import tqdm

# if you want to use envs from Gym, import it
# import gym, gym_bandits
29/6:
# importing necessary stuff
import numpy as np
from pprint import pprint
from tqdm.notebook import tqdm

# if you want to use envs from Gym, import it
# import gym, gym_bandits
29/7:
# generating the underlying probability distribution

probs = np.random.random(2)
29/8:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = numpy.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.reward += self.optimal - reward
29/9:
# our MDP is a function which takes an action and returns a reward

def mab_2_env(action):
    # This is already achieved by the evaluate_choice function above
    gen = np.random.random()

    # for bernoulli bandits, the reward is 1 if the random number is less than the probability of success, else 0
    return gen < probs[action]
29/10:
# function to calculate total regret

def regret(rewards, probs):

    return len(rewards)*np.max(probs) - sum(rewards)

    raise NotImplementedError()
29/11:
# exponentially decaying epsilon greedy strategy

def exponentially_decaying_epsilon_greedy(Q, epsilon, gamma):
    epsilon *= gamma
    if np.random.random() < epsilon:
        return np.random.randint(1, len(Q) + 1), epsilon
    return np.argmax(Q), epsilon
29/12:
# softmax action selection strategy

def softmax_strategy(Q, temperature):
    
    choice_probabilities = [exp(val/temperature) for val in Q]
    choice_probabilities = choice_probabilities/sum(choice_probabilities)

    return np.random.choice(range(1, len(Q)+1), p=choice_probabilities)
29/13: import math
29/14: from math import log
29/15:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(sum, base=N[i])) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    N[choice] += 1
    return choice
29/16:
# epsilon-greedy strategy
# Already implemented for you coz I am nice

def epsilon_greedy(Q, epsilon):
    if np.random.random() < epsilon:
        return np.random.randint(1, len(Q) + 1)
    else:
        return np.argmax(Q) + 1
29/17:
# import plotting libraries

import matplotlib.pyplot as plt
29/18:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = numpy.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.reward += self.optimal - reward
29/19:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = numpy.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.reward += self.optimal - reward
        return reward
29/20:
n = 2
gamma = 0.9
gamma_power = 0.9
Q = np.zeros(n)
bandit = Bernoulli_bandit(n)
for episode in range(1000):
    choice = epsilon_greedy(Q, 0.1)
    Q[choice] += gamma_power*bandit.evaluate_choice(choice)
    gamma_power *= gamma

print(bandit.regret)
print(bandit.probs)
29/21:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = np.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.reward += self.optimal - reward
        return reward
29/22:
n = 2
gamma = 0.9
gamma_power = 0.9
Q = np.zeros(n)
bandit = Bernoulli_bandit(n)
for episode in range(1000):
    choice = epsilon_greedy(Q, 0.1)
    Q[choice] += gamma_power*bandit.evaluate_choice(choice)
    gamma_power *= gamma

print(bandit.regret)
print(bandit.probs)
29/23:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = np.random.normal(self.mean, 1, (n,))
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.regret += self.optimal - reward
        return reward
29/24:
n = 2
gamma = 0.9
gamma_power = 0.9
Q = np.zeros(n)
bandit = Bernoulli_bandit(n)
for episode in range(1000):
    choice = epsilon_greedy(Q, 0.1)
    Q[choice] += gamma_power*bandit.evaluate_choice(choice)
    gamma_power *= gamma

print(bandit.regret)
print(bandit.probs)
29/25:
n = 2
gamma = 0.9
gamma_power = 0.9
Q = np.zeros(n)
bandit = Bernoulli_bandit(n)
for episode in range(1000):
    choice = epsilon_greedy(Q, 0.1)
    Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
    gamma_power *= gamma

print(bandit.regret)
print(bandit.probs)
29/26: np.random.rand(n)
29/27:
class Bernoulli_bandit():
    def __init__(self, n):
        self.N = n
        self.mean = np.random.random()
        self.probs = np.random.normal(self.mean, 1, (n,))
        self.probs = np.random.rand(n)
        self.optimal = np.max(self.probs)
        self.regret = 0.0
        
    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.binomial(1, self.probs[k-1])
        self.regret += self.optimal - reward
        return reward
29/28:
n = 2
gamma = 0.9
gamma_power = 0.9
Q = np.zeros(n)
bandit = Bernoulli_bandit(n)
for episode in range(1000):
    choice = epsilon_greedy(Q, 0.1)
    Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
    gamma_power *= gamma

print(bandit.regret)
print(bandit.probs)
29/29:
n = 2
gamma = 0.9
gamma_power = 0.9
Q = np.zeros(n)
bandit = Bernoulli_bandit(n)
choices = []
for episode in range(1000):
    choice = epsilon_greedy(Q, 0.1)
    choices.append(choice)
    Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
    gamma_power *= gamma

print(bandit.regret)
print(bandit.probs)
29/30: choices
29/31:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/32: regrets
29/33: plt.plot(epsilons, regrets)
29/34: regrets
29/35: plt.plot(epsilons, regrets)
29/36:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/37: plt.plot(epsilons, regrets)
29/38:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/39: plt.plot(epsilons, regrets)
29/40:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/41: plt.plot(epsilons, regrets)
29/42: len(regrets)
29/43: regrets
29/44: bandit.probs
29/45:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/46: values
29/47:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/48: values
29/49:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/50: values
29/51:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/52: values
29/53:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.5]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/54: values
29/55:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/56: values
29/57:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/58: values
29/59:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/60: values
29/61:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/62: values
29/63: band.probs
29/64:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/65: band.probs
29/66: values
29/67: band.probs
29/68: values
29/69:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/70: band.probs
29/71: values
29/72:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/73: band.probs
29/74: values
29/75:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/76: band.probs
29/77: values
29/78:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/79: band.probs
29/80: values
29/81: band.probs
29/82: values
29/83:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/84: band.probs
29/85: values
29/86: band.probs
29/87:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
band = Bernoulli_bandit(n)
for epsilon in epsilon_values:
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    values.append(band.regret)
29/88: band.probs
29/89: values
29/90:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
rewards = []
for i in range(50):
    band = Bernoulli_bandit(n)
    epsilon = epsilon_values[0]
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    rewards.append(band.regret)
    print(abs(band.probs[0] - band.probs[1])/band.regret)
29/91: regrets
29/92: rewards
29/93:
# epsilons = np.linspace(0, 1, 101)
epsilon_values = [0.1]
values = []
rewards = []
results = []
for i in range(50):
    band = Bernoulli_bandit(n)
    epsilon = epsilon_values[0]
    n = 2
    band.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*band.evaluate_choice(choice)
        gamma_power *= gamma
    rewards.append(band.regret)
    results.append(abs(band.probs[0] - band.probs[1])/band.regret)
29/94: results
29/95: rewards
29/96:
print(results)
print(np.std(results))
print(np.mean(results))
29/97:
print(results)
print(np.std(results))
print(np.mean(results))
print(np.std(results)/np.mean(results))
29/98:
print(np.std(rewards))
print(np.mean(rewards))
print(np.std(rewards)/np.mean(rewards))
29/99: np.random.rand(2)
29/100:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.85510268, 0.99764477])
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/101: plt.plot(epsilons, regrets)
29/102:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.85510268, 0.99764477])
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/103: plt.plot(epsilons, regrets)
29/104:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.85510268, 0.99764477])
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/105: plt.plot(epsilons, regrets)
29/106:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.85510268, 0.99764477])
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/107: plt.plot(epsilons, regrets)
29/108:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.85510268, 0.99764477])
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/109: plt.plot(epsilons, regrets)
29/110:
epsilons = np.linspace(0, 1, 101)
regrets = []
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.85510268, 0.99764477])
    n = 2
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/111: plt.plot(epsilons, regrets)
29/112: random_probs = np.random.rand(5)
29/113:
random_probs = np.random.rand(5)
random_probs
29/114:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.83231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/115: plt.plot(epsilons, regrets)
29/116:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.83231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/117: plt.plot(epsilons, regrets)
29/118: plt.scatter(epsilons, regrets)
29/119:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.83231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    regrets.append(bandit.regret)
29/120: plt.scatter(epsilons, regrets)
29/121:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.83231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/122: plt.scatter(epsilons, regrets)
29/123: regrets
29/124:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.83231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    print(choice)
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/125: plt.scatter(regrets)
29/126: plt.scatter(epsilons, regrets)
29/127:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.99231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    print(choice)
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/128: plt.scatter(epsilons, regrets)
29/129:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.99231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] += gamma_power*bandit.evaluate_choice(choice)
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    print(choice)
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/130: plt.scatter(epsilons, regrets)
29/131:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.probs = np.array([0.0819915 , 0.30676123, 0.8593676 , 0.99231319, 0.82083977])
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    N  = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    print(choice)
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/132: plt.scatter(epsilons, regrets)
29/133:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    N  = np.zeros(n)
    for episode in range(100000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    print(choice)
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/134: plt.scatter(epsilons, regrets)
29/135:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    N  = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    print(choice)
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/136:
def implement_softmax(temperature, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = softmax_strategy(Q, temperature)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/137:
def implement_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/138:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilon, gamma)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/139:
def implement_ucb(c, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = ucb(Q, N, c)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/140:
# thompson sampling strategy

def thompson_sampling(Q, N, alpha, beta):
    samples = np.random.normal(loc=Q, scale = alpha/(np.sqrt(N) + beta))
    action = np.argmax(samples)
    # implement the thompson sampling strategy here

    raise NotImplementedError()
29/141:
# thompson sampling strategy

def thompson_sampling(Q, N, alpha, beta):
    samples = np.random.normal(loc=Q, scale = alpha/(np.sqrt(N) + beta))
    choice = np.argmax(samples) + 1
    # implement the thompson sampling strategy here
    return choice
29/142:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 1, 0)
29/143:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 1, 0)
29/144:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 1, 0)
29/145:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 1, 0)
29/146:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/147:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/148:
# thompson sampling strategy

def thompson_sampling(Q, N, alpha, beta):
    samples = np.random.normal(loc=Q, scale = alpha/(np.sqrt(N) + beta))
    print(samples)
    choice = np.argmax(samples) + 1
    # implement the thompson sampling strategy here
    return choice
29/149:
Q = np.array([1, 2, 3])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/150:
Q = np.array([1, 1.1, 0.9])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/151:
Q = np.array([1, 1.1, 0.9])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/152:
Q = np.array([1, 1.1, 0.9])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/153:
Q = np.array([1, 1.1, 0.9])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/154:
Q = np.array([1, 1.1, 0.9])
N = np.array([50, 100, 140])
thompson_sampling(Q, N, 0.5, 0.5)
29/155:
def implement_thompson_sampling(alpha=1, beta=0, gamma = 0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = thompson_sampling(Q, N, alpha, beta)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/156: bandit = Bernoulli_bandit(10)
29/157: bandit.probs
29/158:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    for episode in range(1000):
        evaluate_choice(choice)
    regrets.append(bandit.regret)
29/159: bandit = Bernoulli_bandit(10)
29/160: bandit.probs
29/161:
def implement_epsilon_greedy(epsilon=0.1, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/162:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    for episode in range(1000):
        evaluate_choice(choice)
    regrets.append(bandit.regret)
29/163:
# exponentially decaying epsilon greedy strategy

def exponentially_decaying_epsilon_greedy(Q, epsilon, gamma):
    epsilon *= gamma
    if np.random.random() < epsilon:
        return np.random.randint(1, len(Q) + 1), epsilon
    return np.argmax(Q) + 1, epsilon
29/164:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    for episode in range(1000):
        evaluate_choice(choice)
    regrets.append(bandit.regret)
29/165:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    for episode in range(1000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/166: plt.scatter(epsilons, regrets)
29/167:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/168: plt.scatter(epsilons, regrets)
29/169:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/170:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/171: plt.scatter(epsilons, regrets)
29/172:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/173:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.99, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilonn)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/174:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/175:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.99, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/176:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/177:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.99, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilon, gamma)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/178:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilon, 0.99)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/179:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/180: plt.scatter(epsilons, regrets)
29/181:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilon, 1)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/182:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/183: plt.scatter(epsilons, regrets)
29/184:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice, epsilon = exponentially_decaying_epsilon_greedy(Q, epsilon, 0.99)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/185:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    max_epsilon = epsilon
    min_epsilon = 0.01*epsilon
    decay_rate = 0.05
    decay_episodes = num_episodes*decay_rate
    remaining_episodes = num_episodes - decay_episodes
    epsilons = 0.01
    epsilons /= np.logspace(log10(min_epsilon/epsilon), 0, decay_episodes)
    epsilons *= max_epsilon - min_epsilon
    epsilons += min_epsilon
    epsilons = np.pad(epsilons, (0, remaining_episodes), mode='constant', constant_values=min_epsilon)
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilons[episode])
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
        
    return np.argmax(Q) + 1
29/186:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/187:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    max_epsilon = epsilon
    min_epsilon = 0.01*epsilon
    decay_rate = 0.05
    decay_episodes = num_episodes*decay_rate
    remaining_episodes = num_episodes - decay_episodes
    epsilons = 0.01
    epsilons /= np.logspace(-2, 0, decay_episodes)
    epsilons *= max_epsilon - min_epsilon
    epsilons += min_epsilon
    epsilons = np.pad(epsilons, (0, remaining_episodes), mode='constant', constant_values=min_epsilon)
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilons[episode])
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
        
    return np.argmax(Q) + 1
29/188:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/189:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    max_epsilon = epsilon
    min_epsilon = 0.01*epsilon
    decay_rate = 0.05
    decay_episodes = int(num_episodes*decay_rate)
    remaining_episodes = num_episodes - decay_episodes
    epsilons = 0.01
    epsilons /= np.logspace(-2, 0, decay_episodes)
    epsilons *= max_epsilon - min_epsilon
    epsilons += min_epsilon
    epsilons = np.pad(epsilons, (0, remaining_episodes), mode='constant', constant_values=min_epsilon)
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilons[episode])
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
        
    return np.argmax(Q) + 1
29/190:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/191: plt.scatter(epsilons, regrets)
29/192: from math import log, log10
29/193:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    max_epsilon = epsilon
    min_epsilon = 0.01*epsilon
    decay_rate = 0.05
    decay_episodes = int(num_episodes*decay_rate)
    remaining_episodes = num_episodes - decay_episodes
    epsilons = 0.01
    epsilons /= np.logspace(log10(min_epsilon/max_epsilon), 0, decay_episodes)
    epsilons *= max_epsilon - min_epsilon
    epsilons += min_epsilon
    epsilons = np.pad(epsilons, (0, remaining_episodes), mode='constant', constant_values=min_epsilon)
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilons[episode])
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
        
    return np.argmax(Q) + 1
29/194:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/195: plt.scatter(epsilons, regrets)
29/196: log10(min_epsilon/max_epsilon)
29/197:
def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):
    global bandit
    min_to_max_ratio = 0.01
    max_epsilon = epsilon
    min_epsilon = min_to_max_ratio*epsilon
    decay_rate = 0.05
    decay_episodes = int(num_episodes*decay_rate)
    remaining_episodes = num_episodes - decay_episodes
    epsilons = 0.01
    epsilons /= np.logspace(log10(min_to_max_ratio), 0, decay_episodes)
    epsilons *= max_epsilon - min_epsilon
    epsilons += min_epsilon
    epsilons = np.pad(epsilons, (0, remaining_episodes), mode='constant', constant_values=min_epsilon)
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.zeros(bandit.N)
    for episode in range(num_episodes):
        choice = epsilon_greedy(Q, epsilons[episode])
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
        
    return np.argmax(Q) + 1
29/198:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/199: plt.scatter(epsilons, regrets)
29/200:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/201: plt.scatter(epsilons, regrets)
29/202:
plt.ylabel('Regret')
plt.title('Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/203:
epsilons = np.linspace(0, 1, 101)
regrets = []
n = 5
bandit = Bernoulli_bandit(n)
for epsilon in epsilons:
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    N  = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/204:
plt.ylabel('Regret')
plt.title('Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/205:
plt.ylabel('Regret')
plt.xlabel('Epsilon')
plt.title('Exponentially Decaying Epsilon Greedy POlicy')
plt.scatter(epsilons, regrets)
29/206:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/207:
plt.ylabel('Regret')
plt.xlabel('Epsilon')
plt.title('Exponentially Decaying Epsilon Greedy Policy')
plt.scatter(epsilons, regrets)
29/208: bandit.probs
29/209:
epsilons = np.linspace(0, 1, 101)
regrets = []
for epsilon in epsilons:
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    N  = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/210:
plt.ylabel('Regret')
plt.title('Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/211:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/212: bandit = Bernoulli_bandit(10)
29/213: bandit.probs
29/214:
epsilons = np.linspace(0, 1, 101)
regrets = []
for epsilon in epsilons:
    bandit.regret = 0
    gamma = 0.9
    gamma_power = 0.9
    Q = np.zeros(n)
    N  = np.zeros(n)
    for episode in range(10000):
        choice = epsilon_greedy(Q, epsilon)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    bandit.regret = 0
    choice = np.argmax(Q) + 1
    for episode in range(10000):
        bandit.evaluate_choice(choice)
    regrets.append(bandit.regret)
29/215:
plt.ylabel('Regret')
plt.title('Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/216:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/217:
plt.ylabel('Regret')
plt.xlabel('Epsilon')
plt.title('Exponentially Decaying Epsilon Greedy Policy')
plt.scatter(epsilons, regrets)
29/218:
temperature_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for temp in temperature_values:
    optimal_choice = implement_softmax(temp)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/219:
# softmax action selection strategy

def softmax_strategy(Q, temperature):
    
    choice_probabilities = [np.exp(val/temperature) for val in Q]
    choice_probabilities = choice_probabilities/sum(choice_probabilities)

    return np.random.choice(range(1, len(Q)+1), p=choice_probabilities)
29/220:
temperature_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for temp in temperature_values:
    optimal_choice = implement_softmax(temp)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/221:
temperature_values = np.linspace(0, 1, 101)[1:]
regrets = []
choices = []
for temp in temperature_values:
    optimal_choice = implement_softmax(temp)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/222:
plt.ylabel('Regret')
plt.title('Softmax Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/223:
plt.ylabel('Regret')
plt.title('Softmax Policy')
plt.xlabel('Epsilon')
plt.scatter(temperature_values, regrets)
29/224: regrets
29/225:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/226:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(sum, N[i])) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    N[choice] += 1
    return choice
29/227:
# thompson sampling strategy

def thompson_sampling(Q, N, alpha, beta):
    samples = np.random.normal(loc=Q, scale = alpha/(np.sqrt(N) + beta))
    choice = np.argmax(samples) + 1
    return choice
29/228:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/229:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(s, N[i])) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    N[choice] += 1
    return choice
29/230:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/231:
def implement_ucb(c, gamma=0.9, num_episodes = 10000):
    global bandit
    bandit.regret = 0
    gamma_power = gamma
    Q = np.zeros(bandit.N)
    N = np.ones(bandit.N)
    for episode in range(num_episodes):
        choice = ucb(Q, N, c)
        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)
        N[choice-1] += 1
        gamma_power *= gamma
    return np.argmax(Q) + 1
29/232:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/233:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(s)/N[i])) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    N[choice] += 1
    return choice
29/234:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(s)/N[i]) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    N[choice] += 1
    return choice
29/235:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/236:
plt.ylabel('Regret')
plt.title('Softmax Policy')
plt.xlabel('c')
plt.scatter(c_values, regrets)
29/237:
plt.ylabel('Regret')
plt.title('UCB Policy')
plt.xlabel('c')
plt.scatter(c_values, regrets)
29/238: regrets
29/239:
# 10 arm gaussian bandit
n = 10
# generate the means for each arm
means = 10*np.random.rand(n)

# generate the variance for each arm
variances = []
29/240:
# 10 arm gaussian bandit
n = 10
# generate the means for each arm
means = 10*np.random.rand(n)
means
# generate the variance for each arm
variances = []
29/241:
# 10 arm gaussian bandit
n = 10
# generate the means for each arm
means = 10*np.random.rand(n)
means
# generate the variance for each arm
# variances = []
29/242:
# 10 arm gaussian bandit
n = 10
means = 10*np.random.rand(n)

variances = np.sqrt(means)*np.random.rand()
variances
29/243:
# 10 arm gaussian bandit
n = 10
means = 10*np.random.rand(n)

variances = np.sqrt(means)*np.random.rand()
variances
variances*variances/means
29/244:
# 10 arm gaussian bandit
n = 10
means = 10*np.random.rand(n)

variances = np.sqrt(means)*np.random.rand(n)
variances*variances/means
29/245:
# 10 arm gaussian bandit
n = 10
means = 10*np.random.rand(n)

variances = np.sqrt(means)*np.random.rand(n)
variances
29/246:
class Gaussian_bandit():
    def __init__(self, n):
        self.N = n
        self.means = np.random.rand(n)
        self.variances = np.sqrt(means)*(np.random.rand(n)/2 + 0.5)
        self.optimal = np.max(self.means)

    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        return np.random.uniform(self.means[k-1], self.variances[k-1])
29/247:
class Gaussian_bandit():
    def __init__(self, n):
        self.N = n
        self.means = np.random.rand(n)
        self.variances = np.sqrt(means)*(np.random.rand(n)/2 + 0.5)
        self.optimal = np.max(self.means)
        self.regret = 0.0

    def evaluate_choice(self, k):
        assert 1 <= k <= self.N
        reward = np.random.uniform(self.means[k-1], self.variances[k-1]) 
        self.regret += np.random.uniform(self.optimal, self.variances[k-1]) - reward
        return reward
29/248: gaussian_bandit = Gaussian_bandit(n)
29/249: gaussian_bandit.probs
29/250: gaussian_bandit.means
29/251: gaussian_bandit.variances
29/252: gaussian_bandit.evaluate_choice(4)
29/253: gaussian_bandit.regret
29/254: bandit = Gaussian_bandit(n)
29/255: bandit.means
29/256:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/257:
plt.ylabel('Regret')
plt.title('Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/258:
epsilon_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for epsilon in epsilon_values:
    optimal_choice = implement_decaying_epsilon_greedy(epsilon)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/259:
plt.ylabel('Regret')
plt.title('Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/260:
plt.ylabel('Regret')
plt.title('Decaying Epsilon Greedy Policy')
plt.xlabel('Epsilon')
plt.scatter(epsilons, regrets)
29/261:
temperature_values = np.linspace(0, 1, 101)[1:]
regrets = []
choices = []
for temp in temperature_values:
    optimal_choice = implement_softmax(temp)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/262:
plt.ylabel('Regret')
plt.title('Softmax Policy')
plt.xlabel('Epsilon')
plt.scatter(temperature_values, regrets)
29/263:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/264:
plt.ylabel('Regret')
plt.title('UCB Policy')
plt.xlabel('c')
plt.scatter(c_values, regrets)
29/265:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(s)/N[i]) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    N[choice-1] += 1
    return choice
29/266:
# upper confidence bound strategy

def ucb(Q, N, c):
    s = sum(N)
    choice = np.argmax([Q[i] + np.sqrt(log(s)/N[i]) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1
    return choice
29/267:
# thompson sampling strategy

def thompson_sampling(Q, N, alpha, beta):
    samples = np.random.normal(loc=Q, scale = alpha/(np.sqrt(N) + beta))
    choice = np.argmax(samples) + 1
    return choice
29/268:
c_values = np.linspace(0, 1, 101)
regrets = []
choices = []
for c in c_values:
    optimal_choice = implement_ucb(c)
    bandit.regret = 0
    print(optimal_choice)
    for episode in range(1000):
        bandit.evaluate_choice(optimal_choice)
    regrets.append(bandit.regret)
29/269:
plt.ylabel('Regret')
plt.title('UCB Policy')
plt.xlabel('c')
plt.scatter(c_values, regrets)
30/1:
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
30/2: df = pandas.read_csv('company_sales_data.csv')
30/3: df = pd.read_csv('company_sales_data.csv')
30/4: df.head()
30/5: plt.plot('month_number', 'total_profit', data=df)
30/6:
plt.figure(figsize=(12, 10))
plt.xlabel('Month Number')
plt.ylabel('Total Profit')
plt.plot('month_number', 'total_profit', data=df, lw=3, ls='ro')
30/7:
plt.figure(figsize=(12, 10))
plt.xlabel('Month Number')
plt.ylabel('Total Profit')
plt.plot('month_number', 'total_profit', data=df, lw=3, b='ro')
30/8:
plt.figure(figsize=(12, 10))
plt.xlabel('Month Number')
plt.ylabel('Total Profit')
plt.plot('month_number', 'total_profit', data=df, lw=3)
30/9:
plt.figure(figsize=(12, 10))
plt.xlabel('Month Number')
plt.ylabel('Total Profit')
plt.plot('month_number', 'total_profit', data=df, lw=3, marker="o", color="red")
30/10: fig, ax = plt.subplots()
30/11: df.head()
30/12:
fig, ax = plt.subplots()
for col in df.drop(['total_profit', 'month_number'], axis=1):
    ax.plot('month_number', col, data=df)
30/13:
fig, ax = plt.subplots()
for col in df.drop(['total_profit', 'month_number'], axis=1):
    print(col)
    ax.plot('month_number', col, data=df)
30/14:
fig, ax = plt.subplots()
for col in df.drop(['total_profit', 'month_number', 'total_units'], axis=1):
    ax.plot('month_number', col, data=df)
ax.legend()
ax.set_xlabel('Month Number')
ax.set_ylabel('Number of Units Sold')
30/15: df.pivot()
30/16: df.transpose()
30/17: df.cumsum()
30/18: df.sum()
30/19: type(df.sum())
30/20: values = pd.DataFrame(df.sum())
30/21: values
30/22: values = pd.DataFrame(df.sum()).drop(['month_number', 'total_profit', 'total_units'])
30/23: values
30/24: import plotly.express as px
30/25:
!pip install plotly
# import plotly.express as px
30/26: # import plotly.express as px
30/27: import plotly.express as px
30/28: px.pie(values)
30/29: values = pd.DataFrame(df.sum()).drop(['month_number', 'total_profit', 'total_units']).reset_index()
30/30: values
30/31: px.pie(values, names='index')
30/32: px.pie(data_frame=values, names='index', values='0')
30/33: px.pie(data_frame=values, names='index', values=0)
30/34:
px.pie(data_frame=values, names='index', values=0)
plt.show()
30/35:
px.pie(data_frame=values, names='index', values=0)
px.show()
30/36:
px.pie(data_frame=values, names='index', values=0)
px.figure()
30/37:
px.pie(data_frame=values, names='index', values=0)
px.fig()
30/38: px.pie(data_frame=values, names='index', values=0, color='index')
30/39: plt.scatter('total_profits', 'month_number', data=df)
30/40: plt.scatter(df['total_profits'], df['month_number'])
30/41: plt.scatter(df['total_profit'], df['month_number'])
30/42: values[0]
30/43:
values['number_of_units'] = values[0]
values.drop(0, axis=1)
30/44:
values['number_of_units'] = values[0]
values = values.drop(0, axis=1)
30/45: values
30/46: px.pie(data_frame=values, names=values['index'], values=values['number_of_units'], color='index')
30/47: px.pie(data_frame=values, names=values['index'], values=values['number_of_units'], color=values['index'])
30/48: px.pie(values)
30/49: px.pie(df)
30/50:
fig = px.pie(data_frame=values, names='index', values='number_of_units', color='index')
fig.show()
30/51:
# Sample data
labels = ['Category A', 'Category B', 'Category C', 'Category D']
values = [25, 30, 20, 25]

# Create a pie chart using Plotly Express
fig = px.pie(labels=labels, values=values, title='Sample Pie Chart')
fig.show()
30/52:
# Sample data
labels = ['Category A', 'Category B', 'Category C', 'Category D']
values = [25, 30, 20, 25]

# Create a pie chart using Plotly Express
fig = px.scatter(labels=labels, values=values, title='Sample Pie Chart')
fig.show()
30/53:
# Sample data
labels = ['Category A', 'Category B', 'Category C', 'Category D']
values = [25, 30, 20, 25]

# Create a pie chart using Plotly Express
fig = px.heatmap(labels=labels, values=values, title='Sample Pie Chart')
fig.show()
30/54:
fig = px.pie(values, names='index', values='number_of_units', color='index')
fig.show()
30/55:
values['number_of_units'] = values[0]
values = values.drop(0, axis=1)
30/56: values = pd.DataFrame(df.sum()).drop(['month_number', 'total_profit', 'total_units']).reset_index()
30/57:
values['number_of_units'] = values[0]
values = values.drop(0, axis=1)
30/58: values
30/59:
fig = px.pie(values, names='index', values='number_of_units', color='index')
fig.show()
30/60:
fig = plt.pie('number_of_units', color='index', data=values)
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/61:
fig = plt.pie('number_of_units', colors='index', data=values)
fig.show()
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/62:
fig = plt.pie('number_of_units', labels='index', data=values)
fig.show()
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/63:
fig = plt.pie('number_of_units', labels='index', data=values)
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/64:
fig = plt.pie('number_of_units', labels='index', data=values, autopct='%1.1f%%', startangle=90)
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/65:
fig = plt.pie('number_of_units', labels='index', data=values, autopct='%1.1f%%', startangle=90, shadow=True, radius=2)
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/66:
fig = plt.pie('number_of_units', labels='index', data=values, autopct='%1.1f%%', startangle=90, shadow=True, radius=2, textprops={'color':'blue', 'weight':'bold'})
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/67:
fig = plt.pie('number_of_units', labels='index', data=values, autopct='%1.1f%%', startangle=90, shadow=True, radius=2, textprops={'color':'purple', 'weight':'bold'})
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/68:
fig = plt.pie('number_of_units', labels='index', data=values, autopct='%1.1f%%', startangle=90, shadow=True, radius=2, textprops={'color':'purple'})
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
30/69:
fig = plt.pie('number_of_units', labels='index', data=values, autopct='%1.1f%%', startangle=90, shadow=True, radius=2, textprops={'color':'darkblue'})
# fig = plt.pie(values, names='index', values='number_of_units', color='index')
# fig.show()
32/1:
import numpy as np
from numpy.random import choice
32/2: card_face_values = list(range(14, 1, -1))
32/3:
suits = list(range(3, -1,-1))
suits
32/4: suits = list(range(3, -1,-1))
32/5: card_face_values
32/6: card_face_values = list(range(13, 0, -1))
32/7: card_face_values
32/8:
possible_cards = []
for suit in suits:
    for card in card_face_values:
        possible_cards.append(13*suit + card)
possible_cards
32/9:
possible_cards = []
for suit in suits:
    for card in card_face_values:
        possible_cards.append(13*suit + card)
32/10:
two_card_states = []
for i in range(52):
    for j in range(i+1, 52):
        states.append((possible_cards[i], possible_cards[j]))
32/11:
two_card_states = []
for i in range(52):
    for j in range(i+1, 52):
        two_card_states.append((possible_cards[i], possible_cards[j]))
32/12:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i][0] for i in range(7) if cards[i][1] == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1)
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1)
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1)
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1)
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/13:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i][0] for i in range(7) if cards[i][1] == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1)
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1)
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/14:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i][0] for i in range(7) if cards[i][1] == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/15: value_of_hand([52, 51, 50, 49, 48])
32/16: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/17:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i][0] for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/18: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/19:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/20: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/21:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            print("HERE")
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/22: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/23:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
                    break
            break
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/24: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/25:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/26: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/27:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/28: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/29: card_face_values = list(range(12, -1, -1))
32/30: suits = list(range(3, -1,-1))
32/31:
possible_cards = []
for suit in suits:
    for card in card_face_values:
        possible_cards.append(13*suit + card)
32/32:
two_card_states = []
for i in range(52):
    for j in range(i+1, 52):
        two_card_states.append((possible_cards[i], possible_cards[j]))
32/33:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 5:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/34: value_of_hand([52, 51, 50, 49, 48, 45, 32])
32/35: possible_cards
32/36: value_of_hand([51, 50, 49, 48, 45, 32])
32/37: value_of_hand([51, 50, 49, 48, 45, 32, 24])
32/38: value_of_hand([51, 50, 49, 48, 47, 32, 24])
32/39:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i] - flush_values[i+4] == 5:
                    straight_flush = flush_values[i]
                    if straight_flush == 14:
                        royal_flush = 14
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/40: value_of_hand([51, 50, 49, 48, 47, 32, 24])
32/41:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 4:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 14:
                        royal_flush = 14
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(2, 15):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/42: value_of_hand([51, 50, 49, 48, 47, 32, 24])
32/43:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 4:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 12:
                        royal_flush = 12
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(13):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for values in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for values in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/44: value_of_hand([51, 50, 49, 48, 47, 32, 24])
32/45: value_of_hand([12, 25, 38, 39, 26])
32/46: value_of_hand([12, 25, 38, 39, 26, 1, 2])
32/47:
def value_of_hand(cards):
    assert(len(cards) == 7)
    face_values = [card%13 for card in cards]
    suits = [int(card/13) for card in cards]
    royal_flush = False
    straight_flush = False
    four_of_a_kind = False
    full_house = False
    flush = False
    straight = False
    three_of_a_kind = False
    two_pair = False
    two_of_a_kind = False
    max_value = max(face_values)
    for suit in suits:
        if suits.count(suit) >= 5:
            flush_values = [cards[i]%13 for i in range(7) if int(cards[i]/13) == suit]
            flush_values.sort()
            print(flush_values)
            flush = flush_values[-1]
            for i in range(len(flush_values)-4):
                if flush_values[i+4] - flush_values[i] == 4:
                    straight_flush = flush_values[i+4]
                    if straight_flush == 12:
                        royal_flush = 12
    if royal_flush:
         return 1000 + royal_flush, list(range(royal_flush, royal_flush-5, -1))
    if straight_flush:
        return 900 + straight_flush, list(range(straight_flush, straight_flush-5, -1))
    len_seq = 0
    for num in range(13):
        if face_values.count(num) == 0:
            len_seq = 0
            continue
        len_seq += 1
        if len_seq >= 5:
            straight = num
        if face_values.count(num) == 4:
            four_of_a_kind = num
        elif face_values.count(num) == 3:
            if two_of_a_kind or three_of_a_kind:
                full_house = 20*max(two_of_a_kind, three_of_a_kind) + num
            three_of_a_kind = num
        elif face_values.count(num) == 2:
            if full_house:
                if face_values.count(full_house%20) == 2:
                    full_house = full_house - full_house%20 + num
                else:
                    full_house = 20*(full_house%20) + num
            elif three_of_a_kind:
                full_house = 20*three_of_a_kind + num
            elif two_pair:
                two_pair = 20*(two_pair%20) + num
            elif two_of_a_kind:
                two_pair = 20*two_of_a_kind + num
            two_of_a_kind = num
    if four_of_a_kind:
        max_value = max(face_values)
        if max_value != four_of_a_kind:
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
        else:
            max_value = max([val for val in face_values if val != four_of_a_kind])
            return 800 + max_value, [four_of_a_kind]*4+[max_value]
    if full_house:
        n1 = face_values.count(full_house%20)
        n2 = face_values.count(int(full_house/20))
        return 700 + int(full_house/20), [full_house%20]*n1+[int(full_house/20)]*n2
    if flush:
        return 600 + flush, list(range(flush, flush-5, -1))
    if straight:
        return 500 + straight, list(range(straight, straight-5, -1))
    if three_of_a_kind:
        face_values.sort()
        values = [val for val in face_values if val != three_of_a_kind][-2:] + [three_of_a_kind]*3
        values.sort(reverse=True)
        return 400 + values[0], values
    if two_pair:
        face_values.sort()
        values = [val for val in face_values if val != two_pair%20 and val != int(two_pair/20)][-1] + [two_pair%20]*2 + [int(two_pair/20)]*2
        values.sort(reverse=True)
        return 300 + values[0], values
    if two_of_a_kind:
        face_values.sort()
        values = [val for val in face_values if val != two_of_a_kind][-3] + [two_of_a_kind]*2
        values.sort(reverse=True)
        return 200 + values[0], values
32/48: value_of_hand([12, 25, 38, 39, 26, 1, 2])
33/1: import numpy as np
33/2:
# < START >

# < END >
arr = np.array([[1,2,4],[7,13,21]])
print(arr)
print("Shape:", arr.shape)
33/3:
n_rows = 2
n_columns = 3

# <START>
x = np.random.rand((n_rows, n_columns))
# <END>

print(x)
33/4:
n_rows = 2
n_columns = 3

# <START>
x = np.random.sample((n_rows, n_columns))
# <END>

print(x)
33/5:
# < START >
# Initialize an array ZERO_ARR of dimensions (4, 5, 2) whose every element is 0

# < END >
ZERO_ARR = np.zeros((4,5,2))
print(ZERO_ARR)

# < START >
# Initialize an array ONE_ARR of dimensions (4, 5, 2) whose every element is 1

# < END >
ONE_ARR = np.ones((4,5,2))
print(ONE_ARR)
33/6:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
y_transpose = y.reshape(reverse(y.shape()))
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_transpose = y.reshape((sum(y.shape()), 1))
print(y_flat)
33/7:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
y_transpose = y.reshape(y.shape()[::-1])
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_transpose = y.reshape((sum(y.shape()), 1))
print(y_flat)
33/8:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
y_transpose = y.reshape(y.shape[::-1])
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_transpose = y.reshape((sum(y.shape), 1))
print(y_flat)
33/9:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
y_transpose = y.reshape(y.shape[::-1])
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_transpose = y.reshape((np.prod(y.shape), 1))
print(y_flat)
33/10:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
y_transpose = y.reshape(y.shape[::-1])
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_flat = y.reshape((np.prod(y.shape), 1))
print(y_flat)
33/11:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
def transpose(arr):
    return arr.reshape(arr.shape[::-1])
y_transpose = transpose(y)
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_flat = y.reshape((np.prod(y.shape), 1))
print(y_flat)
33/12:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
def transpose(arr):
    return arr.reshape(arr.shape[::-1])
y_transpose = np.transpose(y)
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_flat = y.reshape((np.prod(y.shape), 1))
print(y_flat)
33/13:
y = np.array([[1, 2, 3],
              [4, 5, 6]])

# < START >
# Create a new array y_transpose that is the transpose of matrix y
y_transpose = np.transpose(y)
# < END >

print(y_transpose)

# < START >
# Create a new array y_flat that contains the same elements as y but has been flattened to a column array

# < END >
y_flat = y.reshape((np.prod(y.shape), 1))
print(y_flat)
33/14:
x = np.array([4, 1, 5, 6, 11])

# <START>
# Create a new array y with the middle 3 elements of x
y = x[1:-1]
# <END>

print(y)

z = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# <START>
# Create a new array w with alternating elements of z
w = z[::2]
# <END>

print(w)
33/15:
arr_2d = np.array([[4, 5, 2],
          [3, 7, 9],
          [1, 4, 5],
          [6, 6, 1]])

# <START>
# Create a 2D array sliced_arr_2d that is of the form [[5, 2], [7, 9], [4, 5]]
sliced_arr_2d = arr[:-1,1:]
# <END>

print(sliced_arr_2d)
33/16:
arr_2d = np.array([[4, 5, 2],
          [3, 7, 9],
          [1, 4, 5],
          [6, 6, 1]])

# <START>
# Create a 2D array sliced_arr_2d that is of the form [[5, 2], [7, 9], [4, 5]]
sliced_arr_2d = arr_2d[:-1,1:]
# <END>

print(sliced_arr_2d)
33/17:
arr1 = np.array([1, 2, 3, 4])
b = 1

# <START>
# Implement broadcasting to add b to each element of arr1

# <END>

print(arr1)

arr2 = np.array([[1, 2, 3],
                 [4, 5, 6]])
arr3 = np.array([[4],
                 [5]])

# <START>
# Multiply each element of the first row of arr2 by 4 and each element of the second row by 5, using only arr2 and arr3
arr2 *= arr3
# <END>

print(arr2)
33/18:
arr1 = np.array([1, 2, 3, 4])
b = 1

# <START>
# Implement broadcasting to add b to each element of arr1
arr1 += b
# <END>

print(arr1)

arr2 = np.array([[1, 2, 3],
                 [4, 5, 6]])
arr3 = np.array([[4],
                 [5]])

# <START>
# Multiply each element of the first row of arr2 by 4 and each element of the second row by 5, using only arr2 and arr3
arr2 *= arr3
# <END>

print(arr2)
33/19:
import time

arr_nonvectorized = np.random.rand(1000, 1000)
arr_vectorized = np.array(arr_nonvectorized) # making a deep copy of the array

start_nv = time.time()

# Non-vectorized approach
# <START>

arr_nonvectorized = [[3*val for val in row] for row in arr_nonvectorized]  

# <END>

end_nv = time.time()
print("Time taken in non-vectorized approach:", 1000*(end_nv-start_nv), "ms")

# uncomment and execute the below line to convince yourself that both approaches are doing the same thing
# print(arr_nonvectorized)

start_v = time.time()

# Vectorized approach
# <START>
arr_vectorized *= 3
# <END>

end_v = time.time()
print("Time taken in vectorized approach:", 1000*(end_v-start_v), "ms")

# uncomment and execute the below line to convince yourself that both approaches are doing the same thing
# print(arr_vectorized)
35/1:
def derivative(f, positions):
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/2:
def call_func(func):
    def wrapper():
        func()
    return wrapper
35/3:
def derivative(f, positions):
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/4:
def multivariable_gradient_descent(func, positions, weights, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        weights = derivative(f, positions)
        position -= alpha*weights
        alpha *= 0.9
35/5:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/6:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        weights = derivative(f, positions)
        position -= alpha*weights
        alpha *= 0.9
    return positions
35/7:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/8: import numpy as np
35/9:
def call_func(func):
    def wrapper():
        func()
    return wrapper
35/10:
def derivative(f, positions):
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/11:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        weights = derivative(f, positions)
        position -= alpha*weights
        alpha *= 0.9
    return positions
35/12:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/13:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        weights = derivative(func, positions)
        position -= alpha*weights
        alpha *= 0.9
    return positions
35/14:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/15:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        weights = derivative(func, positions)
        positions -= alpha*weights
        alpha *= 0.9
    return positions
35/16:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/17:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        positions -= alpha*weights
        alpha *= 0.9
    return positions
35/18:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/19:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = derivative(func, positions)
    while sum(weights) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        positions -= alpha*weights
        # alpha *= 0.9
    return positions
35/20:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/21:
positions = np.array([200,200])
func = 5
multivariable_gradient_descent(func, positions)
35/22:
positions = np.array([200,200], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/23:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/24:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while sum(weights) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        positions -= alpha*weights
        # alpha *= 0.9
    return positions
35/25:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/26:
positions = np.array([2,2], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/27:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while sum(weights) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= alpha*weights
        # alpha *= 0.9
    return positions
35/28:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/29:
positions = np.array([2,2], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/30:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= alpha*weights
        alpha *= 0.9
        alpha = min(alpha, 1e-6)
    return positions
35/31:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/32:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= alpha*weights        
    return positions
35/33:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/34:
positions = np.array([2,2], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/35:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= weights/sum(weights)        
    return positions
35/36:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/37:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= weights/max(0.01, sum(weights))        
    return positions
35/38:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/39:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= weights/min(max(100, sum(weights))        
    return positions
35/40:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= weights/max(100, sum(weights)      
    return positions
35/41:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= weights/max(100, sum(weights))     
    return positions
35/42:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/43:
positions = np.array([2,2], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/44:
positions = np.array([200,200], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/45:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    while abs(sum(weights)) > epsilon:
        print(positions)
        weights = derivative(func, positions)
        print(sum(weights))
        positions -= weights/max(100, abs(sum(weights)))     
    return positions
35/46:
positions = np.array([-2,-2], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/47:
def derivative(f, positions):
    return 2*positions
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/48:
positions = np.array([200,200], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/49:
def derivative(f, positions):
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/50:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/51:
def derivative(f, positions):
    return 2*positions - 200
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/52:
positions = np.array([100,100], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/53:
positions = np.array([50,50], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/54:
positions = np.array([50,50], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
35/55:
positions = np.array([50,50], dtype='float64')
func = 5
positions, count = multivariable_gradient_descent(func, positions)
35/56:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    count = 0
    while abs(sum(weights)) > epsilon:
        count += 1
        weights = derivative(func, positions)
        positions -= weights/max(100, abs(sum(weights)))     
    return positions, count
35/57:
positions = np.array([50,50], dtype='float64')
func = 5
positions, count = multivariable_gradient_descent(func, positions)
35/58: positions
35/59: count
35/60:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    s = abs(sum(weights))
    count = 0
    new_positions = positions.copy()
    while s > epsilon:
        positions = new_positions.copy()
        count += 1
        weights = derivative(func, positions)
        s = abs(sum(weights))
        new_positions -= weights/max(100, s)
        while s > epsilon and new_positions == positions:
            new_positions -= weights/100
    return positions, count
35/61:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/62:
def derivative(f, positions):
    return np.array([4*positions[0]**3 + 2*positions[0]*positions[1]**2, 4*positions[1]**3 -2*positions[1] + 2*positions[1]*positions[0]**2])
35/63:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    s = abs(sum(weights))
    count = 0
    new_positions = positions.copy()
    while s > epsilon:
        positions = new_positions.copy()
        count += 1
        weights = derivative(func, positions)
        s = abs(sum(weights))
        new_positions -= weights/max(100, s)
        while s > epsilon and new_positions == positions:
            new_positions -= weights/100
    return positions, count
35/64:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/65:
def multivariable_gradient_descent(func, positions, alpha = 0.1):
    epsilon = 1e-5
    weights = np.ones(len(positions))
    s = abs(sum(weights))
    count = 0
    new_positions = positions.copy()
    while s > epsilon:
        positions = new_positions.copy()
        count += 1
        weights = derivative(func, positions)
        s = abs(sum(weights))
        new_positions -= weights/max(100, s)
        while s > epsilon and all(new_positions == positions):
            new_positions -= weights/100
    return positions, count
35/66:
positions = [1,1]
func = 5
multivariable_gradient_descent(func, positions)
35/67:
positions = np.array([-2,-2], dtype='float64')
func = 5
multivariable_gradient_descent(func, positions)
36/1:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
36/2:
#import important libraries
!pip install scikit-learn
# import numpy as np
# from sklearn import datasets
# import matplotlib.pyplot as plt
36/3:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
36/4:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
36/5: df = pd.read_csv('Data_Mendeley.csv')
36/6: df.head()
36/7:
import numpy as np

def derivative(X, y, weights):
    def derivative_row(row, res, weight):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, excluded=['res', 'weight'])

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y, weight=weights)

    # Sum along the rows to get the final result
    return np.sum(result, axis=1)

# Example usage:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/8:
def derivative(X, y, weights):
    def derivative_row(row, res, weights):
        return row*(res - np.dot(row, weights))
        
    result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])
result = derivative(X, y, weights)
print(result)
36/9:
def derivative(X, y, weights):
    def derivative_row(row, res, w):
        return row*(res - np.dot(row, w))
        
    result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])
result = derivative(X, y, weights)
print(result)
36/10:
def derivative(X, y, weights):
    def derivative_row(row, res, w):
        return row*(res - np.dot(row, w))
        
    result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])
result = derivative(X, y, weights)
print(result)
36/11:
def derivative(X, y, weights):
    def derivative_row(row, res, w):
        return row*(res - np.dot(row, w))
        
    result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])
result = derivative(X, y, weights)
print(result)
36/12:
def derivative(X, y, weights):
    def derivative_row(row, res, w):
        return row*(res - np.dot(row, w))
        
    result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/13:
def derivative(X, y, weights):
    def derivative_row(row, res, w):
        return row*(res - np.dot(row, w))

    result = pd.Series([0,0,0])
    # result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/14:
def derivative(X, y, weights):
    def derivative_row(row, res, w):
        return row*(res - np.dot(row, w))

    result = pd.Series([0,0,0],[0,0,0])
    # result = np.vectorize(derivative_row, X, y, weights)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/15:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/16:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X.to_numpy(), res=y.to_numpy())

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/17:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]))
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/18:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/19:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = pd.Series([10, 20, 30])
weights = pd.Series([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/20:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]))
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/21:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/22:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]))
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/23:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/24:
import numpy as np

def derivative(X, y, weights):
    def derivative_row(row, res, weight):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, excluded=['res', 'weight'])

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y, weight=weights)

    # Sum along the rows to get the final result
    return np.sum(result, axis=1)

# Example usage:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/25:
import numpy as np

def derivative(X, y, weights):
    def derivative_row(row, res, weight):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, excluded=['res', 'weight'])

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y, weight=weights)

    # Sum along the rows to get the final result
    return np.sum(result, axis=1)

# Example usage:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/26:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/27:
import numpy as np

def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, excluded=['res', 'weight'])

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # Sum along the rows to get the final result
    return np.sum(result, axis=1)

# Example usage:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/28:
import numpy as np

def dderivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, excluded=['res', 'weight'])

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # Sum along the rows to get the final result
    return np.sum(result, axis=1)

# Example usage:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/29:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([0.1, 0.2, 0.3])

result = derivative(X, y, weights)
print(result)
36/30:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]))
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/31:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/32:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/33:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/34:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        print(row*(res - np.dot(row, weights)))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/35:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        print(row)
        print(row*(res - np.dot(row, weights)))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/36:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        print(row)
        print(res)
        print(row*(res - np.dot(row, weights)))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/37:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        print(row)
        print(row*(res - np.dot(row, weights)))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),(n)->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    # result = np.vectorize(derivative_row, X, y)
    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/38:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        print(row)
        print(row*(res - np.dot(row, weights)))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=1)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/39:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        print(row)
        print(row*(res - np.dot(row, weights)))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/40:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]))
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/41:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/42:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/43:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/44:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]))
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        print(s)
        weights -= result/max(100, s)
    return weights
36/45:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/46:
import numpy as np

def dderivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights[:-1]) - weights[-1])

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, excluded=['res', 'weight'])

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, res=y)

    # Sum along the rows to get the final result
    return np.sum(result, axis=1)

# Example usage:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/47:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return row*(res - np.dot(row, weights[:-1]) - weights[-1])

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/48:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        print(s)
        weights -= result/max(100, s)
    return weights
36/49:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/50:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return (row+np.ones(1))*(res - np.dot(row, weights[:-1]) - weights[-1])

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/51:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return (row)*(res - np.dot(row, weights[:-1]) - weights[-1])

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/52:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        return ([1]+row)*(res - np.dot(row, weights[:-1]) - weights[-1])

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/53:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = row.append(1)
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/54:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = row.append([1])
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/55:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = np.append(row, 1)
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/56:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = np.append(row, 1)
        print(row)
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/57:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = np.append(row, 1)
        assert(len(row) == len(weights))
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/58:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = np.append(row, 1)
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n+1)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/59:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = np.append(row, 1)
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/60:
def derivative(X, y, weights):
    
    def derivative_row(row, res):
        row = np.append(row, 1)
        return row*(res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n+1)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/61:
def derivative(X, y, weights):
    def derivative_row(row, res):
        row = np.append(row, 1)
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n+1)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/62:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    np.concatenate(X, np.zeros(len(X)).T, axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/63:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    np.concatenate((X, np.zeros(len(X))).T, axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/64:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    np.concatenate((X, np.zeros(len(X)).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/65:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    np.concatenate((X, np.array(np.zeros(len(X))).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/66:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    np.concatenate((X, np.array([np.zeros(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/67:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    np.concatenate((X, np.array([np.zeros(len(X))]).T), axis=1)
    print(X)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/68:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.zeros(len(X))]).T), axis=1)
    print(X)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)

    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/69:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.zeros(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    print(result)
    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/70:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    print(result)
    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/71:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/72:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/73:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/74:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/75:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/76:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/77:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        print(result)
        s = abs(sum(result))
        weights -= result/max(100, s)
    return weights
36/78:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/79:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        print(result)
        s = abs(sum(result))
        weights += result/max(100, s)
    return weights
36/80:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/81:
X = np.array([[1], [4], [7]])
y = np.array([10])
linear_regression(X, y)
36/82:
X = np.array([[1, 4, 7]])
y = np.array([10])
linear_regression(X, y)
36/83:
X = np.array([[1], [4], [7])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/84:
X = np.array([[1], [4], [7]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/85:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([10, 20, 30])
linear_regression(X, y)
36/86:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
36/87:
X = np.array([[1], [4], [7]])
y = np.array([10, 20, 30])
weights = np.ones(4)
36/88: result = derivative(X, y, weights)
36/89:
X = np.array([[1], [4], [7]])
y = np.array([10, 20, 30])
weights = np.ones(len(X[0]) + 1)
36/90: result = derivative(X, y, weights)
36/91: result
36/92:
def derivative(X, y, weights):
    def derivative_row(row, res):
        print(row, res)
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)

X = np.array([[1], [4], [7]])
y = np.array([10, 20, 30])
weights = np.array([1, 1, 1, 1])

result = derivative(X, y, weights)
print(result)
36/93:
def derivative(X, y, weights):
    def derivative_row(row, res):
        print(row, res)
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
36/94:
X = np.array([[1], [4], [7]])
y = np.array([10, 20, 30])
weights = np.ones(len(X[0]) + 1)
36/95: result = derivative(X, y, weights)
36/96: weights += result/max(abs(sum(result)), 100)
36/97:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
36/98:
print(weights)
result = derivative(X, y, weights)
36/99: result
36/100: weights += result/max(abs(sum(result)), 100)
36/101:
print(weights)
result = derivative(X, y, weights)
36/102: result
36/103: weights += result/max(abs(sum(result)), 100)
36/104:
print(weights)
result = derivative(X, y, weights)
36/105: result
36/106: weights += result/max(abs(sum(result)), 100)
36/107:
print(weights)
result = derivative(X, y, weights)
36/108: result
36/109: weights += result/max(abs(sum(result)), 100)
36/110:
print(weights)
result = derivative(X, y, weights)
36/111: result
36/112: weights += result/max(abs(sum(result)), 100)
36/113:
print(weights)
result = derivative(X, y, weights)
36/114: result
36/115: weights += result/max(abs(sum(result)), 100)
36/116:
print(weights)
result = derivative(X, y, weights)
36/117: result
36/118: weights += result/max(abs(sum(result)), 100)
36/119:
print(weights)
result = derivative(X, y, weights)
36/120: result
36/121: weights += result/max(abs(sum(result)), 100)
36/122:
print(weights)
result = derivative(X, y, weights)
36/123: result
36/124: weights += result/max(abs(sum(result)), 100)
36/125:
print(weights)
result = derivative(X, y, weights)
36/126: result
36/127: weights += result/max(abs(sum(result)), 100)
36/128:
print(weights)
result = derivative(X, y, weights)
36/129: result
36/130: weights += result/max(abs(sum(result)), 100)
36/131:
print(weights)
result = derivative(X, y, weights)
36/132: result
36/133: weights += result/max(abs(sum(result)), 100)
36/134:
print(weights)
result = derivative(X, y, weights)
36/135: result
36/136:
print(weights)
result = derivative(X, y, weights)
36/137: result
36/138: weights += result/max(abs(sum(result)), 100)
36/139:
print(weights)
result = derivative(X, y, weights)
36/140: result
36/141: weights += result/max(abs(sum(result)), 100)
36/142:
print(weights)
result = derivative(X, y, weights)
36/143: result
36/144: weights += result/max(abs(sum(result)), 100)
36/145:
print(weights)
result = derivative(X, y, weights)
36/146: result
36/147: weights += result/max(abs(sum(result)), 100)
36/148:
print(weights)
result = derivative(X, y, weights)
36/149: result
36/150: weights += result/max(abs(sum(result)), 100)
36/151:
print(weights)
result = derivative(X, y, weights)
36/152: result
36/153: weights += result/max(abs(sum(result)), 100)
36/154:
print(weights)
result = derivative(X, y, weights)
36/155: result
36/156: weights += result/max(abs(sum(result)), 100)
36/157:
print(weights)
result = derivative(X, y, weights)
36/158: result
36/159: weights += result/max(abs(sum(result)), 100)
36/160:
print(weights)
result = derivative(X, y, weights)
36/161: result
36/162: weights += result/max(abs(sum(result)), 100)
36/163:
print(weights)
result = derivative(X, y, weights)
36/164: result
36/165: weights += result/max(abs(sum(result)), 100)
36/166:
print(weights)
result = derivative(X, y, weights)
36/167: result
36/168: weights += result/max(abs(sum(result)), 100)
36/169:
print(weights)
result = derivative(X, y, weights)
36/170: result
36/171: weights += result/max(abs(sum(result)), 100)
36/172:
print(weights)
result = derivative(X, y, weights)
36/173: result
36/174: weights += result/max(abs(sum(result)), 100)
36/175:
print(weights)
result = derivative(X, y, weights)
36/176: result
36/177: weights += result/max(abs(sum(result)), 100)
36/178:
print(weights)
result = derivative(X, y, weights)
36/179: result
36/180: weights += result/max(abs(sum(result)), 100)
36/181:
print(weights)
result = derivative(X, y, weights)
36/182: result
36/183: weights += result/max(abs(sum(result)), 100)
36/184: weights += result/100
36/185:
print(weights)
result = derivative(X, y, weights)
36/186: result
36/187: weights += result/100
36/188:
print(weights)
result = derivative(X, y, weights)
36/189: result
36/190: weights += result/100
36/191:
print(weights)
result = derivative(X, y, weights)
36/192: result
36/193: weights += result/100
36/194:
print(weights)
result = derivative(X, y, weights)
36/195: result
36/196: weights += result/100
36/197:
print(weights)
result = derivative(X, y, weights)
36/198: result
36/199: weights += result/100
36/200:
print(weights)
result = derivative(X, y, weights)
36/201: result
36/202: weights += result/100
36/203:
print(weights)
result = derivative(X, y, weights)
36/204: result
36/205: weights += result/100
36/206:
print(weights)
result = derivative(X, y, weights)
36/207: result
36/208: weights += result/100
36/209:
print(weights)
result = derivative(X, y, weights)
36/210: result
36/211: weights += result/100
36/212:
print(weights)
result = derivative(X, y, weights)
36/213: result
36/214: weights += result/100
36/215:
print(weights)
result = derivative(X, y, weights)
36/216: result
36/217: weights += result/100
36/218:
print(weights)
result = derivative(X, y, weights)
36/219: result
36/220: weights += result/100
36/221:
print(weights)
result = derivative(X, y, weights)
36/222: result
36/223: weights += result/100
36/224:
print(weights)
result = derivative(X, y, weights)
36/225: result
36/226: weights += result/100
36/227:
print(weights)
result = derivative(X, y, weights)
36/228: result
36/229: weights += result/100
36/230:
print(weights)
result = derivative(X, y, weights)
36/231: result
36/232: weights += result/100
36/233:
print(weights)
result = derivative(X, y, weights)
36/234: result
36/235: weights += result/100
36/236:
print(weights)
result = derivative(X, y, weights)
36/237: result
36/238: weights += result/100
36/239:
print(weights)
result = derivative(X, y, weights)
36/240: result
36/241: weights += result/100
36/242:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        print(result)
        s = abs(sum(result))
        weights += result/100
    return weights
36/243:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights += result/100
    return weights
36/244:
X = np.array([[1], [4], [7]])
y = np.array([7, 16, 25])
linear_regression(X, y)
36/245:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
36/246:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights += result/100
    return weights
36/247:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
36/248:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.zeros(len(X_train[0]) + 1)
    s = 5
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights += result/100
    return weights
36/249:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
36/250:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.zeros(len(X_train[0]) + 1)
    s = 5
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        print(s)
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights += result/100
    return weights
36/251:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
36/252:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
weights = np.ones(len(X[0]) + 1)
36/253:
print(weights)
result = derivative(X, y, weights)
36/254: result
36/255: weights += result/100
36/256:
print(weights)
result = derivative(X, y, weights)
36/257:
print(weights)
result = derivative(X, y, weights)
36/258: result
36/259: weights += result/100
36/260:
print(weights)
result = derivative(X, y, weights)
36/261: result
36/262: weights += result/max(100, abs(sum(result)))
36/263:
print(weights)
result = derivative(X, y, weights)
36/264: result
36/265: weights += result/max(100, abs(sum(result)))
36/266:
print(weights)
result = derivative(X, y, weights)
36/267: result
36/268: weights += result/max(100, abs(sum(result)))
36/269:
print(weights)
result = derivative(X, y, weights)
36/270: result
36/271: weights += result/max(100, abs(sum(result)))
36/272:
print(weights)
result = derivative(X, y, weights)
36/273: result
36/274: weights += result/max(100, abs(sum(result)))
36/275:
print(weights)
result = derivative(X, y, weights)
36/276: result
36/277: weights += result/max(100, abs(sum(result)))
36/278:
print(weights)
result = derivative(X, y, weights)
36/279: result
36/280: weights += result/max(100, abs(sum(result)))
36/281:
print(weights)
result = derivative(X, y, weights)
36/282: result
36/283: weights += result/max(100, abs(sum(result)))
36/284:
print(weights)
result = derivative(X, y, weights)
36/285: result
36/286: weights += result/max(100, abs(sum(result)))
36/287:
print(weights)
result = derivative(X, y, weights)
36/288: result
36/289: weights += result/max(100, abs(sum(result)))
36/290:
print(weights)
result = derivative(X, y, weights)
36/291: result
36/292: weights += result/max(100, abs(sum(result)))
36/293:
print(weights)
result = derivative(X, y, weights)
36/294: result
36/295: weights += result/max(100, abs(sum(result)))
36/296:
print(weights)
result = derivative(X, y, weights)
36/297: result
36/298: weights += result/max(100, abs(sum(result)))
36/299:
print(weights)
result = derivative(X, y, weights)
36/300: result
36/301: weights += result/max(100, abs(sum(result)))
36/302:
print(weights)
result = derivative(X, y, weights)
36/303: result
36/304: weights += result/max(100, abs(sum(result)))
36/305:
print(weights)
result = derivative(X, y, weights)
36/306: result
36/307: weights += result/max(100, abs(sum(result)))
36/308:
print(weights)
result = derivative(X, y, weights)
36/309: result
36/310: weights += result/max(100, abs(sum(result)))
36/311:
print(weights)
result = derivative(X, y, weights)
36/312: result
36/313: weights += result/max(100, abs(sum(result)))
36/314:
print(weights)
result = derivative(X, y, weights)
36/315: result
36/316: weights += result/max(100, abs(sum(result)))
36/317:
print(weights)
result = derivative(X, y, weights)
36/318: result
36/319: weights += result/max(100, abs(sum(result)))
36/320:
print(weights)
result = derivative(X, y, weights)
36/321: result
36/322: weights += result/max(100, abs(sum(result)))
36/323:
print(weights)
result = derivative(X, y, weights)
36/324: result
36/325: weights += result/max(100, abs(sum(result)))
36/326:
print(weights)
result = derivative(X, y, weights)
36/327: result
36/328: weights += result/max(100, abs(sum(result)))
36/329:
print(weights)
result = derivative(X, y, weights)
36/330: result
36/331: weights += result/max(100, abs(sum(result)))
36/332:
print(weights)
result = derivative(X, y, weights)
36/333: result
36/334: weights += result/max(100, sum(np.abs(result)))
36/335:
print(weights)
result = derivative(X, y, weights)
36/336: result
36/337: weights += result/max(100, sum(np.abs(result)))
36/338:
print(weights)
result = derivative(X, y, weights)
36/339: result
36/340: weights += result/max(100, sum(np.abs(result)))
36/341:
print(weights)
result = derivative(X, y, weights)
36/342: result
36/343: weights += result/max(100, sum(np.abs(result)))
36/344:
print(weights)
result = derivative(X, y, weights)
36/345: result
36/346: weights += result/max(100, sum(np.abs(result)))
36/347:
print(weights)
result = derivative(X, y, weights)
36/348: result
36/349: weights += result/max(100, sum(np.abs(result)))
36/350:
print(weights)
result = derivative(X, y, weights)
36/351: result
36/352: weights += result/max(1000, sum(np.abs(result)))
36/353:
print(weights)
result = derivative(X, y, weights)
36/354: result
36/355: weights += result/max(1000, sum(np.abs(result)))
36/356:
print(weights)
result = derivative(X, y, weights)
36/357: result
36/358: weights += result/max(1000, sum(np.abs(result)))
36/359:
print(weights)
result = derivative(X, y, weights)
36/360: result
36/361: weights += result/max(1000, sum(np.abs(result)))
36/362:
print(weights)
result = derivative(X, y, weights)
36/363: result
36/364: weights += result/max(1000, sum(np.abs(result)))
36/365:
print(weights)
result = derivative(X, y, weights)
36/366: result
36/367: weights += result/max(1000, sum(np.abs(result)))
36/368:
print(weights)
result = derivative(X, y, weights)
36/369: result
36/370: weights += result/max(1000, sum(np.abs(result)))
36/371:
print(weights)
result = derivative(X, y, weights)
36/372: result
36/373: weights += result/max(1000, sum(np.abs(result)))
36/374:
print(weights)
result = derivative(X, y, weights)
36/375: result
36/376: weights += result/max(1000, sum(np.abs(result)))
36/377:
print(weights)
result = derivative(X, y, weights)
36/378: result
36/379: weights += result/max(1000, sum(np.abs(result)))
36/380:
print(weights)
result = derivative(X, y, weights)
36/381: result
36/382: weights += result/max(1000, sum(np.abs(result)))
36/383:
print(weights)
result = derivative(X, y, weights)
36/384: result
36/385: weights += result/max(1000, sum(np.abs(result)))
36/386:
print(weights)
result = derivative(X, y, weights)
36/387: result
36/388: weights += result/max(1000, sum(np.abs(result)))
36/389:
print(weights)
result = derivative(X, y, weights)
36/390: result
36/391: weights += result/max(1000, sum(np.abs(result)))
36/392:
print(weights)
result = derivative(X, y, weights)
36/393: result
36/394: weights += result/max(1000, sum(np.abs(result)))
36/395:
print(weights)
result = derivative(X, y, weights)
36/396: result
36/397: weights += result/max(1000, sum(np.abs(result)))
36/398:
print(weights)
result = derivative(X, y, weights)
36/399: result
36/400: weights += result/max(1000, sum(np.abs(result)))
36/401:
print(weights)
result = derivative(X, y, weights)
36/402: result
36/403: weights += result/max(1000, sum(np.abs(result)))
36/404:
print(weights)
result = derivative(X, y, weights)
36/405: result
36/406: weights += result/max(1000, sum(np.abs(result)))
36/407:
print(weights)
result = derivative(X, y, weights)
36/408: result
36/409: weights += result/max(1000, sum(np.abs(result)))
36/410:
print(weights)
result = derivative(X, y, weights)
36/411: result
36/412: weights += result/max(1000, sum(np.abs(result)))
36/413:
print(weights)
result = derivative(X, y, weights)
36/414: result
36/415: weights += result/max(1000, sum(np.abs(result)))
36/416:
print(weights)
result = derivative(X, y, weights)
36/417: result
36/418: weights += result/max(1000, sum(np.abs(result)))
36/419:
print(weights)
result = derivative(X, y, weights)
36/420: result
36/421: weights += result/max(1000, sum(np.abs(result)))
36/422:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = abs(sum(result))
        weights += result/1000
    return weights
36/423:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    return weights
36/424:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        for iter in range(50):
            count += 1
            result = derivative(X_train, Y_train, weights)
            s = sum(abs(result))
            weights += result/1000
            if s < epsilon:
                break
            print(weights)
            input()
    return weights
36/425:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/1:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        for iter in range(50):
            count += 1
            result = derivative(X_train, Y_train, weights)
            s = sum(abs(result))
            weights += result/1000
            if s < epsilon:
                break
        print(weights)
        input()
    return weights
37/2:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/3:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
37/4: df = pd.read_csv('Data_Mendeley.csv')
37/5: df.head()
37/6:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
37/7:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
weights = np.ones(len(X[0]) + 1)
37/8:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        for iter in range(50):
            count += 1
            result = derivative(X_train, Y_train, weights)
            s = sum(abs(result))
            weights += result/1000
            if s < epsilon:
                break
        print(weights)
        input()
    return weights
37/9:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/10:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        for iter in range(50):
            count += 1
            result = derivative(X_train, Y_train, weights)
            s = sum(abs(result))
            weights += result/1000
            if s < epsilon:
                break
        print(weights)
        print(result)
        input()
    return weights
37/11:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/12:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        for iter in range(50):
            count += 1
            result = derivative(X_train, Y_train, weights)
            s = sum(abs(result))
            weights -= result/1000
            if s < epsilon:
                break
        print(weights)
        print(result)
        input()
    return weights
37/13:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/14:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/15:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        for iter in range(50):
            count += 1
            result = derivative(X_train, Y_train, weights)
            s = sum(abs(result))
            weights += result/1000
            if s < epsilon:
                break
        print(weights)
        print(result)
        input()
    return weights
37/16:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/17:
X = np.array([[1], [4], [7]])
y = np.array([7, 16, 25])
linear_regression(X, y)
37/18:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    return weights
37/19:
X = np.array([[1], [4], [7]])
y = np.array([7, 16, 25])
linear_regression(X, y)
37/20:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/21:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    print(result)
    return weights
37/22:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/23: import sklearn
37/24: from sklearn.linear_model import LinearRegression
37/25: model = LinearRegression()
37/26: model.fit(X, y)
37/27: model.train(X, y)
37/28: model.fit(X, y)
37/29: model.coef_
37/30: X
37/31: model.predict([[1, 2, 3]])
37/32: model.predict([[4, 5, 6]])
37/33:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train[0]) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    return weights
37/34: model.coef_
37/35: model.coef
37/36: model.get_params()
37/37: model.intercept
37/38: model.intercept()
37/39: model.intercept_
37/40: model.coef
37/41: model.fit(X, y)
37/42: model.intercept_
37/43: model.coef_
37/44:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
linear_regression(X, y)
37/45: model.predict(X)
37/46:
def predict(X, weights):
    np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.prod(X, weights.T)
37/47: predict(X, weights)
37/48:
def predict(X, weights):
    np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.dot(X, weights.T)
37/49:
def predict(X, weights):
    np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
37/50: predict(X, weights)
37/51:
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([15, 38, 50])
weights = linear_regression(X, y)
37/52: predict(X, weights)
37/53:
def predict(X, weights):
    print(X, weights)
    np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
37/54: predict(X, weights)
37/55:
def predict(X, weights):
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
37/56: predict(X, weights)
37/57:
def loss(y,y_pred): #Mean Squared Loss
  return np.matmul((y-y_pred).T, (y-y_pred))
37/58: return loss([1, 2, 3], [2, 3, 4])
37/59:  loss([1, 2, 3], [2, 3, 4])
37/60:  loss(np.array([1, 2, 3]), np.array([2, 3, 4]))
37/61:
def loss(y,y_pred): #Mean Squared Loss
    if y.shape[1] == 1:
        return np.matmul((y-y_pred).T, (y-y_pred))
    if y.shape[0] == 1:
        return np.matmul(y-y_pred, (y-y_pred).T)
37/62: loss(np.array([[1], [2], [3]]), np.array([[2], [3], [4]]))
37/63:
def loss(y,y_pred): #Mean Squared Loss
    if y.shape[1] == 1:
        return np.matmul((y-y_pred).T, (y-y_pred))
    if y.shape[0] == 1:
        return np.matmul(y-y_pred, (y-y_pred).T)[0,0]
37/64: loss(np.array([[1], [2], [3]]), np.array([[2], [3], [4]]))
37/65:
def loss(y,y_pred): #Mean Squared Loss
    if y.shape[1] == 1:
        print("HERE")
        return np.matmul((y-y_pred).T, (y-y_pred))
    if y.shape[0] == 1:
        return np.matmul(y-y_pred, (y-y_pred).T)[0,0]
37/66: loss(np.array([[1], [2], [3]]), np.array([[2], [3], [4]]))
37/67: loss(np.ones(4), np.zeros(4))
37/68:
def loss(y,y_pred): #Mean Squared Loss
    print(y.shape)
    if y.shape[1] == 1:
        print("HERE")
        return np.matmul((y-y_pred).T, (y-y_pred))
    if y.shape[0] == 1:
        return np.matmul(y-y_pred, (y-y_pred).T)[0,0]
37/69: loss(np.ones(4), np.zeros(4))
37/70: print(np.zeros(4).info())
37/71: print(np.info(np.zeros(4)))
37/72:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    if y.shape[0] == 1:
        return np.matmul((y-y_pred).T, (y-y_pred))
    if y.shape[1] == 1:
        return np.matmul(y-y_pred, (y-y_pred).T)[0,0]
37/73: print(np.info(np.zeros(4)))
37/74:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    if y.shape[0]:
        return np.matmul((y-y_pred).T, (y-y_pred))
    else:
        return np.matmul(y-y_pred, (y-y_pred).T)[0,0]
37/75: print()
37/76: print(loss(np.zeros(4), np.ones(4)))
37/77: print(loss(np.array([1, 2, 3]), np.array([4, 5, 6])))
37/78: print(loss(np.array([1, 2, 3]), np.array([4, 7, 6])))
37/79: print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
37/80:
# print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
print(np.array([[4], [7], [6]])
37/81:
# print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
print(np.array([[4], [7], [6]]))
37/82:
# print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
print(np.array([[4], [7], [6]]).ndim)
37/83:
# print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
print(loss(pd.Series([1, 2, 3]), pd.Series([4, 7, 6])))
37/84:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    return np.matmul((y-y_pred).T, (y-y_pred))
37/85:
# print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
print(loss(pd.Series([1, 2, 3]), pd.Series([4, 7, 6])))
37/86:
# print(loss(np.array([[1], [2], [3]]), np.array([[4], [7], [6]])))
print(loss(pd.Series([1, 2, 3]), pd.Series([4, 7, 9])))
37/87: df.columns()
37/88: df.columns
37/89: X_test = df["Ave", "StrRate"]
37/90: X_test = df.loc["Ave", "StrRate"]
37/91:
X_test = df[["Ave", "StrRate"]]
X_test
37/92:
X_test = df[["Ave", "StrRate"]]
y_test = df["Final Price"]
37/93:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
37/94: help(train_test_split)
37/95: X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.33, random_state=101)
37/96: X_train
37/97:
X_train = df[["Ave", "StrRate"]]
y_train = df["Final Price"]
37/98: X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=101)
37/99: weights = linear_regression(X_train, y_train)
37/100:
def linear_regression(X_train, Y_train, alpha = 0.1):
    weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    return weights
37/101: weights = linear_regression(X_train, y_train)
37/102: weights = linear_regression(X_train[:3], y_train[:3])
37/103: X_train[:3]
37/104: y_train[:3]
37/105: df.countna()
37/106: df.countna
37/107: df.isna
37/108: df.isnull().sum()
37/109: df["Ave" == 0].sum()
37/110: df[df["Ave"] == 0].sum()
37/111: df[df["Ave"] == 0].count()
37/112:
X_train = df[["Ave", "StrRate"]][df["Ave"] && df["StrRate"]]
y_train = df["Final Price"][df["Ave"] && df["StrRate"]]
37/113:
X_train = df[["Ave", "StrRate"]][(df["Ave"]) & (df["StrRate"])]
y_train = df["Final Price"][df["Ave"] & df["StrRate"]]
37/114:
X_train = df[["Ave", "StrRate"]][(df["Ave"] != 0) & (df["StrRate"] != 0)]
y_train = df["Final Price"][(df["Ave"] != 0) & (df["StrRate"] != 0)]
37/115: X_train[X_train["Ave"] == 0].count()
37/116: X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=101)
37/117: X_train[:3]
37/118: y_train[:3]
37/119: weights = linear_regression(X_train[:3], y_train[:3])
37/120:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
weights = linear_regression(X, y)
37/121:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == "list":
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    return weights
37/122:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
weights = linear_regression(X, y)
37/123:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
print(type(X))
y = [1000000, 28000000, 1200000]
weights = linear_regression(X, y)
37/124:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        weights += result/1000
    return weights
37/125:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
weights = linear_regression(X, y)
37/126:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
37/127:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(abs(result))
        print(result)
        weights += result/1000
    return weights
37/128:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
weights = linear_regression(X, y)
37/129:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
# y = [1000000, 28000000, 1200000]
y = [1, 28, 12]
weights = linear_regression(X, y)
37/130:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(1000, s)
    return weights
37/131:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
# y = [1, 28, 12]
weights = linear_regression(X, y)
37/132:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += 1000*result/max(1e6, s)
    return weights
37/133:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
# y = [1, 28, 12]
weights = linear_regression(X, y)
38/1:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
38/2: df = pd.read_csv('Data_Mendeley.csv')
38/3: df.head()
38/4:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    return np.matmul((y-y_pred).T, (y-y_pred))
38/5:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
38/6:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(1000, s)
    return weights
38/7:
def predict(X, weights):
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
38/8:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
38/9:
X_train = df[["Ave", "StrRate"]][(df["Ave"] != 0) & (df["StrRate"] != 0)]
y_train = df["Final Price"][(df["Ave"] != 0) & (df["StrRate"] != 0)]
38/10: X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=101)
38/11: X_train[:3]
38/12: y_train[:3]
38/13:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
# y = [1, 28, 12]
weights = linear_regression(X, y)
38/14:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, s)
    return weights
38/15:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
# y = [1, 28, 12]
weights = linear_regression(X, y)
39/1:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
39/2: df = pd.read_csv('Data_Mendeley.csv')
39/3: df.head()
39/4:
def forward(x):
  pass
39/5:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    return np.matmul((y-y_pred).T, (y-y_pred))
39/6:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
39/7:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/100
    return weights
39/8:
def predict(X, weights):
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
39/9:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
39/10:
X_train = df[["Ave", "StrRate"]][(df["Ave"] != 0) & (df["StrRate"] != 0)]
y_train = df["Final Price"][(df["Ave"] != 0) & (df["StrRate"] != 0)]
39/11: X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=101)
39/12: X_train[:3]
39/13: y_train[:3]
39/14: weights = linear_regression(X_train[:3], y_train[:3])
39/15:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(1000, s/1000)
    return weights
39/16: weights = linear_regression(X_train[:3], y_train[:3])
39/17:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(1000, s/1000)
    return weights
39/18: weights = linear_regression(X_train[:3], y_train[:3])
39/19:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, s/1000)
    return weights
39/20:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, s/10000)
    return weights
39/21:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, sqrt(s)/1000)
    return weights
39/22: weights = linear_regression(X_train[:3], y_train[:3])
39/23:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, np.sqrt(s)/1000)
    return weights
39/24: weights = linear_regression(X_train[:3], y_train[:3])
39/25:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(1000, s/1000)
    return weights
39/26: weights = linear_regression(X_train[:3], y_train[:3])
39/27:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, s/1000)
    return weights
39/28: weights = linear_regression(X_train[:3], y_train[:3])
39/29:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 500:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, s/1000)
    return weights
39/30:
def linear_regression(X_train, Y_train, alpha = 0.1):
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    s = abs(sum(weights))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        print(result)
        weights += result/max(100, s/1000)
    return weights
39/31: weights = linear_regression(X_train[:3], y_train[:3])
39/32:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
39/33:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
40/1:
def linear_regression(X_train, Y_train, alpha = 0.1):
    X_train = StandardScaler.fit(X_train)
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s > prev_s:
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/2:
def linear_regression(X_train, Y_train, alpha = 0.1):
    X_train = StandardScaler.fit(X_train)
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s >= prev_s and s > epsilon:
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/3:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/4:
#import important libraries
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
40/5: df = pd.read_csv('Data_Mendeley.csv')
40/6: df.head()
40/7:
def forward(x):
  pass
40/8:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    return np.matmul((y-y_pred).T, (y-y_pred))
40/9:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
40/10:
def linear_regression(X_train, Y_train, alpha = 0.1):
    X_train = StandardScaler.fit(X_train)
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s >= prev_s and s > epsilon:
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/11:
def predict(X, weights):
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
40/12:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
40/13:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/14:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list:
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s >= prev_s and s > epsilon:
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/15:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/16:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s >= prev_s and s > epsilon:
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/17:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/18:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s >= prev_s and s > epsilon:
            print("HERE")
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/19:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/20:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 100
        while s >= prev_s and s > epsilon:
            print(result)
            print(s)
            weights += result/alpha
            alpha *= 10
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/21:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            print(result)
            print(s)
            weights -= result/alpha
            alpha *= 10
            weights += result/alpha
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/22:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            weights += result/alpha
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/23:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/24:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            weights += result/alpha
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/25:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/26:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(result)
                print(weights)
                print(type(result))
                print(type(weights))
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/27:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/28:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/29:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/30:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # For Coding
    weights = [1.88646764 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = np.norm(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = np.norm(result)
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = np.norm(result)
        print(result)
    return weights
40/31:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # For Coding
    weights = np.array([1.88646764, 1.07855746])
    result = derivative(X_train, Y_train, weights)
    s = np.norm(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = np.norm(result)
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = np.norm(result)
        print(result)
    return weights
40/32:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/33:
#import important libraries
import numpy as np
from numpy.linalg import norm
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
40/34:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # For Coding
    weights = np.array([1.88646764, 1.07855746])
    result = derivative(X_train, Y_train, weights)
    s = np.norm(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = np.norm(result)
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = np.norm(result)
        print(result)
    return weights
40/35:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/36:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # For Coding
    weights = np.array([1.88646764, 1.07855746])
    result = derivative(X_train, Y_train, weights)
    s = norm(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = norm(result)
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = norm(result)
        print(result)
    return weights
40/37:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/38:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # For Coding
    weights = np.array([1.88646764, 1.07855746])
    result = derivative(X_train, Y_train, weights)
    s = norm(result)
    epsilon = 1e-5
    count = 0
    print(result)
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = norm(result)
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = norm(result)
        print(result)
    return weights
40/39:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/40:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # For Coding
    weights = np.array([1.88646764, 1.07855746])
    result = derivative(X_train, Y_train, weights)
    s = norm(result)
    epsilon = 1e-5
    count = 0
    print(result)
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = norm(result)
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            print(s, prev_s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return 0
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = norm(result)
        print(result)
    return weights
40/41:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/42:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/43:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except UFuncTypeError as error:
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/44:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except UFuncTypeError as error:
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/45:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/46:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except error:
                print(alpa)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/47:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except error:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/48:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/49:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/50:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/51:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        count2 = 0
        while s >= prev_s and s > epsilon:
            count2 += 1
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(count2)
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/52:
X_train = np.array([[5], [7.5], [12.5]])
Y_train = np.array([10, 15, 25])
weights = linear_regression(X_train, Y_train)
40/53:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/54:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/55: 5(101.8864676451.07855746) + 7.5(151.886467647.51.07855746) + 12.5(2512.51.886467641.07855746)
40/56: 5*(101.8864676451.07855746) + 7.5*(151.886467647.51.07855746) + 12.5*(2512.51.886467641.07855746)
40/57: 5*(101.8864676451.07855746) + 7.5*(151.886467647.51.07855746) + 12.5*(2512.51.886467641.07855746)
40/58: 5*(101.88646764*51.07855746) + 7.5*(151.88646764*7.51.07855746) + 12.5*(2512.5*1.886467641.07855746)
40/59: 5*(10 - 1.88646764*5 - 1.07855746) + 7.5*(15 - 1.88646764*7.5 - 1.07855746) + 12.5*(25 -12.5*1.88646764 - 1.07855746)
40/60:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(10):
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/61:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/62:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(10):
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/63:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/64:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(10):
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
        print(s >= prev_s)
    return weights
40/65:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/66:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(count)
        print(weights)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/67:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/68:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/69:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
    return weights
40/70:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/71:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s >= prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(s, prev_s)
        print(result)
    return weights
40/72:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/73:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result)) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(s, prev_s)
        print(result)
    return weights
40/74:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/75:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = norm(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = norm(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = norm(result)
        print(s, prev_s)
        print(result)
    return weights
40/76:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/77:
def evaluation_func(result):
    return sum(np.abs(result))
40/78:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights
40/79:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/80:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/81:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        print(weights)
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights
40/82:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/83:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/84:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(10):
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
        print(s >= prev_s)
    return weights
40/85:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/86:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(18):
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
        print(s >= prev_s)
    return weights
40/87:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/88: model = LinearRegression()
40/89: model.fit(X_train, Y_train)
40/90: model.coef_
40/91: model.intercept
40/92: model.intercept_
40/93:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        print(weights)
        print(s, prev_s)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights
40/94:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights
40/95:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/96:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/97:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon and s/alpha > 1e-7:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/98:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/99:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/100:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/101:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_s:
            weights -= result/aloha
            alpha /= 10
            weights += result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/102:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/103:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_s:
            weights -= result/alpha
            alpha /= 10
            weights += result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/104:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/105:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_s:
            weights -= prev_result/alpha
            alpha /= 10
            weights += result/alpha
            result = derivative(X_train, Y_train, weights)
            prev_result = np.copy(result)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/106:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/107:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while abs(s-prev_s) < epsilon:
            weights -= prev_result/alpha
            alpha /= 10
            weights += result/alpha
            result = derivative(X_train, Y_train, weights)
            prev_result = np.copy(result)
            s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/108:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
41/1:
#importing libraries
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
41/2:
#importing libraries
!pip install keras
# import pandas as pd
# import numpy as np
# from keras.datasets import mnist
# import matplotlib.pyplot as plt
41/3:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
41/4:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
41/5: import keras
41/6: import keras
41/7:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
41/8: import tensorflow.keras
41/9: import tensorflow
41/10:

# Check TensorFlow version
print("TensorFlow version:", tf.__version__)

# Create a simple TensorFlow constant
hello = tf.constant("Hello, TensorFlow!")

# Start a TensorFlow session
with tf.compat.v1.Session() as sess:
    # Run the session to evaluate the constant
    result = sess.run(hello)
    print(result.decode())
41/11: import tensorflow as tf
41/12:

# Check TensorFlow version
print("TensorFlow version:", tf.__version__)

# Create a simple TensorFlow constant
hello = tf.constant("Hello, TensorFlow!")

# Start a TensorFlow session
with tf.compat.v1.Session() as sess:
    # Run the session to evaluate the constant
    result = sess.run(hello)
    print(result.decode())
41/13:

# creating nodes in computation graph
node = tf.Variable(tf.zeros([2,2]))
 
# running computation graph
with tf.Session() as sess:
 
    # initialize all global variables 
    sess.run(tf.global_variables_initializer())
 
    # evaluating node
    print("Tensor value before addition:\n",sess.run(node))
 
    # elementwise addition to tensor
    node = node.assign(node + tf.ones([2,2]))
 
    # evaluate node again
    print("Tensor value after addition:\n", sess.run(node))
41/14:
import tensorflow as tf

# Check TensorFlow version
print("TensorFlow version:", tf.__version__)

# Create a simple TensorFlow constant
hello = tf.constant("Hello, TensorFlow!")

# Eager execution is enabled by default in TensorFlow 2.x
# In eager mode, you don't need a session, and you can directly print the result
print(hello.numpy().decode())
41/15:
from tensorflow.keras import layers, models

# Check TensorFlow version
print("TensorFlow version:", tf.__version__)

# Create a simple neural network
model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# Display the model summary
model.summary()

# Create a random input sample
random_input = tf.random.normal((1, 784))

# Get the model's prediction for the random input
prediction = model(random_input)

# Print the prediction
print("Model prediction:", prediction.numpy())
41/16: import tensorflow.keras
41/17: import tensorflow.keras
41/18:
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])
41/19: import keras
41/20:
import tensorflow as tf
import imutils
41/21: import keras
41/22: import tf.keras as kr
41/23: import tensorflow.keras as kr
41/24:
import tensorflow as tf
import imutils
41/25: print("TensorFlow version:", tf.__version__)
41/26: import tensorflow.python import keras
41/27: from tensorflow.python import keras
41/28:
model = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])
41/29:
print("TensorFlow version:", tf.__version__)

# Create a simple neural network
model = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# Display the model summary
model.summary()

# Create a random input sample
random_input = tf.random.normal((1, 784))

# Get the model's prediction for the random input
prediction = model(random_input)

# Print the prediction
print("Model prediction:", prediction.numpy())
41/30:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
41/31:
import tensorflow as tf
import imutils
41/32: print("TensorFlow version:", tf.__version__)
41/33: from tensorflow.python import keras
41/34:
model = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])
41/35:
print("TensorFlow version:", tf.__version__)

# Create a simple neural network
model = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# Display the model summary
model.summary()

# Create a random input sample
random_input = tf.random.normal((1, 784))

# Get the model's prediction for the random input
prediction = model(random_input)

# Print the prediction
print("Model prediction:", prediction.numpy())
41/36:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
41/37:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from tensorflow.python.keras.datasets import mnist
import matplotlib.pyplot as plt
40/109:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(18):
            print(result)
            print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
        print(s >= prev_s)
    return weights
40/110:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/111:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    count = 0
    for j in range(1):
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(18):
            print(result)
            print(s, prev_s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
        print(s >= prev_s)
    return weights
40/112:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/113:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        # while abs(s-prev_s) < epsilon:
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/114:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
42/1:
import numpy as np
import matplotlib.pyplot as plt
42/2:
x = np.array([1000,100,300,500,750,1100,900,1250,2000,1500,2500,1600,1700,3000,10000,20000,25000,35000])
y = np.array([3.4,0.428,1.160,1.86,2.66,3.68,3.120,4.04,5.28,4.56,5.84,4.72,4.96,6.24,7.4,7.6,7.6])
42/3: assert(len(x) == len(y))
42/4: len(x)
42/5: len(y)
42/6:
x = np.array([1000,100,300,500,750,1100,900,1250,2000,1500,2500,1600,1700,3000,10000,20000,25000,35000])
y = np.array([3.4,0.428,1.160,1.86,2.66,3.68,3.120,4.04,5.28,4.56,5.84,4.72,4.96,6.24,7.4,7.6,7.6,7.6])
42/7: len(y)
42/8: plt.plot(x,y)
42/9: plt.plot(x,y,scale='log')
42/10:
plt.xscale('log')
plt.plot(x,y)
42/11:
plt.xscale('log')
plt.plot(1/x,y)
42/12:
plt.xscale('log')
plt.plot(x,1/y)
42/13:
# plt.xscale('log')
plt.plot(x,1/y)
42/14:
# plt.xscale('log')
plt.plot(1/x,y)
42/15: my_Dict = {x[i]: y[i] for i in range(len(x))}
42/16:
# plt.xscale('log')
plt.plot(list(my_Dict.keys()),list(my_Dict.values()))
42/17:
# plt.xscale('log')
plt.plot(list(my_Dict.keys()),list(my_Dict.values()))
42/18:
plt.xscale('log')
plt.plot(list(my_Dict.keys()),list(my_Dict.values()))
42/19:
plt.xscale('log')
plt.plot(list(my_Dict.keys()),'bo'list(my_Dict.values()))
42/20:
plt.xscale('log')
plt.plot(list(my_Dict.keys()),'bo',list(my_Dict.values()))
42/21:
plt.xscale('log')
plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
42/22: plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
42/23:
plt.axhline(7.8*np.sqrt(2))
plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
42/24:
plt.axhline(7.8*np.sqrt(1/2))
plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
42/25:
plt.axhline(7.8*np.sqrt(1/2))
plt.xscale('log')
plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
42/26:
x = np.array([1000,100,300,500,750,1100,900,1250,2000,1500,2500,1600,1700,3000,10000,20000,25000,35000,40000,50000])
y = np.array([3.4,0.428,1.160,1.86,2.66,3.68,3.120,4.04,5.28,4.56,5.84,4.72,4.96,6.24,7.4,7.6,7.6,7.6,7.6,7.6])
42/27: my_Dict = {x[i]: y[i] for i in range(len(x))}
42/28:
plt.axhline(7.8*np.sqrt(1/2))
plt.xscale('log')
plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
42/29:
plt.axhline(7.8*np.sqrt(1/2))
plt.plot(list(my_Dict.keys()),list(my_Dict.values()),'bo')
40/115:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        prev_weights = np.copy(weights)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/116:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        prev_weights = np.copy(weights)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/117:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/118:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        prev_weights = np.copy(weights)
        while s == prev_s:
            print("HERE")
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/119:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/120:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            # print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(s, prev_s)
        while s == prev_s:
            print("HERE")
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(count)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/121:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 1000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            # print(s, prev_s)
            # print(result)
            # print(s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        while s == prev_s:
            print("HERE")
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/122:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/123:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 10000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/124:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/125:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 10000 and max(result)/alpha > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/126:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/127:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 10000 and max(result)/100 > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/128:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/129: weights
40/130:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 10000 and max(np.abs(result))/100 > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/131:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < 10000:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon and max(np.abs(result))/alpha > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/132:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/133:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s and s > epsilon and max(np.abs(result))/alpha > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/134:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
40/135:
#importinf libraries
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
40/136:
breast_cancer = datasets.load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target
40/137:
# Building the model
m = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 1000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X
x = np.array([5, 7.5, 12.5])
Y = np.array([10, 15, 25])

# Performing Gradient Descent 
for i in range(epochs): 
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c
    
print (m, c)
40/138:
# Building the model
m = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 1000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X
X = np.array([5, 7.5, 12.5])
Y = np.array([10, 15, 25])

# Performing Gradient Descent 
for i in range(epochs): 
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c
    
print (m, c)
40/139:
# Building the model
m = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 10000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X
X = np.array([5, 7.5, 12.5])
Y = np.array([10, 15, 25])

# Performing Gradient Descent 
for i in range(epochs): 
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c
    
print (m, c)
40/140:
# Building the model
m = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 100000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X
X = np.array([5, 7.5, 12.5])
Y = np.array([10, 15, 25])

# Performing Gradient Descent 
for i in range(epochs): 
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c
    
print (m, c)
40/141:
# Building the model
m = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 1000000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X
X = np.array([5, 7.5, 12.5])
Y = np.array([10, 15, 25])

# Performing Gradient Descent 
for i in range(epochs): 
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c
    
print (m, c)
40/142:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = sum(np.abs(result))
    epsilon = 1e-5
    epochs = 100000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = sum(np.abs(result))
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        for i in range(18):
            print(result)
            print(s, prev_s)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = sum(np.abs(result))
        print(result)
        print(s >= prev_s)
    return weights
40/143:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/144:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 10
        weights += result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(result)
        print(s >= prev_s)
    return weights
40/145:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 10
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(result)
        print(s >= prev_s)
    return weights
40/146:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/147:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 100
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 10
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(result)
        print(s >= prev_s)
    return weights
40/148:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/149:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 1000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 10
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(result)
        print(s >= prev_s)
    return weights
40/150:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/151:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 10
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(result)
        print(s >= prev_s)
    return weights
40/152:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/153:
# Building the model
m = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 1000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X
X = np.array([5, 7.5, 12.5])
Y = np.array([10, 15, 25])

# Performing Gradient Descent 
for i in range(epochs): 
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c
    
print (m, c)
40/154:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/155:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 100
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        print(result)
        print(s >= prev_s)
    return weights
40/156:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/157:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        print(count)
        print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(weights)
        print(result)
    return weights
40/158:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 100
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_result:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        print(result)
    return weights
40/159:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/160:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 100
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        print(result)
    return weights
40/161:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linreg(X_train, Y_train)
40/162:
#spliting data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
40/163:
breast_cancer = datasets.load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target
40/164:
#spliting data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
43/1:
#import important libraries
import numpy as np
from numpy.linalg import norm
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
43/2: df = pd.read_csv('Data_Mendeley.csv')
43/3:
#import important libraries
import numpy as np
from numpy.linalg import norm
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
43/4:
#import important libraries
!pip install sklearn
# import numpy as np
# from numpy.linalg import norm
# from sklearn import datasets
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.preprocessing import StandardScaler
43/5:
#import important libraries
!pip install scikit-learn
# import numpy as np
# from numpy.linalg import norm
# from sklearn import datasets
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.preprocessing import StandardScaler
43/6:
#import important libraries
import numpy as np
from numpy.linalg import norm
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
43/7: df = pd.read_csv('Data_Mendeley.csv')
43/8: df.head()
43/9:
def forward(x):
  pass
43/10:
def loss(y,y_pred): #Mean Squared Loss
    assert(y.ndim == 1 and y_pred.ndim == 1)
    return np.matmul((y-y_pred).T, (y-y_pred))
43/11:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
43/12:
def evaluation_func(result):
    return sum(np.abs(result))
43/13:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        print(count)
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(weights)
        print(result)
    return weights
43/14:
def predict(X, weights):
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
43/15:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
43/16:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/17:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        print(count)
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(weights)
        print(result)
    return weights
43/18:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/19:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        print(count)
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/20:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/21:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        print(count)
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/22:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/23:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/24:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/25:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights -= prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/26:
def predict(X, weights):
    X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    return np.matmul(X, weights.T)
43/27:
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
43/28:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/29:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/30:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
43/31:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/32:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/33:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/34:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/35:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/36:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    # weights = [1.88646764, 1.07855746]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/37:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/38:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.64, 1.054]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/39:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/40:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.82876,  1.069276]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/41:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/42:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88447144, 1.07313474]

    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/43:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/44:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.90095335 1.07362765]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/45:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.90095335, 1.07362765]
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count == 0:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/46:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/47:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = np.array([1.90586835, 1.07312868])
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count <= 20:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/48:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/49:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = np.array([1.90952263, 1.05428618])
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count <= 20:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/50:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/51:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = np.array([1.91112565, 1.03560704])
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count <= 20:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/52:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/53:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = np.array([1.91270027, 1.01725884])
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon and count <= 20:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        weights += prev_result/alpha
        # print(s, prev_s)
        # while s == prev_s:
        #     weights += prev_result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     s = evaluation_func(result)
        #     alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/54:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/55:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = np.array([1.91270027, 1.01725884])
    result = derivative(X_train, Y_train, weights)
    print(result)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            print("HERE", weights)
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/56:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        print(count)
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return []
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        print(weights)
        # print(result)
    return weights
43/57:
X_train = [[5], [10], [15]]
Y_train = [10, 20, 30]
weights = linear_regression(X_train, Y_train)
43/58:
X_train = [[5], [7.5], [10]]
Y_train = [10, 15, 20]
weights = linear_regression(X_train, Y_train)
43/59:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/60: weights
43/61:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except Exception as error:
                print(error)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
            print(result)
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/62:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/63:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    count = 0
    while s > epsilon:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except Exception as error:
                print(error)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/64:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/65: weights
43/66: iter_count
43/67:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [1000000, 28000000, 1200000]
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/68:
X = [[33, 84.61], [22.8, 107.54], [22.33, 126.41]]
y = [66, 45.6, 44.66]
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/69:
X = [[33], [22.8], [22.33]]
y = [66, 45.6, 44.66]
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/70:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except Exception as error:
                print(error)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/71:
X = [[1, 2], [2, 3], [4, 5]]
y = [3, 5, 9]
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/72: weights
43/73:
X = [[33.00, 84,61], [22.80, 107.54], [22.33, 126.41]]
y = [66, 45.6, 44.66]
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/74:
X = np.array([[33.00, 84,61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/75:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/76:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print(result.dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/77:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/78:
w = [0.89234235 0.24856174 0.99427802]
alpha = 100000000000000000000
r = [251.57886218  -7.39554165   4.37662853]
w += r/alpha
43/79:
w = [0.89234235, 0.24856174, 0.99427802]
alpha = 100000000000000000000
r = [251.57886218, -7.39554165,   4.37662853]
w += r/alpha
43/80:
w = np.array([0.89234235, 0.24856174, 0.99427802])
alpha = 100000000000000000000
r = np.array([251.57886218, -7.39554165,   4.37662853])
w += r/alpha
43/81:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 100
        weights += prev_result/alpha
        while s > prev_s:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except:
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        print(result)
    return weights
43/82:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/83:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/84:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += prev_result/alpha
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/85:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/86:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/87:
def linreg(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    weights = [1.88646764, 1.07855742]
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epsilon = 1e-5
    epochs = 10000
    count = 0
    for iteration in range(epochs):
        if s < epsilon:
            break
        prev_result = np.copy(result)
        prev_s = s
        print(prev_s)
        print(weights)
        print(result)
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        alpha = 100
        weights += prev_result/alpha
        while s > prev_s:
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except:
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return []
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        print(result)
    return weights
43/88:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= prev_result/alpha
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/89:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= (prev_result/alpha).astype(float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/90:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/91:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/92:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/93: weights
43/94:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/95:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights, count
43/96:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/97:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon:
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/98:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/99:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha > 1e-7:
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/100:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/101:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha > 1e-7:
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/102:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/103:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha > 1e-7:
            print(alpha)
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/104:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/105:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha <= 1e-7:
            print(alpha)
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        # print(s, prev_s)
        # print(result)
    return weights, count
43/106:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/107:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha <= 1e-7:
            print(alpha)
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights, count
43/108:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/109:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) == list or isinstance(X_train, np.ndarray):
        weights = np.ones(len(X_train[0]) + 1)
    else:
        weights = np.ones(len(X_train.columns) + 1)
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha <= 1e7:
            print(alpha)
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights, count
43/110:
X = np.array([[33.00, 84.61], [22.80, 107.54], [22.33, 126.41]])
y = np.array([66, 45.6, 44.66])
# y = [1, 28, 12]
weights = linear_regression(X, y)
43/111:
def evaluation_func(result):
    return mean(np.abs(result))
43/112:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/113:
def evaluation_func(result):
    return np.mean(np.abs(result))
43/114:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/115: weights
43/116:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return mean(weights[y]*(y-y_pred))
43/117: BCELoss([1],[0])
43/118:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return np.mean(weights[y]*(y-y_pred))
43/119: BCELoss([1],[0])
43/120: BCELoss(np.array([1]),np.array([0]))
43/121:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return np.mean(weights[y]*(y*(1-y_pred) + (1-y)*y_pred))
43/122: BCELoss(np.array([1]),np.array([0]))
43/123: BCELoss(np.array([0]),np.array([0]))
43/124: BCELoss(np.array([0]),np.array([0.5]))
43/125:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return np.mean(weights[y]*(y*log(1-y_pred) + (1-y)*log(y_pred)))
43/126: BCELoss(np.array([0]),np.array([0.5]))
43/127:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return np.mean(weights[y]*(y*math.log(1-y_pred) + (1-y)*math.log(y_pred)))
43/128: BCELoss(np.array([0]),np.array([0.5]))
43/129: import math
43/130:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return np.mean(weights[y]*(y*math.log(1-y_pred) + (1-y)*math.log(y_pred)))
43/131: BCELoss(np.array([0]),np.array([0.5]))
43/132: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/133:
def BCELoss(y,y_pred, weights=np.ones(2)):
  return np.mean(weights[y]*(y*math.log(y-y_pred) + (1-y)*math.log(y_pred)))
43/134: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/135:
def BCELoss(y,y_pred, weight):
  return np.mean(weight*(y*math.log(1-y_pred) + (1-y)*math.log(y_pred)))
43/136:
def BCELoss(y,y_pred, weights):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))
        
    return np.mean(np.vectorize(BCELoss(y, y_pred)))
43/137: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/138:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))
        
    return np.mean(np.vectorize(BCELoss(y, y_pred)))
43/139: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/140:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))
        
    return np.mean(np.vectorize(return_loss(y, y_pred)))
43/141: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/142:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
    return np.mean(np.vectorize(lambda: return_loss))
43/143: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/144:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/145: BCELoss(np.array([0, 1]),np.array([0.5, 0.75]))
43/146: BCELoss(np.array([0, 1]),np.array([0.001, 0.99]))
43/147: BCELoss(np.array([0, 1]),np.array([0.001, 0.9999]))
43/148:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        print(weight)
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/149: BCELoss(np.array([0, 1]),np.array([0.001, 0.9999]))
43/150:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        print(weight, true_val, pred)
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/151: BCELoss(np.array([0, 1]),np.array([0.001, 0.9999]))
43/152:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        print(weight, true_val, pred)
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    print(return_func(y, y_pred))
    return np.mean(return_func(y, y_pred))
43/153: BCELoss(np.array([0, 1]),np.array([0.001, 0.9999]))
43/154:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        print(weight, true_val, pred)
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/155: BCELoss(np.array([0, 1]),np.array([0.001, 0.9999]))
43/156:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/157: BCELoss(np.array([0, 1]),np.array([0.001, 0.9999]))
43/158: BCELoss(np.array([0, 1]),np.array([0.000000000000001, 0.9999]))
43/159: BCELoss(np.array([0, 1]),np.array([0.000000000000001, 0.99999999]))
43/160:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return -weight*(true_val*math.log(1-pred) + (1-true_val)*math.log(pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/161: BCELoss(np.array([0, 1]),np.array([0.000000000000001, 0.99999999]))
43/162:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return -weight*(true_val*math.log(pred) + (1-true_val)*math.log(1-pred))

    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/163: BCELoss(np.array([0, 1]),np.array([0.000000000000001, 0.99999999]))
43/164: BCELoss(np.array([0, 1]),np.array([0.999999, 0.0000001]))
43/165: BCELoss(np.array([0, 1]),np.array([1, 0.0000001]))
43/166:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        try:
            return -weight*(true_val*math.log(pred) + (1-true_val)*math.log(1-pred))
        except ValueError as error:
            print("Looks like you have input an incorrect value for the log of odds of each class. We have encountered a", error)
    
    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/167: BCELoss(np.array([0, 1]),np.array([1, 0.0000001]))
43/168:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        try:
            return -weight*(true_val*math.log(pred) + (1-true_val)*math.log(1-pred))
        except ValueError as error:
            print("Looks like you have input an incorrect value for the log of odds of each class. We have encountered a", error)
            return 0
    
    return_func = np.vectorize(return_loss)
    return np.mean(return_func(y, y_pred))
43/169: BCELoss(np.array([0, 1]),np.array([1, 0.0000001]))
43/170:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return -weight*(true_val*math.log(pred) + (1-true_val)*math.log(1-pred))
    
    return_func = np.vectorize(return_loss)
    try:
        return np.mean(return_func(y, y_pred))
    except ValueError as error:
        print("Looks like you have input an incorrect value for the log of odds of each class. We have encountered a", error)
        return 0
43/171: BCELoss(np.array([0, 1]),np.array([1, 0.0000001]))
43/172:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return -weight*(true_val*math.log(pred) + (1-true_val)*math.log(1-pred))
    
    return_func = np.vectorize(return_loss)
    try:
        return np.mean(return_func(y, y_pred))
    except ValueError as error:
        print("Looks like you have input an incorrect value for the log of odds of each class. We have encountered a", error)
        return -np.inf
43/173: BCELoss(np.array([0, 1]),np.array([1, 0.0000001]))
43/174: BCELoss(np.array([0, 1]),np.array([0.00001, 0.0000001]))
43/175:
def BCELoss(y,y_pred, weights=np.array([1,1])):
    def return_loss(true_val, pred):
        weight = weights[true_val]
        return -weight*(true_val*math.log(pred) + (1-true_val)*math.log(1-pred))
    
    return_func = np.vectorize(return_loss)
    try:
        return np.mean(return_func(y, y_pred))
    except ValueError as error:
        print("Looks like you have input an incorrect value for the log of odds of each class. We have encountered a", error)
        return np.inf
43/176: BCELoss(np.array([0, 1]),np.array([0.00001, 0.0000001]))
43/177: BCELoss(np.array([0, 1]),np.array([0.00001, 0.99999]))
43/178:
def return_probs(X, weights):
    probs = 1/(1+np.exp(-(np.dot(X.T, weights))))
    return (2*probs).astype(np.int64)
43/179: return_probs([[0,1],[1,1],[2,1]], [1, 1])
43/180: return_probs(np.array([[0,1],[1,1],[2,1]]), np.array([1, 1]))
43/181:
def return_probs(X, weights):
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    return (2*probs).astype(np.int64)
43/182: return_probs(np.array([[0,1],[1,1],[2,1]]), np.array([1, 1]))
43/183:
def return_probs(X, weights):
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    print(probs)
    return (2*probs).astype(np.int64)
43/184: return_probs(np.array([[0,1],[1,1],[2,1]]), np.array([1, 1]))
43/185: return_probs(np.array([[0,1],[1,1],[2,1]]), np.array([-1, 1]))
43/186:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
43/187:
X = [[1, 2, 3], [2, 3, 4], [5, 6, 7]]
y = [1, 2,3]
logistic_regression(X, y)
43/188:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
        print(y.shape)
    # X = np.concatenate((X, np.ones(Y.shape).T))
43/189:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/190: logistic_regression(X, y)
43/191:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.ones(Y.shape[0]).T))
43/192:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/193: logistic_regression(X, y)
43/194:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.ones(y.shape[0]).T))
43/195:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/196: logistic_regression(X, y)
43/197:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.ones(y.shape[0]).T), axis=1)
43/198:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/199: logistic_regression(X, y)
43/200:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.array([np.ones(y.shape[0]).T)]), axis=1)
43/201:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.array([np.ones(y.shape[0]).T)])), axis=1)
43/202:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.array([np.ones(y.shape[0]).T])), axis=1)
43/203:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/204: logistic_regression(X, y)
43/205:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    print(np.array([np.ones(y.shape[0]).T]))
    X = np.concatenate((X, np.array([np.ones(y.shape[0]).T])), axis=1)
43/206:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/207: logistic_regression(X, y)
43/208:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    print(np.array([np.ones(y.shape[0]).T]))
    X = np.concatenate((X, np.array([np.ones(y.shape[0])])), axis=1)
43/209:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/210: logistic_regression(X, y)
43/211:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    print(np.array([np.ones(y.shape[0]).T]).shape)
    X = np.concatenate((X, np.array([np.ones(y.shape[0])])), axis=1)
43/212:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/213: logistic_regression(X, y)
43/214:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    print(np.array([np.ones(y.shape[0])]).shape)
    X = np.concatenate((X, np.array([np.ones(y.shape[0])])), axis=1)
43/215:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/216: logistic_regression(X, y)
43/217:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    print(np.array([np.ones(y.shape[0])]).shape)
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/218:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/219: logistic_regression(X, y)
43/220:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        print("HERE")
        y = np.array(y)
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    print(X)
43/221:
X = np.zeros((3, 2))
y = [1, 2, 3]
43/222: logistic_regression(X, y)
43/223: X.shape[1]
43/224:
X = np.zeros((3, 2))
y = [1, 2, 3]
X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/225:
X = np.zeros((3, 2))
y = [1, 2, 3]
y = np.array(y)
X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/226:
X = np.zeros((1, 1))
y = [1, 2, 3]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/227: X.shape[1]
43/228:
X = np.zeros((1, 0))
y = [1, 2, 3]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/229: X.shape[1]
43/230: X
43/231:
X = np.zeros((1, 6))
y = [1, 2, 3]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/232: X.shape[1]
43/233:
X = np.zeros((7, ))
y = [1, 2, 3]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/234: X.shape[1]
43/235:
X = np.zeros((7, 0))
y = [1, 2, 3]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/236: X.shape[1]
43/237:
X = np.zeros((7, 1))
y = [1, 2, 3]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/238: X.shape[1]
43/239: X
43/240:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    if X.shape[1] + 1 == len(weights):
        X = np.concatenate((X, np.array([np.ones(len(X))]).T), axis=1)
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
43/241:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/242:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    return np.sum(result, axis=0)
43/243:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha <= 1e7:
            print(alpha)
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights, count
43/244:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/245:
def linear_regression(X_train, Y_train, alpha = 0.1):
    # X_train = StandardScaler.fit(X_train)
    if type(X_train) != np.ndarray and type(X_train) != pd.DataFrame:
        X_train = np.array(X_train)
    if type(Y_train) != np.ndarray and type(Y_train) != pd.DataFrame:
            Y_train = np.array(Y_train)
    assert(X_train.shape[0] == Y_train.shape[0])
    X_train = np.concatenate((X_train, np.array([np.ones(Y_train.shape[0])]).T), axis=1)
    weights = np.ones(X_train.shape[1])
    result = derivative(X_train, Y_train, weights)
    s = evaluation_func(result)
    epoch_time = 100000
    epsilon = 1e-5
    count = 0
    while s > epsilon and count < epoch_time:
        prev_result = np.copy(result)
        prev_s = s
        count += 1
        result = derivative(X_train, Y_train, weights)
        s = evaluation_func(result) + 1
        prev_result = np.copy(result)
        alpha = 100
        weights += result/alpha
        while s > prev_s and s > epsilon and alpha <= 1e7:
            print(alpha)
            weights -= (prev_result/alpha).astype(np.float64)
            alpha *= 10
            try:
                weights += (prev_result/alpha).astype(np.float64)
            except Exception as error:
                print(error)
                print(count)
                print(alpha)
                print(result)
                print(weights)
                print((prev_result/alpha).dtype)
                print(weights.dtype)
                return [], -1
            # prev_result = np.copy(result)
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result) 
        # weights += prev_result/alpha
        # print(s, prev_s)
        while s == prev_s:
            weights += prev_result/alpha
            result = derivative(X_train, Y_train, weights)
            s = evaluation_func(result)
            alpha /= 2
            print(count)
            print(s, prev_s)
        #     weights -= prev_result/alpha
        #     alpha /= 10
        #     weights += result/alpha
        #     result = derivative(X_train, Y_train, weights)
        #     prev_result = np.copy(result)
        #     s = evaluation_func(result)
        print(s, prev_s)
        print(result)
    return weights, count
43/246:
X_train = [[5], [7.5], [12.5]]
Y_train = [10, 15, 25]
weights, iter_count = linear_regression(X_train, Y_train)
#49766 iterations were taken to converge within 1e-5
43/247:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 10000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/248: weights = logistic_regression(X, y)
43/249:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/250: weights = logistic_regression(X, y)
43/251:
def return_probs(X, weights):
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    return probs
    print(probs)
    return (2*probs).astype(np.int64)
43/252: weights = logistic_regression(X, y)
43/253:
def return_probs(X, weights):
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    print(probs)
    return probs
    return (2*probs).astype(np.int64)
43/254: weights = logistic_regression(X, y)
43/255:
def return_probs(X, weights):
    print(weights)
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    print(probs)
    return probs
    return (2*probs).astype(np.int64)
43/256: weights = logistic_regression(X, y)
43/257:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 10000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/258:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/259: weights = logistic_regression(X, y)
43/260:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 10000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += result/alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/261:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/262: weights = logistic_regression(X, y)
43/263:
def return_probs(X, weights):
    # print(weights)
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    # print(probs)
    return probs
    return (2*probs).astype(np.int64)
43/264:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    print(result)
    # pred = return_probs(X, weights)
    # s = BCELoss(y, pred)
    # # prev_s = s
    # s += 1
    # alpha = 0.001
    # count = 0
    # epoch = 10000
    # epsilon = 1e-3
    # while count < epoch and s > epsilon:
    #     # prev_s = s
    #     count += 1
    #     weights += result/alpha
    #     result = derivative(X, y, weights)
    #     pred = return_probs(X, weights)
    #     s = BCELoss(y, pred)
    #     print(s)
    # return weights, count
43/265:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/266: weights = logistic_regression(X, y)
43/267:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    print(result)
    return np.sum(result, axis=0)
43/268: weights = logistic_regression(X, y)
43/269:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    print(result)
    return np.mean(result, axis=0)
43/270:
def evaluation_func(result):
    return np.sum(np.abs(result))
43/271: weights = logistic_regression(X, y)
43/272:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 10000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += result/alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/273:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/274: weights = logistic_regression(X, y)
43/275:
def derivative(X, y, weights):
    def derivative_row(row, res):
        return row * (res - np.dot(row, weights))

    # Vectorize the inner function
    vectorized_derivative = np.vectorize(derivative_row, signature='(n),()->(n)')

    # Apply the vectorized function to each row of X
    result = vectorized_derivative(X, y)
    # print(result)
    return np.mean(result, axis=0)
43/276:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 10000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += result/alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/277:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/278: weights = logistic_regression(X, y)
43/279:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 10000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += result*alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/280:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/281: weights = logistic_regression(X, y)
43/282: weights
43/283:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.001
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += result*alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/284:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/285: weights = logistic_regression(X, y)
43/286: weights
43/287: from sklearn.linear_model import LogisticRegression
43/288:
model = LogisticRegression()
model.fit(X, np.array(y))
43/289: pred = model.predict(y)
43/290: pred = model.predict(X)
43/291: BCELoss(y, pred)
43/292: model.coef_
43/293: model.intercept_
43/294: weights = np.array([0.68431283, -4.78950942])
43/295: probs = return_probs(X, y, weights)
43/296: probs = return_probs(X, weights)
43/297:     X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/298: X = np.concatenate((X, np.array([np.ones(np.array(y).shape[0])]).T), axis=1)
43/299: probs = return_probs(X, weights)
43/300: probs
43/301: BCELoss(y, probs)
43/302:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        # prev_s = s
        count += 1
        weights += result*alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        print(s)
    return weights, count
43/303: weights = logistic_regression(X, y)
43/304:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.ones(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        while s == prev_s:
            weights += result*alpha
            alpha *= 2 
            result = derivative(X, y, weights)
            pred = return_probs(X, weights)
            s = BCELoss(y, pred)
        print(s)
    return weights, count
43/305: weights = logistic_regression(X, y)
43/306:
class LogReg():
    def __init__(self, lr=0.001, n_iters = 1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_samples)
        self.bias = 0

        for num in range(self.n_iters):
            linear_predictions = np.dot(X, self.weights) + self.bias
            predictions = sigmoid(linear_predictions)

            dw = (1/n_samples)*(np.dot(X.T, (predictions-y)))
            db = (1/n_samples)*np.sum(predictions-y)

            self.weights -= self.lr*dw
            self.bias -= self.lr*db

    def sigmoid(self, pred):
        return 1/(1+np.exp(-pred))
43/307: log_model = LogReg()
43/308: model.fit(X, y)
43/309: type(log_model)
43/310: log_model.fit(X, y)
43/311:
X = np.array([[10], [20], [4]])
y = np.array([1, 1, 0])
43/312: log_model.fit(X, y)
43/313: log_model.fit(X, y)
43/314:
class LogReg():
    def __init__(self, lr=0.001, n_iters = 1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for num in range(self.n_iters):
            linear_predictions = np.dot(X, self.weights) + self.bias
            predictions = sigmoid(linear_predictions)

            dw = (1/n_samples)*(np.dot(X.T, (predictions-y)))
            db = (1/n_samples)*np.sum(predictions-y)

            self.weights -= self.lr*dw
            self.bias -= self.lr*db

    def sigmoid(self, pred):
        return 1/(1+np.exp(-pred))
43/315: log_model = LogReg()
43/316:
X = np.array([[10], [20], [4]])
y = np.array([1, 1, 0])
43/317: log_model.fit(X, y)
43/318:
class LogReg():
    def __init__(self, lr=0.001, n_iters = 1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for num in range(self.n_iters):
            linear_predictions = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_predictions)

            dw = (1/n_samples)*(np.dot(X.T, (predictions-y)))
            db = (1/n_samples)*np.sum(predictions-y)

            self.weights -= self.lr*dw
            self.bias -= self.lr*db

    def sigmoid(self, pred):
        return 1/(1+np.exp(-pred))
43/319: log_model.fit(X, y)
43/320:
class LogReg():
    def __init__(self, lr=0.001, n_iters = 1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for num in range(self.n_iters):
            linear_predictions = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_predictions)

            dw = (1/n_samples)*(np.dot(X.T, (predictions-y)))
            db = (1/n_samples)*np.sum(predictions-y)

            self.weights -= self.lr*dw
            self.bias -= self.lr*db

    def sigmoid(self, pred):
        return 1/(1+np.exp(-pred))
43/321: log_model.fit(X, y)
43/322:
class LogReg():
    def __init__(self, lr=0.001, n_iters = 1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def sigmoid(self, pred):
        return 1/(1+np.exp(-pred))
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for num in range(self.n_iters):
            linear_predictions = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_predictions)

            dw = (1/n_samples)*(np.dot(X.T, (predictions-y)))
            db = (1/n_samples)*np.sum(predictions-y)

            self.weights -= self.lr*dw
            self.bias -= self.lr*db
43/323: log_model = LogReg()
43/324:
X = np.array([[10], [20], [4]])
y = np.array([1, 1, 0])
43/325: log_model.fit(X, y)
43/326: log_model.weights
43/327: log_model.bias
43/328: weights = np.array([log_model.weights[0], log_model.bias])
43/329: weighta
43/330: weighta
43/331: weights
43/332: X = np.concatenate((X, np.array([np.ones(np.array(y).shape[0])]).T), axis=1)
43/333:
probs = return_probs(X, weights)
BCELoss(y, probs)
43/334:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        while s == prev_s:
            weights += result*alpha
            alpha *= 2 
            result = derivative(X, y, weights)
            pred = return_probs(X, weights)
            s = BCELoss(y, pred)
        print(s)
    return weights, count
43/335: weights = logistic_regression(X, y)
43/336:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights += result*alpha
        result = derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/337:
def logistic_derivative(X, y, preds):
    def derivative_row(row, res):
        return row*(y-res)
    
    result = np.vectorize(derivative_row, signature='(n),()->(n)')
    return np.mean(result, axis=0)
43/338:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights += result*alpha
        result = logistic_derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/339: weights = logistic_regression(X, y)
43/340:
def logistic_derivative(X, y, preds):
    def derivative_row(row, res):
        return row*(y-res)
    
    vecotrized_log_der = np.vectorize(derivative_row, signature='(n),()->(n)')
    result = vecotrized_log_der(X, preds)
    return np.mean(result, axis=0)
43/341: weights = logistic_regression(X, y)
43/342:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights -= result*alpha
        result = logistic_derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/343: weights = logistic_regression(X, y)
43/344:
def logistic_derivative(X, y, preds):
    def derivative_row(row, true_val, res):
        return row*(true_val-res)
    
    vectorized_log_der = np.vectorize(derivative_row, signature='(n),(),()->(n)')
    result = vectorized_log_der(X, preds)
    return np.mean(result, axis=0)
43/345:
def return_probs(X, weights):
    # print(weights)
    probs = 1/(1+np.exp(-(np.dot(X, weights))))
    # print(probs)
    return probs
    return (2*probs).astype(np.int64)
43/346:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights += result*alpha
        result = logistic_derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/347: weights = logistic_regression(X, y)
43/348:
def logistic_derivative(X, y, preds):
    def derivative_row(row, true_val, res):
        return row*(true_val-res)
    
    vectorized_log_der = np.vectorize(derivative_row, signature='(n),(),()->(n)')
    result = vectorized_log_der(X, y, preds)
    return np.mean(result, axis=0)
43/349:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights += result*alpha
        result = logistic_derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/350: weights = logistic_regression(X, y)
43/351:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, weights)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights -= result*alpha
        result = logistic_derivative(X, y, weights)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/352:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/353: weights = logistic_regression(X, y)
43/354:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/355: weights = logistic_regression(X, y)
43/356:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, preds)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights -= result*alpha
        result = logistic_derivative(X, y, preds)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/357:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    result = logistic_derivative(X, y, preds)
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights += result*alpha
        result = logistic_derivative(X, y, preds)
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/358:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/359: weights = logistic_regression(X, y)
43/360:
def logistic_regression(X, y):
    if type(X) != np.ndarray and type(X) != pd.DataFrame:
        X = np.array(X)
    if type(y) != np.ndarray and type(X) != pd.DataFrame:
        y = np.array(y)
    assert(X.shape[0] == y.shape[0])
    X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    weights = np.zeros(X.shape[1])*0.5
    pred = return_probs(X, weights)
    s = BCELoss(y, pred)
    result = logistic_derivative(X, y, pred)
    # prev_s = s
    s += 1
    alpha = 0.01
    count = 0
    epoch = 100000
    epsilon = 1e-3
    while count < epoch and s > epsilon:
        alpha = 0.01
        prev_s = s
        count += 1
        weights += result*alpha
        pred = return_probs(X, weights)
        s = BCELoss(y, pred)
        result = logistic_derivative(X, y, pred)
        # while s == prev_s:
        #     weights += result*alpha
        #     alpha *= 2 
        #     result = derivative(X, y, weights)
        #     pred = return_probs(X, weights)
        #     s = BCELoss(y, pred)
        print(s)
    return weights, count
43/361:
X = [[10], [20], [4]]
y = [1, 1, 0]
# y = np.array(y)
# X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
43/362: weights = logistic_regression(X, y)
43/363:
#importinf libraries
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
43/364:
breast_cancer = datasets.load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target
43/365:
#spliting data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
43/366: weights = logistic_regression(X_train, y_train)
43/367: weights
43/368:
def test_model(X_test, y_test, weights):
    pred = return_probs(X_test, weights)
    print("Binary Cross Entropy Loss on the test set is:", BCELoss(y_test, pred))
43/369: weights
43/370: weights = weights[0]
43/371: weights
43/372: test_model(X_test, y_test, weights)
43/373:
def test_model(X_test, y_test, weights):
    if type(X_test) != np.ndarray and type(X_test) != pd.DataFrame:
        X_test = np.array(X_test)
    if type(Y_test) != np.ndarray and type(Y_test) != pd.DataFrame:
        Y_test = np.array(Y_test)    
    if (len(weights) == X_test.shape[1] + 1):
        X = np.concatenate((X, np.array([np.ones(y.shape[0])]).T), axis=1)
    pred = return_probs(X_test, weights)
    print("Binary Cross Entropy Loss on the test set is:", BCELoss(y_test, pred))
43/374: test_model(X_test, y_test, weights)
43/375:
def test_model(X_test, y_test, weights):
    if type(X_test) != np.ndarray and type(X_test) != pd.DataFrame:
        X_test = np.array(X_test)
    if type(y_test) != np.ndarray and type(y_test) != pd.DataFrame:
        y_test = np.array(y_test)    
    if (len(weights) == X_test.shape[1] + 1):
        X_test = np.concatenate((X_test, np.array([np.ones(y_test.shape[0])]).T), axis=1)
    pred = return_probs(X_test, weights)
    print("Binary Cross Entropy Loss on the test set is:", BCELoss(y_test, pred))
43/376: test_model(X_test, y_test, weights)
44/1:
import tensorflow as tf
import imutils
44/2: import tensorflow as tf
44/3: print("TensorFlow version:", tf.__version__)
44/4: from tensorflow.python import keras
44/5:
model = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])
44/6:
print("TensorFlow version:", tf.__version__)

# Create a simple neural network
model = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# Display the model summary
model.summary()

# Create a random input sample
random_input = tf.random.normal((1, 784))

# Get the model's prediction for the random input
prediction = model(random_input)

# Print the prediction
print("Model prediction:", prediction.numpy())
44/7:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
# from tensorflow.python.keras.datasets import mnist
import matplotlib.pyplot as plt
45/1:
# activation and loss functions
def ReLU(x, neg_alpha = 0.01):
    return max(x, neg_alpha*x)

def derivative_ReLU():
    pass

def softmax(x):
    return 1/(1+np.exp(-2*x))
45/2:
X = np.linspace(1, 11, 10)
y = softmax(X)
plt.plot(X, y)
45/3:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
# from tensorflow.python.keras.datasets import mnist
import matplotlib.pyplot as plt
45/4:
X = np.linspace(1, 11, 10)
y = softmax(X)
plt.plot(X, y)
45/5:
X = np.linspace(0, 1, 10)
y = softmax(X)
plt.plot(X, y)
45/6:
# activation and loss functions
def ReLU(x, neg_alpha = 0.01):
    return max(x, neg_alpha*x)

def derivative_ReLU():
    pass

def softmax(x):
    return 1/(1+np.exp(1-2*x))
45/7:
X = np.linspace(0, 1, 10)
y = softmax(X)
plt.plot(X, y)
45/8:
X = np.linspace(-100, 100, 200)
y = softmax(X)
plt.plot(X, y)
45/9:
X = np.linspace(-100, 100, 200)
y = softmax(X)
K = derivative_ReLU(X)
plt.plot(X, y)
45/10:
# activation and loss functions
def Leaky_ReLU(x, neg_alpha = 0.01):
    return max(x, neg_alpha*x)

def derivative_ReLU(x, neg_alpha = 0.01):
    if x > 0:
        return 1
    if x == 0:
        return 0
    return neg_alpha

def softmax(x):
    #For two variables
    return 1/(1+np.exp(1-2*x))
45/11:
X = np.linspace(-100, 100, 200)
y = softmax(X)
K = derivative_ReLU(X)
plt.plot(X, y)
45/12:
X = np.linspace(-100, 100, 200)
y = softmax(X)
K = np.vectorize(derivative_ReLU)
K = K(X)
plt.plot(X, yK)
45/13:
X = np.linspace(-100, 100, 200)
y = softmax(X)
K = np.vectorize(derivative_ReLU)
K = K(X)
plt.plot(X, K)
45/14:
import keras
print(keras.__version__)
45/15: import tensorflow as tf
45/16: print("TensorFlow version:", tf.__version__)
45/17:
import keras
print(keras.__version__)
45/18:
#importing libraries
# !pip install keras
import pandas as pd
import numpy as np
from keras.datasets import mnist
import matplotlib.pyplot as plt
45/19:
# activation and loss functions
def Leaky_ReLU(x, neg_alpha = 0.01):
    return max(x, neg_alpha*x)

def derivative_ReLU(x, neg_alpha = 0.01):
    if x > 0:
        return 1
    if x == 0:
        return 0
    return neg_alpha

def softmax(x):
    #For two variables
    return 1/(1+np.exp(1-2*x))
45/20:
X = np.linspace(-100, 100, 200)
y = softmax(X)
K = np.vectorize(derivative_ReLU)
K = K(X)
plt.plot(X, K)
45/21:
X = np.linspace(-100, 100, 200)
y = softmax(X)
K = np.vectorize(derivative_ReLU)
K = K(X)
plt.plot(X, K)
plt.plot(X, y)
45/22: (X_train, Y_train), (X_test, Y_test) = mnist.load_data()
45/23:
# all values of pixels should be in range[0,1]
X_train
45/24:
# all values of pixels should be in range[0,1]
Y_test
45/25:
# all values of pixels should be in range[0,1]
size(X_train)
45/26:
# all values of pixels should be in range[0,1]
X_train.shape
45/27:
# all values of pixels should be in range[0,1]
X_train
45/28:
# all values of pixels should be in range[0,1]
X_train.nunique
45/29:
# all values of pixels should be in range[0,1]
X_train.sum
45/30:
# all values of pixels should be in range[0,1]
X_train.sum()
45/31:
# all values of pixels should be in range[0,1]
X_train.shape
45/32: import openCV
45/33: import PIL
45/34:
im = Image.open(X_train[0])
im.show()
45/35: from PIL import Image
45/36:
im = Image.open(X_train[0])
im.show()
45/37:
im = Image.open(Image.fromarray(X_train[0]))
im.show()
45/38:
im = Image.fromarray(X_train[0])
# im.show()
45/39: im
45/40:
im = Image.fromarray(X_train[0])
im.show()
45/41:
im = Image.fromarray(X_train[0])
im.show()
45/42: im.size
45/43:
class Layer:
    def __init__(self, num_layers = 2, stride = 10):
        self.activation_func = derivative_ReLU
        self.
45/44:
#complete the class of neural network

class NN:
  def __init__(self, num_layers = 2, stride=10):
      self.num_layers = num_layers
      self.stride=10
      self.padding=1
      self.epoch = 10000
      self.layers = np.array([Layer() for _ in range(self.num_layers)])  
    #Our parameters are 2 nodes in the input layer, 2 nodes in the hidden layer, one node in the output layer

  def train(self):
    for _ in range(len(self.epoch)):
        picture = X[np.random.randint(X.size[0])]
      
  def forward_propagation(self):
      pass

  def one_hot(self): #return a 0 vector with 1 only in the position corresponding to the value in test target
      pass

  def backward_propagation(self):
      pass

  def update_params(self):
      pass

  def get_predictions(self):
      pass

  def get_accuracy(self):
      pass

  def gradient_descent(self):
      pass

  def make_predictions(self):
      pass

  def show_prediction(self): #show the prediction and actual output for an image in mnist dataset
      pass
45/45:
class Layer:
    def __init__(self, num_layers = 2, stride = 10):
        self.activation_func = derivative_ReLU
45/46:
picture = X[np.random.randint(X.size[0])]
picture = picture.reshape((28*28,))
picture = np.concatenate((picture, np.array(np.ones(1))))
45/47:
picture = X[np.random.randint(X.size)]
picture = picture.reshape((28*28,))
picture = np.concatenate((picture, np.array(np.ones(1))))
45/48:
# picture = X[np.random.randint(X.size)]
# picture = picture.reshape((28*28,))
# picture = np.concatenate((picture, np.array(np.ones(1))))
X.shape
45/49:
picture = X_train[np.random.randint(X_train.size)]
# picture = picture.reshape((28*28,))
# picture = np.concatenate((picture, np.array(np.ones(1))))
X.shape
45/50:
picture = X_train[np.random.randint(X_train.shape[0])]
# picture = picture.reshape((28*28,))
# picture = np.concatenate((picture, np.array(np.ones(1))))
X.shape
45/51:
picture = X_train[np.random.randint(X_train.shape[0])]
picture = picture.reshape((28*28,))
picture = np.concatenate((picture, np.array(np.ones(1))))
45/52: picture
45/53:
class Layer:
    def __init__(self, num_nodes=[]):
        self.activation_func = Leaky_ReLU

    def set_feature_number(self, number_of_features):
        self.number_of_features = number_of_features
        self.weights = [np.random.random() for i in range(self.number_of_features + 1)]
        
    def forward_propagation(self, X):
        return self.activation_func(np.dot(self.weights, X))
45/54: lay = Layer()
45/55: lay.set_feature_number(3)
45/56: lay.weights
45/57: lay.forward_propagation([1, 1, 2, 3])
45/58:
class Layer:
    def __init__(self, number_of_features = 28*28):
        self.activation_func = Leaky_ReLU
        self.number_of_features = number_of_features
        self.weights = [np.random.random() for i in range(self.number_of_features) + 1]

    def set_feature_number(self, number_of_features):
        self.number_of_features = number_of_features
        self.weights = [np.random.random() for i in range(self.number_of_features + 1)]
        
    def forward_propagation(self, X):
        return self.activation_func(np.dot(self.weights, X))
48/1:
_DATA_KEYS = ["a","b","c"]
class Device:
    def __init__(self, id):
        self._id = id
        self.records = []
        self.sent = []


    def obtainData(self) -> dict:
        """Returns a single new datapoint from the device.
        Identified by type `record`. `timestamp` records when the record was sent and `dev_id` is the device id.
        `data` is the data collected by the device."""
        if random.random() < 0.4:
            # Sometimes there's no new data
            return {}


        rec = {
            'type': 'record', 'timestamp': datetime.datetime.now().isoformat(), 'dev_id': self._id,
            'data': {kee: str(uuid.uuid4()) for kee in _DATA_KEYS}
        };self.sent.append(rec)
        return rec


    def probe(self) -> dict:
        """Returns a probe request to be sent to the SyncService.
        Identified by type `probe`. `from` is the index number from which the device is asking for the data."""
        if random.random() < 0.5:
            # Sometimes the device forgets to probe the SyncService
            return {}


        return {'type': 'probe', 'dev_id': self._id, 'from': len(self.records)}


    def onMessage(self, data: dict):
        """Receives updates from the server"""
        if random.random() < 0.6:
            # Sometimes devices make mistakes. Let's hope the SyncService handles such failures.
            return
       
        if data['type'] == 'update':
            _from = data['from']
            if _from > len(self.records):
                return
            self.records = self.records[:_from] + data['data']
48/2:
import random
import datetime
import uuid




# OBJECTIVES TODO:
# 1) Read the code and understand it.
# 2) Read the code again and understand it better.
# 3) Feel free to do 1 and 2 however many times you feel like.
# 4) Complete the SyncService implementation. Note that the SyncService.onMessage and SyncService.__init__ function signature must not be altered.
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/3:
import random
import datetime
import uuid




# OBJECTIVES TODO:
# 1) Read the code and understand it.
# 2) Read the code again and understand it better.
# 3) Feel free to do 1 and 2 however many times you feel like.
# 4) Complete the SyncService implementation. Note that the SyncService.onMessage and SyncService.__init__ function signature must not be altered.
48/4:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/5:
_DATA_KEYS = ["a","b","c"]
class Device:
    def __init__(self, id):
        self._id = id
        self.records = []
        self.sent = []


    def obtainData(self) -> dict:
        """Returns a single new datapoint from the device.
        Identified by type `record`. `timestamp` records when the record was sent and `dev_id` is the device id.
        `data` is the data collected by the device."""
        if random.random() < 0.4:
            # Sometimes there's no new data
            return {}


        rec = {
            'type': 'record', 'timestamp': datetime.datetime.now().isoformat(), 'dev_id': self._id,
            'data': {kee: str(uuid.uuid4()) for kee in _DATA_KEYS}
        };self.sent.append(rec)
        return rec


    def probe(self) -> dict:
        """Returns a probe request to be sent to the SyncService.
        Identified by type `probe`. `from` is the index number from which the device is asking for the data."""
        if random.random() < 0.5:
            # Sometimes the device forgets to probe the SyncService
            return {}


        return {'type': 'probe', 'dev_id': self._id, 'from': len(self.records)}


    def onMessage(self, data: dict):
        """Receives updates from the server"""
        if random.random() < 0.6:
            # Sometimes devices make mistakes. Let's hope the SyncService handles such failures.
            return
       
        if data['type'] == 'update':
            _from = data['from']
            if _from > len(self.records):
                return
            self.records = self.records[:_from] + data['data']
48/6:
class SyncService:
    def __init__(self):
        pass
       
    def onMessage(self, data: dict):
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
48/7:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices: _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
48/8:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/9: print(uuid.uuid4)
48/10: print(str(uuid.uuid4()))
48/11: print(str(uuid.uuid4()))
48/12: print(str(uuid.uuid4()))
48/13:
_DATA_KEYS = ["a","b","c"]
class Device:
    def __init__(self, id):
        self._id = id
        self.records = []
        self.sent = []


    def obtainData(self) -> dict:
        """Returns a single new datapoint from the device.
        Identified by type `record`. `timestamp` records when the record was sent and `dev_id` is the device id.
        `data` is the data collected by the device."""
        if random.random() < 0.4:
            # Sometimes there's no new data
            return {}


        rec = {
            'type': 'record', 'timestamp': datetime.datetime.now().isoformat(), 'dev_id': self._id,
            'data': {kee: str(uuid.uuid4()) for kee in _DATA_KEYS}
        }
        self.sent.append(rec)
        return rec


    def probe(self) -> dict:
        """Returns a probe request to be sent to the SyncService.
        Identified by type `probe`. `from` is the index number from which the device is asking for the data."""
        if random.random() < 0.5:
            # Sometimes the device forgets to probe the SyncService
            return {}


        return {'type': 'probe', 'dev_id': self._id, 'from': len(self.records)}


    def onMessage(self, data: dict):
        """Receives updates from the server"""
        if random.random() < 0.6:
            # Sometimes devices make mistakes. Let's hope the SyncService handles such failures.
            return
       
        if data['type'] == 'update':
            _from = data['from']
            if _from > len(self.records):
                return
            self.records = self.records[:_from] + data['data']
48/14:
_DATA_KEYS = ["a","b","c"]
class Device:
    def __init__(self, id):
        self._id = id
        self.records = []
        self.sent = []


    def obtainData(self) -> dict:
        """Returns a single new datapoint from the device.
        Identified by type `record`. `timestamp` records when the record was sent and `dev_id` is the device id.
        `data` is the data collected by the device."""
        if random.random() < 0.4:
            # Sometimes there's no new data
            return {}


        rec = {
            'type': 'record', 'timestamp': datetime.datetime.now().isoformat(), 'dev_id': self._id,
            'data': {kee: str(uuid.uuid4()) for kee in _DATA_KEYS}
        }
        self.sent.append(rec)
        return rec


    def probe(self) -> dict:
        """Returns a probe request to be sent to the SyncService.
        Identified by type `probe`. `from` is the index number from which the device is asking for the data."""
        if random.random() < 0.5:
            # Sometimes the device forgets to probe the SyncService
            return {}


        return {'type': 'probe', 'dev_id': self._id, 'from': len(self.records)}


    def onMessage(self, data: dict):
        """Receives updates from the server"""
        if random.random() < 0.6:
            # Sometimes devices make mistakes. Let's hope the SyncService handles such failures.
            return
       
        if data['type'] == 'update':
            _from = data['from']
            if _from > len(self.records):
                return
            self.records = self.records[:_from] + data['data']
48/15:
class SyncService:
    def __init__(self):
        self.records = []
       
    def onMessage(self, data: dict):
        if not data:
            return
        if data.get('type') == 'record':
            self.records.append(data)
            return
        if data.get('type') == 'probe':
            rec = {'type': 'update', 'data': [datapoint for datapoint in self.records if datapoint.get('dev_id') == data.get('dev_id')][data.get('from'):], 'from': data.get('from')}
            return rec
            
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
48/16:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
48/17: testSyncing()
48/18:
_DATA_KEYS = ["a","b","c"]
class Device:
    def __init__(self, id):
        self._id = id
        self.records = []
        self.sent = []


    def obtainData(self) -> dict:
        """Returns a single new datapoint from the device.
        Identified by type `record`. `timestamp` records when the record was sent and `dev_id` is the device id.
        `data` is the data collected by the device."""
        if random.random() < 0.4:
            # Sometimes there's no new data
            return {}


        rec = {
            'type': 'record', 'timestamp': datetime.datetime.now().isoformat(), 'dev_id': self._id,
            'data': {kee: str(uuid.uuid4()) for kee in _DATA_KEYS}
        }
        self.sent.append(rec)
        return rec


    def probe(self) -> dict:
        """Returns a probe request to be sent to the SyncService.
        Identified by type `probe`. `from` is the index number from which the device is asking for the data."""
        if random.random() < 0.5:
            # Sometimes the device forgets to probe the SyncService
            return {}


        return {'type': 'probe', 'dev_id': self._id, 'from': len(self.records)}


    def onMessage(self, data: dict):
        """Receives updates from the server"""
        if random.random() < 0.6:
            # Sometimes devices make mistakes. Let's hope the SyncService handles such failures.
            return
       
        if data.get('type') == 'update':
            _from = data['from']
            if _from > len(self.records):
                return
            self.records = self.records[:_from] + data['data']
48/19:
class SyncService:
    def __init__(self):
        self.records = []
       
    def onMessage(self, data: dict):
        if not data:
            return {'type':'update', 'data': [], 'from'}
        if data.get('type') == 'record':
            self.records.append(data)
            return
        if data.get('type') == 'probe':
            rec = {'type': 'update', 'data': [datapoint for datapoint in self.records if datapoint.get('dev_id') == data.get('dev_id')][data.get('from'):], 'from': data.get('from')}
            return rec
            
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
48/20:
class SyncService:
    def __init__(self):
        self.records = []
       
    def onMessage(self, data: dict):
        if not data:
            return {'type':'update', 'data': [], 'from': 1e7}
        if data.get('type') == 'record':
            self.records.append(data)
            return
        if data.get('type') == 'probe':
            rec = {'type': 'update', 'data': [datapoint for datapoint in self.records if datapoint.get('dev_id') == data.get('dev_id')][data.get('from'):], 'from': data.get('from')}
            return rec
            
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
48/21:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
48/22: testSyncing()
48/23:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(100)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
48/24:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/25: testSyncing()
48/26:
class SyncService:
    def __init__(self):
        self.records = []
       
    def onMessage(self, data: dict):
        if not data:
            return {'type':'update', 'data': [], 'from': 1e7}
        if data.get('type') == 'record':
            self.records.append(data)
            return
        if data.get('type') == 'probe':
            rec = {'type': 'update', 'data': self.records[data.get('from'):], 'from': data.get('from')}
            return rec
            
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
48/27:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(100)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
48/28:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/29: testSyncing()
48/30:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(100)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
    print("Successful")
48/31:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/32: testSyncing()
48/33:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
    print("Successful")
48/34:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/35: testSyncing()
48/36:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        print(i)
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
    print("Successful")
48/37:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        print(i)
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
    print("Successful")
48/38:
class SyncService:
    def __init__(self):
        self.records = []
        self.max_from = 1e7
       
    def onMessage(self, data: dict):
        if not data:
            return {'type':'update', 'data': [], 'from': self.max_from}
        if data.get('type') == 'record':
            self.records.append(data)
            self.max_from = max(self.max_from, len(self.records) + 1000)
            return
        if data.get('type') == 'probe':
            rec = {'type': 'update', 'data': self.records[data.get('from'):], 'from': data.get('from')}
            return rec
            
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
48/39:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        print(i)
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
    print("Successful")
48/40:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
48/41: testSyncing()
49/1: # Coding a solution to Tic Tac Toe
49/2:
import numpy as np
from numpy.random import choice
from math import ceil
49/3: sequences = [[0,1,2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]
49/4:
def is_winning(given_string):
    global sequences
    for sequence in sequences:
        if given_string[sequence[0]] == given_string[sequence[1]] == given_string[sequence[2]]:
            if given_string[sequence[0]] == 'X':
                return 1
            elif given_string[sequence[0]] == 'O':
                return 2
    return 0
49/5:
def is_draw(given_string):
    global sequences
    for sequence in sequences:
        if not (any([given_string[index] == 'X' for index in sequence]) and any([given_string[index] == 'O' for index in sequence])):
            return False
    return True
49/6:
def evaluate_pos(given_string):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    other_pos = [index for index in range(9) if given_string[index] == '.']
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), [],  []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos, [index for index in other_pos if not any([index in lis for lis in [p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos]])]
49/7: evaluate_pos('.........')
49/8: states = []
49/9: starting_string = '.........'
49/10:
def fill_states(n):
    global states
    if n == 1:
        return ["X", "O", "."] 
    res = fill_states(n-1)
    return ["X" + short_string for short_string in res] + ["O" + short_string for short_string in res] + ["." + short_string for short_string in res]
49/11: states = fill_states(9)
49/12:
import numpy as np
from numpy.random import choice
from math import ceil
import json
49/13:
with open('final_policy.json') as file:
    new_policy = json.load(file)
49/14: new_policy
49/15:
def testing_game(two_player = False):
    global new_policy
    player_num = 1
    player = 'X'
    new_state = '.........'
    if player == 'X':
        action = new_policy[new_state]
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        if two_player:
            if player == 'X':
                action = new_policy[new_state]
            else:
                print(new_state)
                action = int(input()) - 1
        else:
            action = new_policy[new_state]
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    if not any([l for l in tup]):
        return True
    else:
        return False
50/1:
import random
import datetime
import uuid




# OBJECTIVES TODO:
# 1) Read the code and understand it.
# 2) Read the code again and understand it better.
# 3) Feel free to do 1 and 2 however many times you feel like.
# 4) Complete the SyncService implementation. Note that the SyncService.onMessage and SyncService.__init__ function signature must not be altered.
50/2:
_DATA_KEYS = ["a","b","c"]
class Device:
    def __init__(self, id):
        self._id = id
        self.records = []
        self.sent = []


    def obtainData(self) -> dict:
        """Returns a single new datapoint from the device.
        Identified by type `record`. `timestamp` records when the record was sent and `dev_id` is the device id.
        `data` is the data collected by the device."""
        if random.random() < 0.4:
            # Sometimes there's no new data
            return {}


        rec = {
            'type': 'record', 'timestamp': datetime.datetime.now().isoformat(), 'dev_id': self._id,
            'data': {kee: str(uuid.uuid4()) for kee in _DATA_KEYS}
        }
        self.sent.append(rec)
        return rec


    def probe(self) -> dict:
        """Returns a probe request to be sent to the SyncService.
        Identified by type `probe`. `from` is the index number from which the device is asking for the data."""
        if random.random() < 0.5:
            # Sometimes the device forgets to probe the SyncService
            return {}


        return {'type': 'probe', 'dev_id': self._id, 'from': len(self.records)}


    def onMessage(self, data: dict):
        """Receives updates from the server"""
        if random.random() < 0.6:
            # Sometimes devices make mistakes. Let's hope the SyncService handles such failures.
            return
       
        if data.get('type') == 'update':
            _from = data['from']
            if _from > len(self.records):
                return
            self.records = self.records[:_from] + data['data']
50/3:
class SyncService:
    def __init__(self):
        self.records = []
        self.max_from = 1e7
       
    def onMessage(self, data: dict):
        if not data:
            return {'type':'update', 'data': [], 'from': self.max_from}
        if data.get('type') == 'record':
            self.records.append(data)
            self.max_from = max(self.max_from, len(self.records) + 1000)
            return
        if data.get('type') == 'probe':
            rec = {'type': 'update', 'data': self.records[data.get('from'):], 'from': data.get('from')}
            return rec
            
        """Handle messages received from devices.
        Return the desired information in the correct format (type `update`, see Device.onMessage and testSyncing to understand format intricacies) in response to a `probe`.
        No return value required on handling a `record`."""
        raise NotImplementedError()
50/4:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1000)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
    print("Successful")
50/5:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
50/6: testSyncing()
50/7:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1000)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
50/8:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(10000)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
50/9:
def assertEquivalent(d1:dict, d2:dict):
    assert d1['dev_id'] == d2['dev_id']
    assert d1['timestamp'] == d2['timestamp']
    for kee in _DATA_KEYS:
        assert d1['data'][kee] == d2['data'][kee]
50/10: testSyncing()
50/11:
def testSyncing():
    devices = [Device(f"dev_{i}") for i in range(10)]
    syn = SyncService()
   
    _N = int(1e6)
    for i in range(_N):
        for _dev in devices:
            syn.onMessage(_dev.obtainData())
            _dev.onMessage(syn.onMessage(_dev.probe()))


    done = False
    while not done:
        for _dev in devices:
            _dev.onMessage(syn.onMessage(_dev.probe()))
        num_recs = len(devices[0].records)
        done = all([len(_dev.records) == num_recs for _dev in devices])


    ver_start = [0] * len(devices)
    for i,rec in enumerate(devices[0].records):
        _dev_idx = int(rec['dev_id'].split("_")[-1])
        assertEquivalent(rec, devices[_dev_idx].sent[ver_start[_dev_idx]])
        for _dev in devices[1:]:
            assertEquivalent(rec, _dev.records[i])
        ver_start[_dev_idx] += 1
51/1:
import numpy as np
from numpy.random import choice
from math import ceil
import json
51/2:
with open('final_policy.json') as file:
    new_policy = json.load(file)
51/3:
def testing_game(two_player = False):
    global new_policy
    player_num = 1
    player = 'X'
    new_state = '.........'
    if player == 'X':
        action = new_policy[new_state]
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        if two_player:
            if player == 'X':
                action = new_policy[new_state]
            else:
                print(new_state)
                action = int(input()) - 1
        else:
            action = new_policy[new_state]
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    if not any([l for l in tup]):
        return True
    else:
        return False
51/4: testing_game(True)
51/5:
def is_winning(given_string):
    global sequences
    for sequence in sequences:
        if given_string[sequence[0]] == given_string[sequence[1]] == given_string[sequence[2]]:
            if given_string[sequence[0]] == 'X':
                return 1
            elif given_string[sequence[0]] == 'O':
                return 2
    return 0
51/6:
def is_draw(given_string):
    global sequences
    for sequence in sequences:
        if not (any([given_string[index] == 'X' for index in sequence]) and any([given_string[index] == 'O' for index in sequence])):
            return False
    return True
51/7:
def evaluate_pos(given_string):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    other_pos = [index for index in range(9) if given_string[index] == '.']
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), [],  []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos, [index for index in other_pos if not any([index in lis for lis in [p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos]])]
51/8:
def testing_game(two_player = False):
    global new_policy
    player_num = 1
    player = 'X'
    new_state = '.........'
    if player == 'X':
        action = new_policy[new_state]
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        if two_player:
            if player == 'X':
                action = new_policy[new_state]
            else:
                print(new_state)
                action = int(input()) - 1
        else:
            action = new_policy[new_state]
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    if not any([l for l in tup]):
        return True
    else:
        return False
51/9: testing_game(True)
51/10: sequences = [[0,1,2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]
51/11:
def is_winning(given_string):
    global sequences
    for sequence in sequences:
        if given_string[sequence[0]] == given_string[sequence[1]] == given_string[sequence[2]]:
            if given_string[sequence[0]] == 'X':
                return 1
            elif given_string[sequence[0]] == 'O':
                return 2
    return 0
51/12:
def is_draw(given_string):
    global sequences
    for sequence in sequences:
        if not (any([given_string[index] == 'X' for index in sequence]) and any([given_string[index] == 'O' for index in sequence])):
            return False
    return True
51/13:
def evaluate_pos(given_string):
    p1_winning_pos = []
    p1_attack_pos = []
    p2_winning_pos = []
    p2_attack_pos = []
    other_pos = [index for index in range(9) if given_string[index] == '.']
    not_draw = False
    for sequence in sequences:
        chars = [given_string[index] for index in sequence]
        if chars.count('X') == 0 or chars.count('O') == 0:
            not_draw = True
            if chars.count('X') == 3:
                return list(range(1, 10)), [], [], [], []
            if chars.count('O') == 3:
                return [], [], list(range(1, 10)), [],  []
            if chars.count('O') == 0:
                if chars.count('X') == 2:
                    p1_winning_pos += [index for index in sequence if given_string[index] != 'X']
                elif chars.count('X') == 1:
                    p1_attack_pos += [index for index in sequence if given_string[index] != 'X']
            if chars.count('X') == 0:
                if chars.count('O') == 2:
                    p2_winning_pos += [index for index in sequence if given_string[index] != 'O']
                elif chars.count('O') == 1:
                    p2_attack_pos += [index for index in sequence if given_string[index] != 'O']
    if not not_draw:
        return [], [], [], [], []
    else:
        return p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos, [index for index in other_pos if not any([index in lis for lis in [p1_winning_pos, p1_attack_pos, p2_winning_pos, p2_attack_pos]])]
51/14: testing_game(True)
56/1:
from time import sleep

def argmax(given_list):
    index = 0
    maximum = given_list[index]
    max_index = index
    while index < 64:
        if given_list[index] > maximum:
            maximum = given_list[index]
            max_index = index
        index += 1
    return max_index

def clickable_buttons(white_buttons_list, black_buttons_list, black_to_play):
    return [i for i in range(64) if check_if_can_place(i, black_to_play, white_buttons_list, black_buttons_list) != -1]
    
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*column_number,8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        assert(list_of_buttons[0] in player1_list)
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
            return num_converted
    return -1


def check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play):
    white_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, False)
    black_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, True)
    if not white_buttons_list or not black_buttons_list or (not white_clickable_buttons and not black_clickable_buttons):
        if len(white_buttons_list) > len(black_buttons_list):
            print(f"Game Over. The winner is White")
        elif len(white_buttons_list) < len(black_buttons_list):
            print(f"Game Over. The winner is Black")
        elif len(white_buttons_list) == len(black_buttons_list):
            print("Game over. The game resulted in a draw.")
        return None
    if not white_clickable_buttons:
        black_to_play = True
        return black_to_play
    if not black_clickable_buttons:
        black_to_play = False
        return black_to_play
    black_to_play = not black_to_play
    return black_to_play


def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    choice = argmax(clickable_buttons(white_buttons_list, black_buttons_list, False))
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)

white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/2: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/3:
from time import sleep

def argmax(given_list):
    index = 0
    maximum = given_list[index]
    max_index = index
    while index < 64:
        if given_list[index] > maximum:
            maximum = given_list[index]
            max_index = index
        index += 1
    return max_index

def clickable_buttons(white_buttons_list, black_buttons_list, black_to_play):
    return [i for i in range(64) if check_if_can_place(i, black_to_play, white_buttons_list, black_buttons_list) != -1]
    
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*column_number,8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
            return num_converted
    return -1


def check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play):
    white_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, False)
    black_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, True)
    if not white_buttons_list or not black_buttons_list or (not white_clickable_buttons and not black_clickable_buttons):
        if len(white_buttons_list) > len(black_buttons_list):
            print(f"Game Over. The winner is White")
        elif len(white_buttons_list) < len(black_buttons_list):
            print(f"Game Over. The winner is Black")
        elif len(white_buttons_list) == len(black_buttons_list):
            print("Game over. The game resulted in a draw.")
        return None
    if not white_clickable_buttons:
        black_to_play = True
        return black_to_play
    if not black_clickable_buttons:
        black_to_play = False
        return black_to_play
    black_to_play = not black_to_play
    return black_to_play


def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    choice = argmax(clickable_buttons(white_buttons_list, black_buttons_list, False))
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)

white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/4: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/5: np.linspace(0, 64, 64)
56/6: import numpy as np
56/7: np.linspace(0, 64, 64)
56/8: np.linspace(0, 63, 64)
56/9: np.linspace(0, 63, 64).reshape(8, 8)
56/10:
from time import sleep

def argmax(given_list):
    index = 0
    maximum = given_list[index]
    max_index = index
    while index < 64:
        if given_list[index] > maximum:
            maximum = given_list[index]
            max_index = index
        index += 1
    return max_index

def clickable_buttons(white_buttons_list, black_buttons_list, black_to_play):
    return [i for i in range(64) if check_if_can_place(i, black_to_play, white_buttons_list, black_buttons_list) != -1]
    
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*row_number, 8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
            return num_converted
    return -1


def check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play):
    white_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, False)
    black_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, True)
    if not white_buttons_list or not black_buttons_list or (not white_clickable_buttons and not black_clickable_buttons):
        if len(white_buttons_list) > len(black_buttons_list):
            print(f"Game Over. The winner is White")
        elif len(white_buttons_list) < len(black_buttons_list):
            print(f"Game Over. The winner is Black")
        elif len(white_buttons_list) == len(black_buttons_list):
            print("Game over. The game resulted in a draw.")
        return None
    if not white_clickable_buttons:
        black_to_play = True
        return black_to_play
    if not black_clickable_buttons:
        black_to_play = False
        return black_to_play
    black_to_play = not black_to_play
    return black_to_play


def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    choice = argmax(clickable_buttons(white_buttons_list, black_buttons_list, False))
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)

white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/11: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/12:
from time import sleep

def argmax(given_list):
    index = 0
    maximum = given_list[index]
    max_index = index
    while index < 64:
        if given_list[index] > maximum:
            maximum = given_list[index]
            max_index = index
        index += 1
    return max_index

def clickable_buttons(white_buttons_list, black_buttons_list, black_to_play):
    return [i for i in range(64) if check_if_can_place(i, black_to_play, white_buttons_list, black_buttons_list) != -1]
56/13:
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*row_number, 8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
            return num_converted
    return -1
56/14:
def check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play):
    white_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, False)
    black_clickable_buttons = clickable_buttons(white_buttons_list, black_buttons_list, True)
    if not white_buttons_list or not black_buttons_list or (not white_clickable_buttons and not black_clickable_buttons):
        if len(white_buttons_list) > len(black_buttons_list):
            print(f"Game Over. The winner is White")
        elif len(white_buttons_list) < len(black_buttons_list):
            print(f"Game Over. The winner is Black")
        elif len(white_buttons_list) == len(black_buttons_list):
            print("Game over. The game resulted in a draw.")
        return None
    if not white_clickable_buttons:
        black_to_play = True
        return black_to_play
    if not black_clickable_buttons:
        black_to_play = False
        return black_to_play
    black_to_play = not black_to_play
    return black_to_play
56/15:
def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    choice = argmax(clickable_buttons(white_buttons_list, black_buttons_list, False))
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)
56/16:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/17: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/18:
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*row_number+column_number, 8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
            return num_converted
    return -1
56/19:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/20: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/21: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/22:
from time import sleep

def argmax(given_list):
    index = 0
    maximum = given_list[index]
    max_index = index
    length = len(given_list)
    while index < length:
        if given_list[index] > maximum:
            maximum = given_list[index]
            max_index = index
        index += 1
    return max_index

def clickable_buttons(white_buttons_list, black_buttons_list, black_to_play):
    return [i for i in range(64) if check_if_can_place(i, black_to_play, white_buttons_list, black_buttons_list) != -1]
56/23: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/24: white_buttons_list
56/25: black_buttons_list
56/26:
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*row_number+column_number, 8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                if black_to_play:
                    black_buttons_list.append(number)
                else:
                    white_buttons_list.append(number)
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
            return num_converted
    return -1
56/27: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/28: black_buttons_list
56/29:
def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    choice = argmax(clickable_buttons(white_buttons_list, black_buttons_list, False))
    print(choice)
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)
56/30:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/31: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/32: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/33:
from time import sleep

def argmax(given_list):
    index = 0
    maximum = given_list[index]
    max_index = index
    length = len(given_list)
    while index < length:
        if given_list[index] > maximum:
            maximum = given_list[index]
            max_index = index
        index += 1
    return max_index

def clickable_buttons(white_buttons_list, black_buttons_list, black_to_play):
    return [i for i in range(64) if check_if_can_place(i, black_to_play, white_buttons_list, black_buttons_list) != -1]
56/34:
def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    choice = argmax([check_if_can_place(i, False, white_buttons_list, black_buttons_list, only_check=True) for i in range(64)])
    print(choice)
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)
56/35:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/36: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/37: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/38:
def button_clicked(button, white_buttons_list, black_buttons_list, black_to_play):
    check_if_can_place(button, black_to_play, white_buttons_list, black_buttons_list, only_check=False)
    black_to_play = check_if_switch_sides(white_buttons_list, black_buttons_list, black_to_play)
    if black_to_play is None:
        return
    if not black_to_play:
        sleep(0.75)
        computer_play(white_buttons_list, black_buttons_list)


def computer_play(white_buttons_list, black_buttons_list):
    print([check_if_can_place(i, False, white_buttons_list, black_buttons_list, only_check=True) for i in range(64)])
    choice = argmax([check_if_can_place(i, False, white_buttons_list, black_buttons_list, only_check=True) for i in range(64)])
    print(choice)
    check_if_can_place(choice, False, white_buttons_list, black_buttons_list, only_check=False)
    check_if_switch_sides(white_buttons_list, black_buttons_list, False)
56/39:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/40: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/41: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/42:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    print(value)
56/43: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/44:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print(value)
56/45:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print(line)
56/46:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print("".join(line))
56/47: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/48: white_buttons_list
56/49:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/50: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/51:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                print(8*i+j)
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print("".join(line))
56/52: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/53:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print("".join(line))
56/54:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    print(value)
    for line in value:
        print("".join(line))
56/55: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/56:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8]*8
    print(value)
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print("".join(line))
56/57: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/58:
def display(white_buttons_list, black_buttons_list):
    value = ["........"]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print("".join(line))
56/59:
def display(white_buttons_list, black_buttons_list):
    value = ["........"]*8
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print(line)
56/60: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/61:
def display(white_buttons_list, black_buttons_list):
    value = [['.']*8 for i in range(8)]
    for i in range(8):
        for j in range(8):
            if 8*i + j in white_buttons_list:
                value[i][j] = 'W'
            if 8*i + j in black_buttons_list:
                value[i][j] = 'B'
    for line in value:
        print(line)
56/62: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/63: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/64: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/65: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/66:
list_1 = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]
print(list_1.count(1))
56/67: button_clicked(8*3+2, white_buttons_list, black_buttons_list, True)
56/68: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/69: button_clicked(8*1+2, white_buttons_list, black_buttons_list, True)
56/70: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/71:
def check_if_can_place(number, black_to_play, white_buttons_list, black_buttons_list, only_check=True):
    if number in white_buttons_list or number in black_buttons_list:
        return -1
    row_number = number // 8
    column_number = number % 8
    if not black_to_play:
        player1_list = white_buttons_list
        player2_list = black_buttons_list
    else:
        player1_list = black_buttons_list
        player2_list = white_buttons_list
    num_converted = 0
    for list_of_buttons in [range(8*row_number + column_number, 8*row_number + 8), range(8*row_number+column_number, 8*row_number-1,-1), range(8*row_number+column_number,-1,-8), range(8*row_number+column_number, 64, 8), range(number, 64, 9), range(number, -1, -9), range(number, 64, 7), range(number, -1, -7)]:
        index = 1
        while index < len(list_of_buttons) and list_of_buttons[index] in player2_list:
            index += 1
        if 1 < index < len(list_of_buttons) and list_of_buttons[index] in player1_list:
            num_converted += index - 1
            if not only_check:
                if black_to_play:
                    black_buttons_list.append(number)
                else:
                    white_buttons_list.append(number)
                for button in list_of_buttons[1: index]:
                    if black_to_play:
                        white_buttons_list.remove(button)
                        black_buttons_list.append(button)
                    else:
                        black_buttons_list.remove(button)
                        white_buttons_list.append(button)
    if num_converted:
        return num_converted
    return -1
56/72:
white_buttons_list = [8*3+3, 8*4+4]
black_buttons_list = [8*4+3, 8*3+4]
56/73: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/74: button_clicked(19, white_buttons_list, black_buttons_list, True)
56/75: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/76: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/77: button_clicked(26, white_buttons_list, black_buttons_list, True)
56/78: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
56/79: print(clickable_buttons(white_buttons_list, black_buttons_list, True))
56/80: button_clicked(10, white_buttons_list, black_buttons_list, True)
56/81: display(white_buttons_list=white_buttons_list, black_buttons_list=black_buttons_list)
57/1: pip install regex-engine
57/2:
from regex_engine import generator

generate = generator()
    
regex = generate.numerical_range(2, 16)
    
print(regex)
57/3:
from regex_engine import generator

generate = generator()
    
regex = generate.numerical_range(0, 9999)
    
print(regex)
60/1: from matplotlib import pyplot as plt
60/2:
x = [64, 96, 128, 160, 192, 224, 256]
y = [100, 60, 52, 48, 44, 40, 36]
y2 = [125, 144.23, 156.25, 170.45, 187.5, 208.33]
60/3: plt.plot(x, y)
60/4: fig, ax = plt.plot(x, y)
60/5:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color)
60/6:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx
60/7:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx
ax2.plot(x, y2)
60/8:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.plot(x, y2)
60/9:
x = [64, 96, 128, 160, 192, 224, 256]
y = [100, 60, 52, 48, 44, 40, 36]
y2 = [75, 125, 144.23, 156.25, 170.45, 187.5, 208.33]
60/10:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.plot(x, y2)
60/11:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.plot(x, y2, color='red')
60/12:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, color='red')
60/13:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', ,bo', color='red')
ax2.plot(x, y2, color='red')
60/14:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', 'bo', color='red')
ax2.plot(x, y2, color='red')
60/15:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, color='red')
60/16:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, 'red')
60/17:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, 'r+')
60/18:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, 'ro')
60/19:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y, 'bo') 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, 'ro')
60/20:
fig, ax = plt.subplots()
ax.set_xlabel('PWM Value') 
ax.set_ylabel('Time Period in ms') 
ax.plot(x, y, 'bo', alpha=0.3) 
# ax.tick_params(axis ='y', labelcolor = color) 
ax2 = ax.twinx()
ax2.set_ylabel('Frequency in RPM', color='red')
ax2.plot(x, y2, 'ro', alpha=0.3)
61/1:
import re
student_id = input()
print(re.match(r'^(([23][BbMm][0-9]{4})|([0-9]{9}))$', student_id.strip()))
61/2:
import re
student_id = input()
print(re.match(r'^(([23][BbMm][0-9]{4})|([0-9]{9}))$', student_id.strip()))
61/3:
import re
student_id = input()
print(re.match(r'^((23[BbMm][0-9]{4})|([0-9]{9}))$', student_id.strip()))
61/4:
import re
student_id = input()
print(re.match(r'^((23[BbMm][0-9]{4})|([0-9]{9}))$', student_id.strip()))
61/5:
import re
student_id = input()
print(re.match(r'^((23[BbMm][0-9]{4})|([0-9]{9}))$', student_id.strip()))
61/6:
import re
student_id = input()
print(re.match(r'^((23[BbMm][0-9]{4})|([0-9]{9}))$', student_id.strip()))
62/1: import json
62/2: from json import jsonify
62/3: import jsonify
62/4: json.write({'Hi': 'Hello})
62/5: json.write({'Hi': 'Hello'})
62/6: json.dumps({'Hi': 'Hello'})
63/1:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
if len(sys.argv) != 2:
    print("Usage: python main.py n")
    sys.exit()

n = int(sys.argv[1])
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
while True:
    curr_number = primes[current_index]
    if curr_number*curr_number > n:
        primes += potential_candidates
        break
    # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
    potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
    if potential_candidates:
        primes.append(potential_candidates[0])
        potential_candidates.pop(0)
        current_index += 1
    else:
        break
print(primes)
time_stop = process_time_ns()
# print("Elapsed time during the whole program in seconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/2:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 10**6
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
while True:
    curr_number = primes[current_index]
    if curr_number*curr_number > n:
        primes += potential_candidates
        break
    # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
    potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
    if potential_candidates:
        primes.append(potential_candidates[0])
        potential_candidates.pop(0)
        current_index += 1
    else:
        break
print(primes)
time_stop = process_time_ns()
# print("Elapsed time during the whole program in seconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/3:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 10**6
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
while True:
    curr_number = primes[current_index]
    if curr_number*curr_number > n:
        primes += potential_candidates
        break
    # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
    potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
    if potential_candidates:
        primes.append(potential_candidates[0])
        potential_candidates.pop(0)
        current_index += 1
    else:
        break
print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in seconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/4:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 10**6
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
while True:
    curr_number = primes[current_index]
    if curr_number*curr_number > n:
        primes += potential_candidates
        break
    # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
    potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
    if potential_candidates:
        primes.append(potential_candidates[0])
        potential_candidates.pop(0)
        current_index += 1
    else:
        break
print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in nanoseconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/5:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 10**6
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
all_nums = [0]*(n+1)
primes = []
for i in range(2, n+1):
    if all_nums[i] == 0:
        all_nums[i] = i
        primes.append(i)
    j = 0
    while i*primes[j] <= n:
        all_nums[i*primes[j]] = primes[j]
        if primes[j] == all_nums[i]:
            break

# while True:
#     curr_number = primes[current_index]
#     if curr_number*curr_number > n:
#         primes += potential_candidates
#         break
#     # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
#     potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
#     if potential_candidates:
#         primes.append(potential_candidates[0])
#         potential_candidates.pop(0)
#         current_index += 1
#     else:
#         break
# print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in nanoseconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/6:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 10
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
all_nums = [0]*(n+1)
primes = []
for i in range(2, n+1):
    if all_nums[i] == 0:
        all_nums[i] = i
        primes.append(i)
    j = 0
    while i*primes[j] <= n:
        all_nums[i*primes[j]] = primes[j]
        if primes[j] == all_nums[i]:
            break

# while True:
#     curr_number = primes[current_index]
#     if curr_number*curr_number > n:
#         primes += potential_candidates
#         break
#     # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
#     potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
#     if potential_candidates:
#         primes.append(potential_candidates[0])
#         potential_candidates.pop(0)
#         current_index += 1
#     else:
#         break
# print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in nanoseconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/7:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 10
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
all_nums = [0]*(n+1)
primes = []
for i in range(2, n+1):
    if all_nums[i] == 0:
        all_nums[i] = i
        primes.append(i)
    j = 0
    while i*primes[j] <= n:
        all_nums[i*primes[j]] = primes[j]
        if primes[j] == all_nums[i]:
            break
        j += 1

# while True:
#     curr_number = primes[current_index]
#     if curr_number*curr_number > n:
#         primes += potential_candidates
#         break
#     # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
#     potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
#     if potential_candidates:
#         primes.append(potential_candidates[0])
#         potential_candidates.pop(0)
#         current_index += 1
#     else:
#         break
print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in nanoseconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/8:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 1000
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
all_nums = [0]*(n+1)
primes = []
for i in range(2, n+1):
    if all_nums[i] == 0:
        all_nums[i] = i
        primes.append(i)
    j = 0
    while i*primes[j] <= n:
        all_nums[i*primes[j]] = primes[j]
        if primes[j] == all_nums[i]:
            break
        j += 1

# while True:
#     curr_number = primes[current_index]
#     if curr_number*curr_number > n:
#         primes += potential_candidates
#         break
#     # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
#     potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
#     if potential_candidates:
#         primes.append(potential_candidates[0])
#         potential_candidates.pop(0)
#         current_index += 1
#     else:
#         break
print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in nanoseconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
63/9:
'''
    Sieve of Eratosthenes
    Author: Saksham Rathi
'''

import sys
from time import process_time_ns

# read the parameter from command line using sys
n = 1000000
if n == 1:
    print([])
    sys.exit()
if n < 3:
    print([2])
    sys.exit()
potential_candidates = list(range(3, n+1, 2))
potential_candidates.pop(0)
primes = [2, 3]
current_index = 1

time_start = process_time_ns()
all_nums = [0]*(n+1)
primes = []
for i in range(2, n+1):
    if all_nums[i] == 0:
        all_nums[i] = i
        primes.append(i)
    j = 0
    while i*primes[j] <= n:
        all_nums[i*primes[j]] = primes[j]
        if primes[j] == all_nums[i]:
            break
        j += 1

# while True:
#     curr_number = primes[current_index]
#     if curr_number*curr_number > n:
#         primes += potential_candidates
#         break
#     # potential_candidates = list(filter(lambda x: x%curr_number != 0, potential_candidates))
#     potential_candidates = [num for num in potential_candidates if num%curr_number != 0]
#     if potential_candidates:
#         primes.append(potential_candidates[0])
#         potential_candidates.pop(0)
#         current_index += 1
#     else:
#         break
print(len(primes))
time_stop = process_time_ns()
print("Elapsed time during the whole program in nanoseconds:", time_stop-time_start)  

# your implementation for printing the list of prime numbers
64/1: import random
64/2: random.randint(2/2, 4)
64/3: random.randint(1/2, 4)
64/4: random.randint(2/2, 4)
64/5: random.randint(3/2, 4)
64/6: random.randint(1/2, 4)
64/7: random.randint(2/2, 4)
64/8: random.randint(1/2, 4)
64/9:
y = 40
x = random.randint(y / 2 + 4, y - 4)
65/1: import matplotlib
65/2: import matplotlib.pyplot as plt
65/3: num_iters = 0
65/4:
import matplotlib.pyplot as plt
from random import randint
65/5: num_iters = 0
65/6: excluded = random.randint(0,3)
65/7: excluded = randint(0,3)
65/8: probabilities = np.zeros((40,40))
65/9:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
65/10: probabilities = np.zeros((40,40))
65/11:
for i in range(4):
    if i == excluded:
        continue
    if i == 0:
        x = randint(4, 16)
        y = randint(4, 16)
    elif i == 1:
        x = randint(24, 36)
        y = randint(4, 16)
    elif i == 2:
        x = randint(4, 16)
        y = randint(24, 36)
    else:
        x = randint(24, 36)
        y = randint(4, 16)
    probabilities[x-1][y-1] += 1
    probabilities[x-2][y-1] += 1
    probabilities[x][y-1] += 1
    probabilities[x-1][y-2] += 1
    probabilities[x-1][y] += 1
    probabilities[x-2][y] += 1
    probabilities[x-2][y-2] += 1
    probabilities[x][y] += 1
    probabilities[x][y-2] += 1
65/12:
for i in range(4):
    if i == excluded:
        continue
    if i == 0:
        x = randint(4, 16)
        y = randint(4, 16)
    elif i == 1:
        x = randint(24, 36)
        y = randint(4, 16)
    elif i == 2:
        x = randint(4, 16)
        y = randint(24, 36)
    else:
        x = randint(24, 36)
        y = randint(4, 16)
    probabilities[x-1][y-1] += 1
    probabilities[x-2][y-1] += 1
    probabilities[x][y-1] += 1
    probabilities[x-1][y-2] += 1
    probabilities[x-1][y] += 1
    probabilities[x-2][y] += 1
    probabilities[x-2][y-2] += 1
    probabilities[x][y] += 1
    probabilities[x][y-2] += 1
    num_iters + 1
65/13:
for _ in range(100):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(4, 16)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters + 1
65/14: print(probabilities)
65/15:
excluded = randint(0,3)
excluded
65/16:
for _ in range(100):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(4, 16)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters + 1
65/17: print(probabilities)
65/18:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/19:
for _ in range(10000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(4, 16)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters + 1
65/20:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/21:
for _ in range(10000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters + 1
65/22: print(probabilities)
65/23:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/24: probabilities /= num_iters
65/25:
for _ in range(10000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/26: probabilities /= num_iters
65/27: print(probabilities)
65/28: num_iters = 0
65/29: probabilities = np.zeros((40,40))
65/30:
excluded = randint(0,3)
excluded
65/31:
for _ in range(10000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/32: probabilities /= num_iters
65/33: print(probabilities)
65/34:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/35:
count_not_present = 0
for _ in range(10000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(12-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(32-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(12-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(32-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/36: print(count_not_present/num_iters)
65/37:
count_not_present = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(12-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(32-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(12-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(32-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/38: print(count_not_present/num_iters)
65/39:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(12-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(32-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(12-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(32-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/40: print(count_not_present/num_iters)
65/41: probabilities = np.zeros((40,40))
65/42: num_iters = 0
65/43: print(num_iters)
65/44:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(12-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(32-x) > 1 or abs(12 - y) > 1:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(12-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(32-x) > 1 or abs(32 - y) > 1:
                count_not_present += 1
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/45: print(count_not_present/num_iters)
65/46: print(num_iters)
65/47:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 1 or abs(10 - y) > 1:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 1 or abs(10 - y) > 1:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 1 or abs(30 - y) > 1:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 1 or abs(30 - y) > 1:
                count_not_present += 1
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/48: print(num_iters)
65/49: print(count_not_present/num_iters)
65/50: print(count_not_present/num_iters)
65/51:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/52: probabilities /= num_iters
65/53:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/54: print(count_not_present/num_iters)
65/55:
count_not_present = 0
num_iters = 0
for _ in range(100):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 1 or abs(10 - y) > 1:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 1 or abs(10 - y) > 1:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 1 or abs(30 - y) > 1:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 1 or abs(30 - y) > 1:
                count_not_present += 1
        print(x,y)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/56:
count_not_present = 0
num_iters = 0
for _ in range(100):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        print(x,y)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/57: print(num_iters)
65/58: print(count_not_present/num_iters)
65/59:
count_not_present = 0
num_iters = 0
for _ in range(10000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        # print(x,y)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/60: print(num_iters)
65/61: print(count_not_present/num_iters)
65/62:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        # print(x,y)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/63: probabilities /= num_iters
65/64: print(num_iters)
65/65: print(count_not_present/num_iters)
65/66:
for i in probabilities:
    print(i)
65/67: print(probabilities.amax())
65/68: print(amax(probabilities))
65/69: print(np.amax(probabilities))
65/70: print(probabilities.argmax())
65/71: print(probabilities)
65/72: print(probabilities.argmax())
65/73: print(probabilities[33][24]
65/74: print(probabilities[33][24])
65/75: print(probabilities[33][27])
65/76: print(probabilities[32][27])
65/77: print(probabilities[33][7])
65/78: print(probabilities[32:35][6:9])
65/79: print(probabilities[32:35,6:9])
65/80:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        # print(x,y)
        probabilities[x-1][y-1] += 1
        probabilities[x-2][y-1] += 1
        probabilities[x][y-1] += 1
        probabilities[x-1][y-2] += 1
        probabilities[x-1][y] += 1
        probabilities[x-2][y] += 1
        probabilities[x-2][y-2] += 1
        probabilities[x][y] += 1
        probabilities[x][y-2] += 1
        num_iters += 1
65/81: probabilities /= num_iters
65/82: print(num_iters)
65/83: print(count_not_present/num_iters)
65/84: print(probabilities)
65/85: print(np.amax(probabilities))
65/86: print(probabilities[32:35,6:9])
65/87: print(probabilities.argmax())
65/88:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        # print(x,y)
        probabilities[x-3:x+2,y-3:y+2] += np.ones((3,3))
        # probabilities[x-1][y-1] += 1
        # probabilities[x-2][y-1] += 1
        # probabilities[x][y-1] += 1
        # probabilities[x-1][y-2] += 1
        # probabilities[x-1][y] += 1
        # probabilities[x-2][y] += 1
        # probabilities[x-2][y-2] += 1
        # probabilities[x][y] += 1
        # probabilities[x][y-2] += 1
        num_iters += 1
65/89:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        # print(x,y)
        probabilities[x-3:x+2,y-3:y+2] += np.ones((5,5))
        # probabilities[x-1][y-1] += 1
        # probabilities[x-2][y-1] += 1
        # probabilities[x][y-1] += 1
        # probabilities[x-1][y-2] += 1
        # probabilities[x-1][y] += 1
        # probabilities[x-2][y] += 1
        # probabilities[x-2][y-2] += 1
        # probabilities[x][y] += 1
        # probabilities[x][y-2] += 1
        num_iters += 1
65/90: probabilities /= num_iters
65/91: print(num_iters)
65/92: print(count_not_present/num_iters)
65/93: print(probabilities)
65/94: print(np.amax(probabilities))
65/95: print(probabilities.argmax())
65/96:
fig, ax = plt.subplots()
contour = ax.contourf(probabilities, cmap='RdGy')

cbar = fig.colorbar(contour)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Contour Map')

# Show the plot
plt.show()
65/97: print(probabilities.argmax())
65/98: rewards = np.ones(40,40) * 5
65/99: rewards = np.ones((40,40)) * 5
65/100: print(rewards)
65/101:
def set_rewards():
    global rewards
    rewards = np.ones((40,40))*5
    excluded = randint(0,3)
        for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
65/102:
def set_rewards():
    global rewards
    rewards = np.ones((40,40))*5
    excluded = randint(0,3)
        for i in range(4):
            if i == excluded:
                continue
            if i == 0:
                x = randint(4, 16)
                y = randint(4, 16)
            elif i == 1:
                x = randint(24, 36)
                y = randint(4, 16)
            elif i == 2:
                x = randint(4, 16)
                y = randint(24, 36)
            else:
                x = randint(24, 36)
                y = randint(24, 36)
            # print(x,y)
            rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
65/103:
def set_rewards():
    global rewards
    rewards = np.ones((40,40))*5
    excluded = randint(0,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
65/104:
def game():
    pass
65/105:
mdp = state: [action: probability for action]
mdp = {
    (x, y): 0 for x in range(40)
    for y in range(40)
}
65/106:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): 0 for x in range(40)
    for y in range(40)
}
65/107:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [] for x in range(40)
    for y in range(40)
}
65/108:
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key, value in actions.items():
        actions[key] = 1/len(value)
65/109:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): {} for x in range(40)
    for y in range(40)
}
65/110:
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key, value in actions.items():
        actions[key] = 1/len(value)
65/111:
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key, value in actions.items():
        actions[key] = 1/len(actions)
65/112: print(mdp)
65/113:
class Player():
    def __init__(self):
        self.position.x = 0
        self.position.y = 0
65/114:
class Player():
    def __init__(self):
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/115:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/116:
class Player():
    def __init__(self):
        self.position = ()
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/117:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/118:
class Player():
    def __init__(self):
        self.position = (x, y)
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/119:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/120:
class Player():
    def __init__(self):
        self.position = None
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/121:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/122:
class Player():
    def __init__(self):
        self.position = Tuple()
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/123:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/124:
class Player():
    def __init__(self):
        self.position = ()
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/125:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/126:
class Player():
    def __init__(self):
        self.position = (x, y)
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/127:
class Player():
    def __init__(self):
        self.position = (0, 0)
        self.position.x = 0
        self.position.y = 0
        print(self.position)
65/128:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/129:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
65/130:
gamma = 0.9
epsilon = 0.1
for player in range(8):
    play = Player()
65/131:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp =     {(x, y): {} for x in range(40)    for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions[0] = 0
            if state[0] >= 1:
                actions[1] = 0
            if state[1] + 1 < 40:
                actions[2] = 0
            if state[1] > 0:
                actions[3] = 0
            for key, value in actions.items():
                actions[key] = 1/len(actions)
65/132:
gamma = 0.9
epsilon = 0.1
for _ in range(8):
    player = Player()
65/133:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): {} for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions[0] = 0
            if state[0] >= 1:
                actions[1] = 0
            if state[1] + 1 < 40:
                actions[2] = 0
            if state[1] > 0:
                actions[3] = 0
            for key, value in actions.items():
                actions[key] = 1/len(actions)
65/134:
excluded = choice([0,2])
excluded
65/135:
import matplotlib.pyplot as plt
from random import randint, choice
import numpy as np
65/136:
excluded = choice([0,2])
excluded
65/137:
excluded = randint(2,4)
excluded
65/138:
excluded = randint(2,4)
excluded
65/139:
count_not_present = 0
num_iters = 0
for _ in range(100000):
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
            if abs(10-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
            if abs(30-x) > 2 or abs(10 - y) > 2:
                count_not_present += 1
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
            if abs(10-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        else:
            x = randint(24, 36)
            y = randint(24, 36)
            if abs(30-x) > 2 or abs(30 - y) > 2:
                count_not_present += 1
        # print(x,y)
        probabilities[x-3:x+2,y-3:y+2] += np.ones((5,5))
        # probabilities[x-1][y-1] += 1
        # probabilities[x-2][y-1] += 1
        # probabilities[x][y-1] += 1
        # probabilities[x-1][y-2] += 1
        # probabilities[x-1][y] += 1
        # probabilities[x-2][y] += 1
        # probabilities[x-2][y-2] += 1
        # probabilities[x][y] += 1
        # probabilities[x][y-2] += 1
        num_iters += 1
65/140:
excluded = randint(2, 4)
excluded
65/141:
for i in range(4):
    if i == excluded:
        continue
    if i == 0:
        x = randint(4, 16)
        y = randint(4, 16)
    elif i == 1:
        x = randint(24, 36)
        y = randint(4, 16)
    elif i == 2:
        x = randint(4, 16)
        y = randint(24, 36)
    else:
        x = randint(24, 36)
        y = randint(24, 36)
    rewards[x-2:x+1,y-2:y+1] += np.ones((3,3))
65/142:
for i in range(4):
    if i == excluded:
        continue
    if i == 0:
        x = randint(4, 16)
        y = randint(4, 16)
    elif i == 1:
        x = randint(24, 36)
        y = randint(4, 16)
    elif i == 2:
        x = randint(4, 16)
        y = randint(24, 36)
    else:
        x = randint(24, 36)
        y = randint(24, 36)
    rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
65/143: rewards = np.ones((40,40)) * 5
65/144: print(rewards)
65/145:
for i in range(4):
    if i == excluded:
        continue
    if i == 0:
        x = randint(4, 16)
        y = randint(4, 16)
    elif i == 1:
        x = randint(24, 36)
        y = randint(4, 16)
    elif i == 2:
        x = randint(4, 16)
        y = randint(24, 36)
    else:
        x = randint(24, 36)
        y = randint(24, 36)
    rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
65/146:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/147:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): {} for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions[0] = 0
            if state[0] >= 1:
                actions[1] = 0
            if state[1] + 1 < 40:
                actions[2] = 0
            if state[1] > 0:
                actions[3] = 0
            for key, value in actions.items():
                actions[key] = 1/len(actions)
65/148:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/149:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): {} for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            for key, value in actions.items():
                actions[key] = 1/np.sum(actions)
            self.mdp[state] = actions
65/150:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/151:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [] for x in range(40)
    for y in range(40)
}
65/152:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): {} for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            for key, value in actions.items():
                actions[key] = 1/np.sum(actions)
            self.mdp[state] = actions
65/153:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/154:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [] for x in range(40)
    for y in range(40)
}
65/155:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            for key, value in actions.items():
                actions[key] = 1/np.sum(actions)
            self.mdp[state] = actions
65/156:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/157:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions /= np.sum(array)
            self.mdp[state] = actions
66/1: import sklearn
66/2: from sklearn.linear_model import LinearRegression
66/3: import pandas as pd
66/4: data = pd.read_csv("CorrelationMatrix_returns.csv")
66/5: data.head()
66/6: pd.corr(data)
66/7: np.corr(data)
66/8: import numpy as np
66/9: np.corrcoef(data)
66/10: import matplotlib.plt as plt
66/11: from matplotlib import pyplot as plt
66/12: plt.countourf(np.corrcoef(data))
66/13: plt.countourf(np.corrcoef(data))
66/14: plt.contourf(np.corrcoef(data))
65/158:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions /= np.sum(array)
            self.mdp[state] = actions
65/159:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/160:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions /= np.sum(actions)
            self.mdp[state] = actions
65/161:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/162:
class Player():
    def __init__(self):
        self.position = (0, 0)
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions = actions / np.sum(actions)
            self.mdp[state] = actions
65/163:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/164: players[0].mdp
65/165:
class Player():
    def __init__(self):
        self.position = (0, 0)
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions = actions / np.sum(actions)
            self.mdp[state] = actions
65/166:
while any([player.can_move for player in players]]:
    for player in players:
        player.can_move = False
65/167:
while any([player.can_move for player in players]):
    for player in players:
        player.can_move = False
65/168:
class Player():
    def __init__(self):
        self.position = (0, 0)
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions = actions / np.sum(actions)
            self.mdp[state] = actions
65/169:
while any([player.can_move for player in players]):
    for player in players:
        player.can_move = False
65/170:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/171: players[0].mdp
65/172:
while any([player.can_move for player in players]):
    for player in players:
        player.can_move = False
65/173:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append(1)
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append(1)
            else:
                actions.append(0)
            actions = np.array(actions)
            actions = actions / np.sum(actions)
            self.mdp[state] = actions
65/174:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/175:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            player.
65/176:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            movement = random.choice(range(4), p=player.mdp[tuple(player.position)])
65/177:
import matplotlib.pyplot as plt
from random import randint, choice
import random
import numpy as np
65/178:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            movement = random.choice(range(4), p=player.mdp[tuple(player.position)])
    break
65/179:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            movement = random.choice(range(4), p=player.mdp[tuple(player.position)])
    break
65/180: myarray = random.choice([1, 2, 3, 4, 5], p = [0.1, 0.3, 0.5, 0.1, 0.0], size = 50)
65/181: myarray = random.choice([1, 2, 3, 4, 5], [0.1, 0.3, 0.5, 0.1, 0.0], size = 50)
65/182: myarray = random.choice([1, 2, 3, 4, 5], [0.1, 0.3, 0.5, 0.1, 0.0])
65/183: myarray = random.choice([1, 2, 3, 4, 5], p=[0.1, 0.3, 0.5, 0.1, 0.0])
65/184:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            movement = choice(range(4), p=player.mdp[tuple(player.position)])
    break
65/185:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            movement = choice(range(4), p=player.mdp[tuple(player.position)])
    break
69/1:
def game():
    global policy, rewards
    player_num = 1
    player = 'X'
    new_state = '.........'
    episode_sequence = [[new_state]]
    action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
    episode_sequence[-1].append(action)
    new_state = new_state[:action] + player + new_state[action+1:]
    tup = evaluate_pos(new_state)
    while any([0 < len(l) < 9 for l in tup]):
        #print(new_state, action)
        if player_num == 1:
            player_num = 2
            player = 'O'
        else:
            player_num = 1
            player = 'X'
        episode_sequence.append([new_state])
        action = choice(range(9), p=[frac[0]/frac[1] for frac in policy[new_state]])
        episode_sequence[-1].append(action)
        new_state = new_state[:action] + player + new_state[action+1:]
        tup = evaluate_pos(new_state)
    G = 0
    if player_num == 1:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    else:
        if not any([l for l in tup]):
            G = -5
        else:
            G = 0
    if G == -5:
        reward = -5
    else:
        reward = +10
    for q_value in episode_sequence[::-2]:
        action = q_value[1]
        state = q_value[0]
        #print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i][0] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    if G == -5:
        reward = -5
    else:
        reward = -20
    for q_value in episode_sequence[-2::-2]:
        action = q_value[1]
        state = q_value[0]
        #print(state, action, reward)
        rewards[state][action] = (rewards[state][action]*reward_counts[state][action] + reward)/(reward_counts[state][action] + 1)
        reward_counts[state][action] += 1
        # print(rewards[state])
        if reward == -20:
            summation = [policy[state][action][1] - policy[state][action][0], policy[state][action][1]]
            if summation[0]:
                policy[state][action] = [0,1]
                policy[state] = [[val[0]*summation[1], val[1]*summation[0]] for val in policy[state]]
            reward *= gamma
            continue
        A = np.argmax(list(rewards[state].values()))
        num = len([i for i in range(9) if i != A and policy[state][i][0] != 0])
        if num:
            policy[state] = [[9,10] if act == A else [1*(policy[state][act][0] != 0), num*10] for act in range(9)]
        else:
            policy[state] = [[1,1] if act == A else [0,1] for act in range(9)]
        # print(policy[state])
        reward *= gamma
    # print(episode_sequence)
    return G
69/2:
import numpy as np
from numpy.random import choice
from math import ceil
import json
65/186: choice([1,2,3,4,5])
65/187: choice([1,2,3,4,5],[1,2])
65/188:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
from np.random import choice
65/189:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
from numpy.random import choice
65/190:
while any([player.can_move for player in players]):
    for player in players:
        if player.can_move:
            movement = choice(range(4), p=player.mdp[tuple(player.position)])
    break
65/191:
while any([player.can_move for player in players]):
    for _ in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
    break
65/192: states_and_actions = [[],[],[],[],[],[],[],[]]
65/193:
while any([player.can_move for player in players]):
    for _ in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
    break
65/194: states_and_actions
65/195:
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
    break
65/196: states_and_actions
65/197: reward = 0
65/198:
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
    break
65/199: reward
65/200:
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        rewards[player.position[0], player.position[1]] = 0
    break
65/201: states_and_actions
65/202:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        rewards[player.position[0], player.position[1]] = 0
    break
65/203: states_and_actions
65/204: reward
65/205:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
for player in players:
    player.position = [0,0]
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        rewards[player.position[0], player.position[1]] = 0
    break
65/206: states_and_actions
65/207: reward
65/208: rewards[player.position[0], player.position[1]]
65/209: rewards[1, 0]
65/210: print(rewards)
65/211:
def game():
    # Reinitialize rewards, and also the position of the boards
    # Initialize reward to Zero
    # Create new players, with the previous players' mdps
    pass
65/212: rewards = np.ones((40,40)) * 5
65/213: print(rewards)
65/214:
def set_rewards():
    global rewards
    rewards = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
65/215:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
for player in players:
    player.position = [0,0]
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        rewards[player.position[0], player.position[1]] = 0
    break
65/216: states_and_actions
65/217: reward
65/218:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
65/219:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 1000
    return result
65/220:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
rewards = set_rewards()
for player in players:
    player.position = [0,0]
while any([player.can_move for player in players]):
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        rewards[player.position[0], player.position[1]] = 0
    break
65/221: states_and_actions
65/222: reward
65/223:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
rewards = set_rewards()
for player in players:
    player.position = [0,0]
num_moves = 0
while any([player.can_move for player in players]) and num_moves < 1000:
    num_moves += 1
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        if rewards[player.position[0], player.position[1]] == 1000:
            player.can_move = False
            rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
        rewards[player.position[0], player.position[1]] = 0
    break
65/224: states_and_actions
65/225: reward
65/226:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
rewards = set_rewards()
for player in players:
    player.position = [0,0]
num_moves = 0
while any([player.can_move for player in players]) and num_moves < 1000:
    num_moves += 1
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append((player.position, movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        if rewards[player.position[0], player.position[1]] == 1000:
            player.can_move = False
            rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
        rewards[player.position[0], player.position[1]] = 0
65/227: states_and_actions
65/228: reward
65/229:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
rewards = set_rewards()
for player in players:
    player.position = [0,0]
num_moves = 0
while any([player.can_move for player in players]) and num_moves < 1000:
    num_moves += 1
    for i in range(8):
        player = players[i]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        states_and_actions[i].append(((player.position[0], player.position[1]), movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        if rewards[player.position[0], player.position[1]] == 1000:
            player.can_move = False
            rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
        rewards[player.position[0], player.position[1]] = 0
65/230: states_and_actions
65/231: reward
65/232:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
rewards = set_rewards()
for player in players:
    player.position = [0,0]
num_moves = 0
while any([player.can_move for player in players]) and num_moves < 1000:
    num_moves += 1
    for i in range(8):
        player = players[i]
        x = player.position[0]
        y = player.position[1]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        if x > 0 and rewards[x-1][y] == 1000:
            movement = 2
        elif x < 39 and rewards[x+1][y] == 1000:
            movement = 1
        elif y > 0 and rewards[x][y-1] == 1000:
            movement = 4
        elif y < 39 and rewards[x][y+1] == 1000:
            movement = 3
        elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
            movement = choice([1,3])
        elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
            movement = choice([1,4])
        elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
            movement = choice([2,4])
        elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
            movement = choice([2,3])
        states_and_actions[i].append(((player.position[0], player.position[1]), movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        if rewards[player.position[0], player.position[1]] == 1000:
            player.can_move = False
            rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
        rewards[player.position[0], player.position[1]] = 0
65/233: states_and_actions
65/234: reward
65/235:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/236:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append((20,1))
            else:
                actions.append(0)
            if state[0] >= 1:
                actions.append((20,1))
            else:
                actions.append(0)
            if state[1] + 1 < 40:
                actions.append((20,1))
            else:
                actions.append(0)
            if state[1] > 0:
                actions.append((20,1))
            else:
                actions.append(0)
            actions = np.array(actions)
            self.mdp[state] = actions
65/237:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/238:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append((20,1))
            else:
                actions.append((0,0))
            if state[0] >= 1:
                actions.append((20,1))
            else:
                actions.append((0,0))
            if state[1] + 1 < 40:
                actions.append((20,1))
            else:
                actions.append((0,0))
            if state[1] > 0:
                actions.append((20,1))
            else:
                actions.append((0,0))
            actions = np.array(actions)
            self.mdp[state] = actions
65/239:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/240: players[0].mdp
65/241:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        for state_action in states_and_actions[i][::-1]:
65/242:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1) 
            player.mdp[state_action[0]][state_action[1]][1] += 1
65/243: states_and_actions
65/244: reward
65/245:
states_and_actions = [[],[],[],[],[],[],[],[]]
reward = 0
rewards = set_rewards()
for player in players:
    player.position = [0,0]
num_moves = 0
while any([player.can_move for player in players]) and num_moves < 1000:
    num_moves += 1
    for i in range(8):
        player = players[i]
        x = player.position[0]
        y = player.position[1]
        if not player.can_move:
            continue
        movement = choice(range(1, 5), p=player.mdp[tuple(player.position)])
        if x > 0 and rewards[x-1][y] == 1000:
            movement = 2
        elif x < 39 and rewards[x+1][y] == 1000:
            movement = 1
        elif y > 0 and rewards[x][y-1] == 1000:
            movement = 4
        elif y < 39 and rewards[x][y+1] == 1000:
            movement = 3
        elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
            movement = choice([1,3])
        elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
            movement = choice([1,4])
        elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
            movement = choice([2,4])
        elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
            movement = choice([2,3])
        states_and_actions[i].append(((player.position[0], player.position[1]), movement))
        if movement == 1:
            player.position[0] += 1
        elif movement == 2:
            player.position[0] -= 1
        elif movement == 3:
            player.position[1] += 1
        else:
            player.position[1] -= 1
        reward += rewards[player.position[0], player.position[1]]
        if rewards[player.position[0], player.position[1]] == 1000:
            player.can_move = False
            rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
        rewards[player.position[0], player.position[1]] = 0
65/246: states_and_actions
65/247: players[1].mdp
65/248: game()
65/249: states_and_actions
65/250:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1) 
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/251: game()
65/252: game()
65/253: game()
65/254: game()
65/255:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1) 
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/256: game()
65/257: game()
65/258: set_rewards()
65/259: arr = set_rewards()
65/260: arr.count(1000)
65/261: arr.amax()
65/262: np.amax(arr)
65/263:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
65/264:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1) 
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/265: game()
65/266: game()
65/267: game()
65/268: game()
65/269: game()
65/270: game()
65/271: players[1].mdp
65/272:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/273:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1) 
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/274: game()
65/275: players[1].mdp
65/276: players[1].mdp[(0,0)][1]
65/277: players[1].mdp[(0,0)][0]
65/278: players[1].mdp[(0,0)][0][0]
65/279:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            print((Q * n + given_rew)/(n+1))
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/280: game()
65/281:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            print((Q * n + given_rew)/(n+1))
            print(player.mdp[state_action[0]][state_action[1]][0])
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/282: game()
65/283:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = float((Q * n + given_rew)/(n+1))
            print((Q * n + given_rew)/(n+1))
            print(player.mdp[state_action[0]][state_action[1]][0])
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/284: game()
65/285: type(players[1].mdp)
65/286: type(players[1].mdp[(0,0)])
65/287: type(players[1].mdp[(0,0)][0])
65/288: type(players[1].mdp[(0,0)][0][0])
65/289:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append((20,1))
            else:
                actions.append((0,0))
            if state[0] >= 1:
                actions.append((20,1))
            else:
                actions.append((0,0))
            if state[1] + 1 < 40:
                actions.append((20,1))
            else:
                actions.append((0,0))
            if state[1] > 0:
                actions.append((20,1))
            else:
                actions.append((0,0))
            actions = np.array(actions, dtype=np.float64)
            self.mdp[state] = actions
65/290:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            print((Q * n + given_rew)/(n+1))
            print(player.mdp[state_action[0]][state_action[1]][0])
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/291: game()
65/292:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/293:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            print((Q * n + given_rew)/(n+1))
            print(player.mdp[state_action[0]][state_action[1]][0])
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/294: game()
65/295:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            print((Q * n + given_rew)/(n+1))
            print(player.mdp[state_action[0]][state_action[1]][0])
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/296: game()
65/297:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/298: game()
65/299: game()
65/300: game()
65/301: game()
65/302: game()
65/303: players[1].mdp
65/304:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append((6,1))
            else:
                actions.append((0,0))
            if state[0] >= 1:
                actions.append((6,1))
            else:
                actions.append((0,0))
            if state[1] + 1 < 40:
                actions.append((6,1))
            else:
                actions.append((0,0))
            if state[1] > 0:
                actions.append((6,1))
            else:
                actions.append((0,0))
            actions = np.array(actions, dtype=np.float64)
            self.mdp[state] = actions
65/305:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
65/306:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/307: game()
65/308: players[1].mdp
65/309: game()
65/310: game()
65/311: game()
65/312: game()
65/313: game()
65/314: players[1].mdp
65/315:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
65/316: game()
70/1:
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
70/2:
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
70/3:
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
70/4:
predictions = model(x_train[:1]).numpy()
predictions
70/5: tf.nn.softmax(predictions).numpy()
70/6: loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
70/7: loss_fn(y_train[:1], predictions).numpy()
70/8:
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])
70/9: model.fit(x_train, y_train, epochs=5)
70/10: model.evaluate(x_test,  y_test, verbose=2)
71/1: import sklearn
71/2: import sklearn
71/3: import sklearn
71/4: import sklearn
71/5: import sklearn
71/6: from sklearn.linear_model import LinearRegression
71/7: import pandas as pd
71/8: data = pd.read_csv("CorrelationMatrix_returns.csv")
71/9: data.head()
71/10: import numpy as np
71/11: np.corrcoef(data)
71/12: from matplotlib import pyplot as plt
71/13: new_data = np.corrcoef(data)
71/14: print(new_data[0])
71/15:
x = range(1,101)
y = new_data[0]
plt.plot(x,y)
71/16:
x = range(1,101)
y = new_data[0]
plt.scatter(x,y)
71/17:
x = range(3,101)
y = new_data[0][2:]
plt.scatter(x,y)
71/18:
x = range(1,101)
y = new_data[3]
plt.scatter(x,y)
71/19: print(new_data[3])
71/20:
sectors = [] # THis will store 100 lists, with 25 elements each
for row in new_data:
    sectors.append([i for i in range(0,100) if row[i] >= np.percentile(row, 75)])
71/21: print(sectors)
71/22:
for row in sectors:
    print row
71/23:
for row in sectors:
    print(row)
71/24:
for row in sectors:
    print(len(row) + ": " row)
71/25:
for row in sectors:
    print(len(row) + ": " +  row)
71/26:
for row in sectors:
    print(len(row), ": ", row)
71/27:
separation_between_nums = []
for i in range(len(sectors)):
    for j in range(i, len(sectors):
        separation_between_nums.append(len([num for num in sectors[i] if num not in sectors[j]] + [num for num in sectors[j] if num not in sectors[ji]))
71/28:
separation_between_nums = []
for i in range(len(sectors)):
    for j in range(i, len(sectors):
        separation_between_nums.append(len([num for num in sectors[i] if num not in sectors[j]] + [num for num in sectors[j] if num not in sectors[j]))
71/29:
separation_between_nums = []
for i in range(len(sectors)):
    for j in range(i, len(sectors):
        separation_between_nums.append(len([num for num in sectors[i] if num not in sectors[j]] + [num for num in sectors[j] if num not in sectors[i]]))
71/30:
separation_between_nums = []
for i in range(len(sectors)):
    for j in range(i, len(sectors)):
        separation_between_nums.append(len([num for num in sectors[i] if num not in sectors[j]] + [num for num in sectors[j] if num not in sectors[i]]))
71/31: print(separation_between_nums)
71/32:
new_sepn = np.array(separation_between_nums)
np.percentile(new_sepn, 25)
71/33:
new_sepn = np.array(separation_between_nums)
np.percentile(new_sepn, 50)
71/34: np.percentile(new_sepn, 75)
71/35: new_sepn = np.array(separation_between_nums)
71/36: np.percentile(new_sepn, 75)
71/37: np.percentile(new_sepn, 100)
71/38: np.percentile(new_sepn, 25)
71/39:
index = 0
new_sectors = []
for i in range(100):
    new_sectors.append([])
    for j in range(i, 100):
        if new_sepn[index] <= 20:
            new_sectors[i].append(j)
        index += 1
71/40: new_sectors
72/1: data.head()
72/2: data.head()
72/3: data = pd.read_csv('input_game.csv')
72/4: import pandas as pd
72/5: data = pd.read_csv('input_game.csv')
72/6: data.head()
72/7: data = pd.read_csv('input_game.csv')
72/8: data.head()
72/9: data
72/10: data['game_id' == 0]
72/11: data['game_id' = 0]
72/12: data['game_id' == 0]
72/13: data = pd.DataFrame(data)
72/14: data['game_id' == 0]
72/15: data[1]
72/16: data.columns
72/17: data['game_id']
72/18: data[data['game_id'] == 0]
72/19: game_one_data = data[data['game_id'] == 0]
72/20: game_one_data
72/21: game_one_data['p1_lies'] = game_one_data['p1_action'] == 'TRUST'
72/22: game_one_data
72/23: game_one_data['p1_lies'] = game_one_data['p1_action'] == 'CHEAT'
72/24: game_one_data
72/25: game_one_data['p1_lies'] = 1*(game_one_data['p1_action'] == 'TRUST')
72/26: game_one_data
72/27: game_one_data['p2_lies'] = 1*(game_one_data['p2_action'] == 'TRUST')
72/28: game_one_data
72/29: pd.cumsum(game_one_data['p1_lies'])
72/30: game_one_data['p1_lies'].cumsum()
72/31: game_one_data['p1_lies'].mean()
72/32: game_one_data['p1_lies'].cummean()
72/33: game_one_data['p1_lies'].expanding().mean()
72/34: game_one_data['p2_lies'].expanding().mean()
72/35:
import pandas as pd
import numpy as np
72/36: np.corrcoef(game_one_data['p1_lies'], game_one_data['p2_lies'].expanding().mean())
72/37: np.corrcoef(game_one_data['p1_lies'][:-1], game_one_data['p2_lies'][1:])
72/38: np.corrcoef(game_one_data['p1_lies'][1:], game_one_data['p2_lies'][:-1])
72/39: data.tail()
72/40: data.tail()
72/41: player_1_games = data[data['p1_id'] == 174]
72/42: player_1_games
72/43:
player_1_games['p1_lies'] = 1*(player_1_games['p1_action']=='TRUST')
player_1_games['p2_lies'] = 1*(player_1_games['p2_action']=='TRUST')
72/44: player_1_games
72/45: np.corrcoef(player_1_games['p1_lies'][1:], player_1_games['p2_lies'][:-1])
72/46: data['player_id'].value_counts
72/47: data['player_id'].value_counts()
72/48: data['player_id'].value_counts
72/49: data['p1_id'].value_counts
72/50: data['p1_id'].value_counts()
72/51: data['p1_id'].values()
72/52: data['p1_id'].value_counts()
72/53: type(data['p1_id'].value_counts())
72/54: data['p1_id'].value_counts().index()
72/55: data['p1_id'].value_counts().index
72/56: value_counts = list(data['p1_id'].value_counts().index)
72/57: value_counts
72/58: 174 in value_counts
72/59:
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    player_games['p1_lies'] = 1*(player_games['p1_action'] == 'TRUST')
    player_games['p2_lies'] = 1*(player_games['p2_action'] == 'TRUST')
    print(np.corrcoef(player_games['p1_lies'][1:], player_games['p2_lies'][:-1]))
72/60:
copycat_coeffs = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    player_games['p1_lies'] = 1*(player_games['p1_action'] == 'TRUST')
    player_games['p2_lies'] = 1*(player_games['p2_action'] == 'TRUST')
    copycat_coeffs.append(np.corrcoef(player_games['p1_lies'][1:], player_games['p2_lies'][:-1]))
72/61: copycat_coeffs
72/62:
copycat_coeffs = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    player_games['p1_lies'] = 1*(player_games['p1_action'] == 'TRUST')
    player_games['p2_lies'] = 1*(player_games['p2_action'] == 'TRUST')
    copycat_coeffs.append(np.corrcoef(player_games['p1_lies'][1:], player_games['p2_lies'][:-1])[0][1])
72/63: copycat_coeffs
72/64:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.5:
        print(i)
72/65:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.5:
        print(value_counts[i])
72/66:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.8:
        print(value_counts[i])
72/67:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.75:
        print(value_counts[i])
72/68: len(value_counts)
72/69: 13 in value_counts
72/70: dictionary_of_strategies = {}
72/71:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.75:
        dictionary_of_strategies[i] = 'copycat'
72/72: dictionary_of_strategies
72/73: dictionary_of_strategies = {}
72/74:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.75:
        dictionary_of_strategies[value_counts[i]] = 'copycat'
72/75: dictionary_of_strategies
72/76:
data['p1_lies'] = 1*(data['p1_action'] == 'TRUST')
data['p2_lies'] = 1*(data['p2_action'] == 'TRUST')
72/77:
copycat_coeffs = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    copycat_coeffs.append(np.corrcoef(player_games['p1_lies'][1:], player_games['p2_lies'][:-1])[0][1])
72/78:
copycat_coeffs = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    copycat_coeffs.append(np.corrcoef(player_games['p1_lies'][1:], player_games['p2_lies'][:-1])[0][1])
72/79: copycat_coeffs
72/80: copycat_coeffs[value_counts.index(174)]
72/81: dictionary_of_strategies
72/82: new_data = data.grouby('game_id').agg(np.coercoff(data['p1_lies'], data['p2_lies']))
72/83: new_data = data.groupby('game_id').agg(np.coercoff(data['p1_lies'], data['p2_lies']))
72/84: new_data = data.groupby('game_id').agg(np.corrcoef(data['p1_lies'], data['p2_lies']))
72/85: new_data = data.groupby('game_id').agg(np.corr(data['p1_lies'], data['p2_lies']))
72/86: new_data = data.groupby('game_id').corr()
72/87: new_data = data['game_id', 'p1_lies', 'p2_lies'].groupby('game_id').corr()
72/88: new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/89: new_data
72/90: new_data
72/91:
threshold = 0.5
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.loc[('p1_lies', 'p2_lies')]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/92:
threshold = 0.5
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.loc['p1_lies', 'p2_lies']
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/93:
threshold = 0.5
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/94: data['p2_lies'] = data['p2_lies'].shift(-1)
72/95:
data['p2_lies']
# new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/96:
data['p2_lies'].head()
# new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/97: data['p2_lies'] = data['p2_lies'].shift(1)
72/98:
data['p2_lies'].head()
# new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/99:
data = data.dropna()
# new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/100:
data
# new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/101:
# data
new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/102:
threshold = 0.5
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/103:
threshold = 0.5
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/104: data[]
72/105: new_data
72/106: data[data['game_id'] == 0]
72/107:
data = pd.read_csv('input_game.csv')
data = pd.DataFrame(data)
data['p1_lies'] = 1*(data['p1_action'] == 'TRUST')
data['p2_lies'] = 1*(data['p2_action'] == 'TRUST')
72/108: data[data['game_id'] == 0]
72/109: data['p2_lies'] = data['p2_lies'].shift(1)
72/110: data[data['game_id'] == 0]
72/111:
data.dropna()
new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/112: data[data['game_id'] == 0]
72/113:
data=data.dropna()
new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/114: data[data['game_id'] == 0]
72/115:
copycat_coeffs = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    copycat_coeffs.append(np.corrcoef(player_games['p1_lies'], player_games['p2_lies'])[0][1])
72/116:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.75:
        print(value_counts[i])
        # dictionary_of_strategies[value_counts[i]] = 'copycat'
72/117:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.6:
        print(value_counts[i])
        # dictionary_of_strategies[value_counts[i]] = 'copycat'
72/118:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.5:
        print(value_counts[i])
        # dictionary_of_strategies[value_counts[i]] = 'copycat'
72/119:
for i in range(len(copycat_coeffs)):
    if copycat_coeffs[i] > 0.75:
        print(value_counts[i])
        # dictionary_of_strategies[value_counts[i]] = 'copycat'
72/120: dictionary_of_strategies
72/121:
data=data.dropna()
new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/122:
threshold = 0.5
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/123:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        print(f"High correlation for game_id {game_id}: {corr_p1_p2}")
72/124:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data['game_id' == game_id]['p1_id'][0]
            print(player1)
72/125:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data['game_id' == game_id][0]['p1_id']
            print(player1)
72/126:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            print(data['game_id' == game_id])
            # print(player1)
72/127:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            print(data[data['game_id'] == game_id])
            # print(player1)
72/128:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            print(data[data['game_id'] == game_id][0]['p1_id'])
            # print(player1)
72/129:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            print(data[data['game_id'] == game_id]['p1_id'][0])
            # print(player1)
72/130:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            print(data[data['game_id'] == game_id]['p1_id'].iloc[0])
            # print(player1)
72/131:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/132:
reverse_copycats = {}
potential_copycats = {}
72/133:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/134: reverse_copycats
72/135: potential_copycats
72/136:
def reinitialize_data():
    global data
    data = pd.read_csv('input_game.csv')
    data = pd.DataFrame(data)
    data['p1_lies'] = 1*(data['p1_action'] == 'TRUST')
    data['p2_lies'] = 1*(data['p2_action'] == 'TRUST')
72/137:
reinitialize_data()
# data['rolling_coeff_2'] = data['p2_lies'].rolling().mean(n=2)
72/138: data['rolling_coeff_2'] = data['p2_lies'].rolling().mean(n=2)
72/139: data['rolling_coeff_2'] = data['p2_lies'].rolling(n=2).mean()
72/140: data['rolling_coeff_2'] = data['p2_lies'].rolling(2).mean()
72/141: data['rolling_coeff_2']
72/142:
data['rolling_coeff_3'] = data['p2_lies'].rolling(3).mean()
data['rolling_coeff_4'] = data['p2_lies'].rolling(4).mean()
72/143: data = data.dropna()
72/144: data
72/145:
data['rolling_coeff_2'].shift(1)
data['rolling_coeff_3'].shift(2)
data['rolling_coeff_4'].shift(3)
data = data.dropna()
72/146: data.head()
72/147:
data['rolling_coeff_2'] = data['rolling_coeff_2'].shift(1)
data['rolling_coeff_3'] = data['rolling_coeff_3'].shift(2)
data['rolling_coeff_4'] = data['rolling_coeff_4'].shift(3)
data = data.dropna()
72/148: data.head()
72/149:
two_rolling = []
three_rolling = []
four_rolling = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    two_rolling.append(np.corrcoef(player_games['p1_lies'], player_games['rolling_coeff_2'])[0][1])
    three_rolling.append(np.corrcoef(player_games['p1_lies'], player_games['rolling_coeff_3'])[0][1])
    four_rolling.append(np.corrcoef(player_games['p1_lies'], player_games['rolling_coeff_4'])[0][1])
72/150: two_rolling
72/151:
for value in two_rolling:
    if abs(value) > 0.5:
        print(abs(value))
72/152:
for value in two_rolling:
    if abs(value) > 0.6:
        print(abs(value))
72/153:
for i in range(len(two_rolling)):
    value = two_rolling[i]
    if abs(value) > 0.6:
        print(value_counts[i])
        print(abs(value))
72/154:
for i in range(len(two_rolling)):
    value = two_rolling[i]
    if abs(value) > 0.7:
        print(value_counts[i])
        print(abs(value))
72/155:
for i in range(len(three_rolling)):
    value = three_rolling[i]
    if abs(value) > 0.7:
        print(value_counts[i])
        print(abs(value))
72/156:
for i in range(len(four_rolling)):
    value = four_rolling[i]
    if abs(value) > 0.7:
        print(value_counts[i])
        print(abs(value))
72/157:
for i in range(len(two_rolling)):
    value = two_rolling[i]
    if abs(value) > 0.7:
        # print(value_counts[i])
        print(abs(value))
72/158:
for i in range(len(two_rolling)):
    value = two_rolling[i]
    if abs(value) > 0.8:
        print(value_counts[i])
        print(abs(value))
72/159:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
72/160:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id]
    plt.plot(data['p1_lies'], data['p2_lies'])
    break
72/161:
for player_id in value_counts[2:]:
    player_games = data[data['p1_id']==player_id]
    plt.plot(data['p1_lies'], data['p2_lies'])
    break
72/162:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id]
    x = range(len(player_games))
    plt.plot(x, data['p1_lies'], x, data['p2_lies'])
    break
72/163:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id]
    x = range(len(player_games))
    plt.plot(x, player_games['p1_lies'], x, player_games['p2_lies'])
    break
72/164:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'])
    break
72/165:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'])
    break
72/166:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],alpha=0.3)
    break
72/167:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'],'r', x, player_games['p2_lies'],'b',alpha=0.3)
    break
72/168:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],'b',alpha=0.3)
    break
72/169:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],marker='b',alpha=0.3)
    break
72/170:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],'b',alpha=0.3)
    break
72/171:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],alpha=0.3)
    break
72/172:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:100]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],alpha=0.5)
    break
72/173:
for player_id in value_counts[1:]:
    player_games = data[data['p1_id']==player_id][1:1000]
    x = range(len(player_games))
    plt.scatter(x, player_games['p1_lies'], x, player_games['p2_lies'],alpha=0.5)
    break
72/174:
average_std = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    average_std.append(player_games['p1_lies'].rolling(4).mean().std())
72/175: average_std
72/176: value_counts.index(5)
72/177: average_std[73]
72/178: average_std[72:75]
72/179:
for i in range(len(value_counts)):
    if average_std[i] < 0.1:
        print(value_counts[i])
72/180:
for i in range(len(value_counts)):
    if average_std[i] < 0.1:
        print(value_counts[i], ": ", average_std[i])
72/181: dictionary_of_strategies
72/182:
dictionary_of_repeat_fours = {}
for i in range(len(value_counts)):
    if average_std[i] < 0.1:
        print(value_counts[i], ": ", average_std[i])
        dictionary_of_repeat_fours[value_counts[i]] = 'repeat_four'
72/183: dictionary_of_repeat_fours
72/184:
dictionary_of_repeat_fours = {}
for i in range(len(value_counts)):
    if average_std[i] < 0.2:
        print(value_counts[i], ": ", average_std[i])
        dictionary_of_repeat_fours[value_counts[i]] = 'repeat_four'
72/185: dictionary_of_repeat_fours
72/186:
average_3_std = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    average_std.append(player_games['p1_lies'].rolling(3).mean().std())
72/187: average_3_std
72/188:
average_3_std = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    average_std.append(player_games['p1_lies'].rolling(3).mean().std())
72/189: average_3_std
72/190:
average_std = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    average_std.append(player_games['p1_lies'].rolling(4).mean().std())
72/191:
average_3_std = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    average_3_std.append(player_games['p1_lies'].rolling(3).mean().std())
72/192: average_3_std
72/193: average_3_std.min()
72/194: min(average_3_std)
72/195: min(average_3_std)
72/196:
for i in range(len(average_3_std)):
    if abs(average_3_std) < 0.12:
        print(i)
72/197:
for i in range(len(average_3_std)):
    if abs(average_3_std[i]) < 0.12:
        print(value_counts[i])
72/198:
means = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    means.append(player_games['p1_lies'].mean())
72/199: means
72/200:
for i in range(len(value_counts)):
    if abs(means-1) < 0.1:
        print(value_counts[i])
72/201:
for i in range(len(value_counts)):
    if abs(means[i]-1) < 0.1:
        print(value_counts[i])
72/202:
for i in range(len(value_counts)):
    if abs(means[i]) < 0.1:
        print(value_counts[i])
72/203:
for i in range(len(value_counts)):
    if abs(means[i]-1) < 0.125:
        print(value_counts[i])
72/204:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    continuing.append((player_games['p1_lies'][1:]-player_games[:-1]).std())
72/205: contin
72/206: continuing
72/207: mean
72/208: means
72/209:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    continuing.append((player_games['p1_lies'][1:]-player_games['p1_lies'][:-1]).std())
72/210: continuing
72/211:
for i in range(len(value_counts)):
    if continuing[i]:
        print(continuing[i])
72/212:
for key, value in dictionary_of_repeat_fours.items():
    print(key, value)
72/213:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    continuing.append((player_games['p1_lies'][1:]-player_games['p1_lies'][:-1]).mean())
72/214:
for i in range(len(value_counts)):
    if continuing[i]:
        print(continuing[i])
72/215:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print((player_games['p1_lies'][1:]-player_games['p1_lies'][:-1]).std())
72/216:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print(player_games['p1_lies'][1:]-player_games['p1_lies'][:-1])
    break
72/217:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print(player_games['p1_lies'][1:])
    print(player_games['p1_lies'][:-1])
    print(player_games['p1_lies'][1:]-player_games['p1_lies'][:-1])
    break
72/218:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print(np.array(player_games['p1_lies'])[1:]-np.array(player_games['p1_lies'])[:-1])
    break
72/219:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print(np.std(np.array(player_games['p1_lies'])[1:]-np.array(player_games['p1_lies'])[:-1]))
    break
72/220:
continuing = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    continuing.append(np.std(np.array(player_games['p1_lies'])[1:]-np.array(player_games['p1_lies'])[:-1]))
72/221:
for i in range(len(value_counts)):
    if continuing[i]:
        print(continuing[i])
72/222:
for i in range(len(value_counts)):
    if continuing[i] < 0.5:
        print(continuing[i])
72/223:
for i in range(len(value_counts)):
    if continuing[i] < 0.4:
        print(continuing[i])
72/224:
for i in range(len(value_counts)):
    if continuing[i] < 0.3:
        print(continuing[i])
72/225:
for i in range(len(value_counts)):
    if continuing[i] < 0.3:
        print(value_counts[i], continuing[i])
72/226:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_1_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_1_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/227:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/228:
for i in range(len(value_counts)):
    if switch_on_cheat[i] < 0.1:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/229:
for i in range(len(value_counts)):
    if 0.9 < switch_on_cheat[i]:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/230:
for i in range(len(value_counts)):
    if 0.9 < switch_on_cheat[i]:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/231:
for i in range(len(value_counts)):
    if 0.8 < switch_on_cheat[i]:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/232:
for i in range(len(value_counts)):
    if 0.7 < switch_on_cheat[i]:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/233: max(switch_on_cheat)
72/234:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 'TRUST':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/235:
for i in range(len(value_counts)):
    if 0.7 < switch_on_cheat[i]:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/236: max(switch_on_cheat)
72/237:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 'TRUST':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    print(count_switch_after_prev_cheat)
    print(count_switch)
    break
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/238:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 'TRUST':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    print(count_switch_after_prev_cheat)
    print(count_switch_after_prev_trust)
    print(count_switch)
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/239:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 'TRUST':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    print(count_switch_after_prev_cheat)
    print(count_switch_after_prev_trust)
    print(count_switch)
    break
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/240:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            print(player_games['p2_lies'].iloc[i-1])
            if player_games['p2_lies'].iloc[i-1] == 'CHEAT':
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 'TRUST':
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    print(count_switch_after_prev_cheat)
    print(count_switch_after_prev_trust)
    print(count_switch)
    break
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/241:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 0:
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 1:
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    print(count_switch_after_prev_cheat)
    print(count_switch_after_prev_trust)
    print(count_switch)
    break
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/242:
switch_on_cheat = []
switch_on_trust = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    count_switch = 0
    prev = player_games['p1_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p1_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p2_lies'].iloc[i-1] == 0:
                count_switch_after_prev_cheat += 1
            if player_games['p2_lies'].iloc[i-1] == 1:
                count_switch_after_prev_trust += 1
        prev = player_games['p1_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
72/243:
for i in range(len(value_counts)):
    if 0.9 < switch_on_cheat[i]:
        print(value_counts[i], ": ", switch_on_cheat[i])
72/244:
for i in range(len(value_counts)):
    if 0.9 < switch_on_trust[i]:
        print(value_counts[i], ": ", switch_on_trust[i])
72/245:
for i in range(len(value_counts)):
    if 0.9 < switch_on_trust[i]:
        print(value_counts[i], ": ", switch_on_trust[i])
72/246:
for i in range(len(value_counts)):
    if 0.1 > switch_on_trust[i]:
        print(value_counts[i], ": ", switch_on_trust[i])
72/247:
for i in range(len(value_counts)):
    if 0.8 < switch_on_trust[i]:
        print(value_counts[i], ": ", switch_on_trust[i])
72/248:
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    player_games = player_games.groupby('game_id').agg({'p1_lies': 'mean', 'p2_lies': 'mean'})
    print(player_games)
    break
72/249:
for player_id in value_counts:
    print(player_id)
    player_games = data[data['p1_id']==player_id]
    player_games = player_games.groupby('game_id').agg({'p1_lies': 'mean', 'p2_lies': 'mean'})
    print(player_games)
    break
72/250:
for player_id in value_counts:
    print(player_id)
    player_games = data[data['p1_id']==player_id]
    player_games = player_games.groupby('game_id').agg({'p1_lies': 'mean', 'p2_lies': 'mean'})
    for i in range(len(player_games)):
        player_games[i] = min(player_games[i], 1-player_games[i])
    print(player_games)
    break
72/251: reinitialize_data()
72/252:
five_rolling = []
six_rolling = []
seven_rolling = []
data['rolling_coeff_3'] = data['p2_lies'].rolling(3).mean()
data['rolling_coeff_4'] = data['p2_lies'].rolling(4).mean()
data['rolling_coeff_2'] = data['rolling_coeff_2'].shift(1)
data['rolling_coeff_3'] = data['rolling_coeff_3'].shift(2)
data['rolling_coeff_4'] = data['rolling_coeff_4'].shift(3)
data = data.dropna()
72/253:
five_rolling = []
six_rolling = []
seven_rolling = []
data['rolling_coeff_5'] = data['p2_lies'].rolling(5).mean()
data['rolling_coeff_6'] = data['p2_lies'].rolling(6).mean()
data['rolling_coeff_7'] = data['p2_lies'].rolling(7).mean()
data = data.dropna()
data['rolling_coeff_5'] = data['rolling_coeff_5'].shift(4)
data['rolling_coeff_6'] = data['rolling_coeff_6'].shift(5)
data['rolling_coeff_7'] = data['rolling_coeff_7'].shift(6)
data = data.dropna()
72/254:
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    five_rolling.append(np.corrcoef(player_games['p1_lies'], player_games['rolling_coeff_5'])[0][1])
    six_rolling.append(np.corrcoef(player_games['p1_lies'], player_games['rolling_coeff_6'])[0][1])
    seven_rolling.append(np.corrcoef(player_games['p1_lies'], player_games['rolling_coeff_7'])[0][1])
72/255: print(five_rolling)
72/256:
for i in range(len(value_counts)):
    if abs(five_rolling[i]) > 0.7:
        print(value_counts[i], five_rolling[i])
72/257:
for i in range(len(value_counts)):
    if abs(six_rolling[i]) > 0.7:
        print(value_counts[i], six_rolling[i])
72/258:
for i in range(len(value_counts)):
    if abs(seven_rolling[i]) > 0.7:
        print(value_counts[i], seven_rolling[i])
72/259: reinitialize_data()
72/260:
repeat_2 = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id][2:]
    repeat_2.append(np.std(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2]))
72/261: repeat_2
72/262:
repeat_2 = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id][2:]
    print(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2])
    break
    repeat_2.append(np.std(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2]))
72/263:
repeat_2 = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2])
    break
    repeat_2.append(np.std(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2]))
72/264:
repeat_2 = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    print(np.array(player_games['p1_id'])[2:])
    print(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2])
    break
    repeat_2.append(np.std(np.array(player_games['p1_id'])[2:]-np.array(player_games['p1_id'])[:-2]))
72/265:
repeat_2 = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    # print(np.array(player_games['p1_lies'])[2:]-np.array(player_games['p1_lies'])[:-2])
    # break
    repeat_2.append(np.std(np.array(player_games['p1_lies'])[2:]-np.array(player_games['p1_lies'])[:-2]))
72/266: repeat_2
72/267:
for i in range(len(value_counts)):
    if repeat_2[i] > 0.7:
        print(value_counts[i], repeat_2[i])
72/268:
for i in range(len(value_counts)):
    if repeat_2[i] > 0.8:
        print(value_counts[i], repeat_2[i])
72/269:
for i in range(len(value_counts)):
    if repeat_2[i] < 0.1:
        print(value_counts[i], repeat_2[i])
72/270:
for i in range(len(value_counts)):
    if repeat_2[i] < 0.2:
        print(value_counts[i], repeat_2[i])
72/271: min(value_counts)
72/272: min(repeat_2)
72/273:
for i in range(len(value_counts)):
    if repeat_2[i] < 0.25:
        print(value_counts[i], repeat_2[i])
72/274: data_55 = data[data['p1_id'] == 55 or data['p2_id'] == 55]
72/275: data_55 = data[any(data['p1_id'] == 55, data['p2_id'] == 55)]
72/276: data_55 = data[any([data['p1_id'] == 55, data['p2_id'] == 55])]
72/277: data_55 = data.any([data['p1_id'] == 55, data['p2_id'] == 55])
72/278: data_55 = data_55 = data['p1_id'] == 55 | data['p2_id'] == 55
72/279: data_55 = data['p1_id'] == 55 || data['p2_id'] == 55
72/280: data_55 = data['p1_id'] == 55 | data['p2_id'] == 55
72/281: data_55 = any(data['p1_id'] == 55, data['p2_id'] == 55)
72/282: data_55 = any([data['p1_id'] == 55, data['p2_id'] == 55])
72/283: data_55 = (data['p1_id'] == 55) | (data['p2_id'] == 55)
72/284: data_55
72/285: data_55 = data[(data['p1_id'] == 55) | (data['p2_id'] == 55)]
72/286: data_55
72/287: data_55.to_csv('data_55.csv')
72/288: data_55 = data[(data['p1_id'] == 31) | (data['p2_id'] == 31)]
72/289: data_55.to_csv('data_31.csv')
72/290: reinitialize_data()
72/291:
data['reward'] = data['p1_lies']*(1-data['p2_lies'])
# 1 if 1,1
# 0 if 0,1 
# 2 if 1,0
72/292: data['reward']
72/293: data['reward'] = data['reward'].shift(1)
72/294: data.dropna()
72/295: data = data.dropna()
72/296:
reward_based = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    reward_based.append(np.corrcoef(player_games['p1_lies'], player_games['reward'])[0][1])
72/297: reward_based
72/298:
for i in range(len(value_counts)):
    if abs(reward_based[i]) > 0.8:
        print(value_counts[i], reward_based[i])
72/299: data_55.to_csv('data_176.csv')
72/300: data_55 = data[(data['p1_id'] == 176) | (data['p2_id'] == 176)]
72/301: data_55.to_csv('data_176.csv')
72/302: max(data['reward'])
72/303:
data['reward'] = data['p1_lies']*(2-data['p2_lies'])
# 1 if 1,1
# 0 if 0,1 
# 2 if 1,0
72/304: data['reward'] = data['reward'].shift(1)
72/305:
# data['reward'] = data['reward'].shift(1)
data = data.dropna()
72/306:
reward_based = []
for player_id in value_counts:
    player_games = data[data['p1_id']==player_id]
    reward_based.append(np.corrcoef(player_games['p1_lies'], player_games['reward'])[0][1])
72/307:
for i in range(len(value_counts)):
    if abs(reward_based[i]) > 0.8:
        print(value_counts[i], reward_based[i])
72/308: data_55.to_csv('data_92.csv')
72/309: data_55 = data[(data['p1_id'] == 92) | (data['p2_id'] == 92)]
72/310: data_55.to_csv('data_92.csv')
72/311: new_data[112]
72/312: new_data.iloc[112]
72/313: new_data
72/314: new_data[num_data['game_id'] == 112]
72/315: new_data[new_data['game_id'] == 112]
72/316: new_data[112]
72/317: new_data.iloc[0]
72/318: new_data.iloc[111]
72/319: new_data.iloc[111:113]
72/320: new_data.iloc[111:114]
72/321: new_data.iloc[222:228]
72/322: reinitialize_data()
72/323: new_data = data[['game_id', 'p1_lies', 'p2_lies', 'reward']].groupby('game_id').corr()
72/324: new_data = data[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/325: p1_shifted_back = data
72/326:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/327:
p1_shifted_back['p1_lies'] = p1_shifted_back['p1_lies'].shift(1)
p1_shifted_back = p1_shifted_back.dropna()
72/328: new_data = p1_shifted_back[['game_id', 'p1_lies', 'p2_lies']].groupby('game_id').corr()
72/329:
threshold = 0.75
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/330: potential_copycats
72/331:
threshold = 0.75
potential_copycats = []
reverse_copycats = []
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/332: potential_copycats
72/333:
threshold = 0.75
potential_copycats = {}
reverse_copycats = {}
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p1_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/334: potential_copycats
72/335: 79 in potential_copycats
72/336: new_data
72/337: new_data.iloc[222:228]
72/338:
threshold = 0.75
potential_copycats = {}
reverse_copycats = {}
for game_id, corr_data in new_data.groupby(level=0):
    # Get the correlation coefficient between p1_lies and p2_lies
    corr_p1_p2 = corr_data.iloc[0,1]
    
    # Check if the correlation coefficient is above the threshold
    if abs(corr_p1_p2) > threshold:
        if corr_p1_p2 > 0:
            player1 = data[data['game_id'] == game_id]['p2_id'].iloc[0]
            if player1 not in potential_copycats:
                potential_copycats[player1] = 1
            else:
                potential_copycats[player1] += 1
        if corr_p1_p2 < 0:
            player1 = data[data['game_id'] == game_id]['p2_id'].iloc[0]
            if player1 not in reverse_copycats:
                reverse_copycats[player1] = 1
            else:
                reverse_copycats[player1] += 1
72/339: potential_copycats
72/340: reverse_copycats
72/341: data['xor'] = data['p1_lies']^data['p2_lies']
72/342: data = data.dropna()
72/343: data['xor'] = data['p1_lies']^data['p2_lies']
72/344: data['xor'] = int(data['p1_lies'])^int(data['p2_lies'])
72/345: data['xor'] = data['p1_lies']^data['p2_lies']
72/346: type(data['p1_lies'])
72/347: data['xor'] = np.array(data['p1_lies'], dtype='int64')^np.array(data['p2_lies'], dtype='int64')
72/348: data['xor']
72/349: data.to_csv('Modified_data.csv')
72/350: data['xor_prev'] = [0] + np.array(data['p1_lies'][1:], dtype='int64')^np.array(data['p2_lies'][:-1])
72/351: data['xor_prev'] = np.array([0] + list(data['p1_lies'][1:]), dtype='int64')^np.array(list(data['p2_lies'][:-1]) + [0])
72/352: data.to_csv('Modified_data.csv')
72/353:
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    for game_id, game_data in player_data.groupby('game_id'):
        print(game_data)
        break
    break
72/354:
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg('p1_lies', 'p2_lies'))
    break
72/355:
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg({'p1_lies', 'count'}))
    break
72/356:
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg({'p1_lies', 'count'}))
    break
72/357:
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg({'p1_lies', 'mean'}))
    break
72/358:
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg('mean'))
    break
72/359:
for player in value_counts:
    player_data = data[data['p1_id'] == player][['p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg('mean'))
    break
72/360:
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    print(player_data.groupby('game_id').agg('mean'))
    break
72/361:
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = min(player_data['p1_lies'], 1-player_data['p1_lies'])
    print(player_data)
    break
72/362:
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.min(player_data['p1_lies'], 1-player_data['p1_lies'])
    print(player_data)
    break
72/363:
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    print(player_data)
    break
72/364:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.index:
        if player_data[game]['p1_lies'] < 0.1:
            player_switches[player] = player_switches.get(player, 0) + 1
    break
72/365:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.index:
        if player_data[game]['p1_lies'] < 0.1:
            if player not in player_switches:
                player_switches[player] =  1
            else:
                player_switches[player] +=  1
    break
72/366:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game, _ in player_data:
        if game['p1_lies'] < 0.1:
            if player not in player_switches:
                player_switches[player] = 1
            else:
                player_switches[player] +=  1
    break
72/367:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data:
        if game['p1_lies'] < 0.1:
            if player not in player_switches:
                player_switches[player] = 1
            else:
                player_switches[player] +=  1
    break
72/368:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data:
        print(game)
        # if game['p1_lies'] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
    break
72/369:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.rows():
        print(game)
        # if game['p1_lies'] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
    break
72/370:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.iterrows():
        print(game)
        # if game['p1_lies'] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
    break
72/371:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.iterrows():
        if game['p1_lies'] < 0.1:
            if player not in player_switches:
                player_switches[player] = 1
            else:
                player_switches[player] +=  1
    break
72/372:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.iterrows():
        if game[0] < 0.1:
            if player not in player_switches:
                player_switches[player] = 1
            else:
                player_switches[player] +=  1
    break
72/373: playe
72/374: player_switches
72/375:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.iterrows():
        if game[0] < 0.1:
            if player not in player_switches:
                player_switches[player] = 1
            else:
                player_switches[player] +=  1
72/376: player_switches
72/377:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.iterrows():
        print(game, game[0])
        if game[0] < 0.1:
            if player not in player_switches:
                player_switches[player] = 1
            else:
                player_switches[player] +=  1
72/378:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    for game in player_data.iterrows():
        print(game[1])
        # if game[0] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
    break
72/379:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    player_switches[player] = (player_data['p1_lies'] < 0.1).sum()
        # if game[0] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
72/380: player_switches
72/381:
for value, keys in player_switches.items():
    print(player, player_switches[player])
72/382:
for value, keys in player_switches.items():
    if keys > 1:
        print(player, player_switches[player])
72/383:
for value, keys in player_switches.items():
    if keys > 1:
        print(player, player_switches[player])
72/384:
for value, keys in player_switches.items():
    if keys > 1:
        print(value, leys)
72/385:
for value, keys in player_switches.items():
    if keys > 1:
        print(value, keys)
72/386:
player_switches = {}
for player in value_counts:
    player_data = data[data['p1_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p1_lies'] = np.minimum(player_data['p1_lies'], 1-player_data['p1_lies'])
    player_switches[player] = (player_data['p1_lies'] < 0.05).sum()
        # if game[0] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
72/387:
for value, keys in player_switches.items():
    if keys > 1:
        print(value, keys)
72/388: rev_value_counts = data['p2_id'].value_counts().index
72/389: rev_value_counts
72/390:
player_switches = {}
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player][['game_id', 'p1_lies', 'p2_lies']]
    # player_data.groupby('game_id').agg('p1_lies', 'p2_lies')
    player_data = player_data.groupby('game_id').agg('mean')
    player_data['p2_lies'] = np.minimum(player_data['p2_lies'], 1-player_data['p2_lies'])
    player_switches[player] = (player_data['p2_lies'] < 0.05).sum()
        # if game[0] < 0.1:
        #     if player not in player_switches:
        #         player_switches[player] = 1
        #     else:
        #         player_switches[player] +=  1
72/391:
for value, keys in player_switches.items():
    if keys > 1:
        print(value, keys)
72/392: data_55 = data[(data['p1_id'] == 20) | (data['p2_id'] == 20)]
72/393: data_55.to_csv('data_20.csv')
72/394:
player = 20
player_data = (data[data['p2_id'] == player]) | (data['p1_id'] == player)
73/1:
player = 20
player_data = data[data['p2_id'] == player]
73/2: reinitialize_data()
73/3:
def reinitialize_data():
    global data
    data = pd.read_csv('input_game.csv')
    data = pd.DataFrame(data)
    data['p1_lies'] = 1*(data['p1_action'] == 'TRUST')
    data['p2_lies'] = 1*(data['p2_action'] == 'TRUST')
73/4: reinitialize_data()
73/5:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
73/6: reinitialize_data()
73/7:
player = 20
player_data = data[data['p2_id'] == player]
73/8: player_data
73/9: player_data['p1_lies'] = player_data['p1_lies'].rolling(2).mean()
73/10: player_data.dropna()
73/11: np.corrcoef(player_data['p1_lies'], player_data['p2_lies'])
73/12: np.corrcoef(player_data['p1_lies'], player_data['p2_lies'])
73/13: player_data['p1_lies']
73/14: player_data = player_data.dropna()
73/15: np.corrcoef(player_data['p1_lies'], player_data['p2_lies'])
73/16:
player = 100
player_data = data[data['p2_id'] == player]
73/17: array = np.array(player_data['p1_lies'][0:-1] | player_data['p2_lies'][1:])
73/18: arr = np.array(player_data['p1_lies'][0:-1] | player_data['p2_lies'][1:])
73/19: arr = np.array(player_data['p1_lies'][0:-1] | player_data['p1_lies'][1:])
73/20: arr = np.array(player_data['p1_lies'][0:-1] | player_data['p1_lies'][1:])[1:]
73/21: np.corrcoef(player_data['p1_lies'][2:], arr)
73/22: arr
73/23: player_data['p1_lies'][0:-1]
73/24:
player = 100
player_data = data[data['p2_id'] == player]
73/25: player_data['p1_lies'][0:-1]
73/26:
player = 100
player_data = data[data['p1_id'] == player]
73/27: player_data['p1_lies'][0:-1]
73/28: arr = np.array(player_data['p2_lies'][0:-1] | player_data['p2_lies'][1:])[1:]
73/29: arr
73/30: np.corrcoef(player_data['p1_lies'][1:], arr)
73/31: arr
73/32: player_data['p1_lies']
73/33: np.corrcoef(player_data['p1_lies'][1:], 1-arr)
73/34: np.corrcoef(player_data['p1_lies'][1:], arr)
73/35: arr
73/36: player_data['p1_lies']
73/37: np.corrcoef(player_data['p1_lies'][2:], arr)
73/38: np.corrcoef(player_data['p1_lies'][1:], arr)
73/39: arr
73/40: player_data['p1_lies']
73/41: data_55 = data[(data['p1_id'] == 100) | (data['p2_id'] == 100)]
73/42: data_55.to_csv('data_100.csv')
73/43:
player = 100
player_data = data[data['p1_id'] == player]
73/44: arr = np.array(player_data['p2_lies'][0:-1] | player_data['p2_lies'][1:])[1:]
73/45:
# np.corrcoef(player_data['p1_lies'][1:], arr)
player_data['p2_lies']
73/46:
# np.corrcoef(player_data['p1_lies'][1:], arr)
player_data['p2_lies']
73/47: arr
73/48: player_data['p2_lies'][-2] | player_data['p2_lies'][-1]
73/49: arr = np.array(np.array(player_data['p2_lies'][0:-1], dtype='int64') | player_data['p2_lies'][1:])[1:]
73/50:
# np.corrcoef(player_data['p1_lies'][1:], arr)
player_data['p2_lies']
73/51: player_data['p2_lies'][-2] | player_data['p2_lies'][-1]
73/52: player_data['p2_lies'][len(player_data)-2] | player_data['p2_lies'][-1]
73/53: player_data['p2_lies'].iloc[len(player_data)-2] | player_data['p2_lies'].iloc[len(player_data)-1]
73/54: arr = np.array(np.array(player_data['p2_lies'][0:-1], dtype='int64') | np.array(player_data['p2_lies'][1:], dtype='int64'))[1:]
73/55:
arr
# player_data['p2_lies'].iloc[len(player_data)-2] | player_data['p2_lies'].iloc[len(player_data)-1]
73/56: np.corrcoef(arr, player_data['p1_lies'][1:])
73/57: np.corrcoef(arr, player_data['p1_lies'][2:])
73/58: plt.plot(arr, player_data['p1_lies'][2:])
73/59: plt.plot(range(len(arr), arr)
73/60: plt.plot(range(len(arr), arr))
73/61: plt.plot(range(len(arr)), arr)
73/62: plt.scatter(range(len(arr)), arr)
73/63:
plt.scatter(range(len(arr)), arr)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:])
73/64:
plt.scatter(range(len(arr)), arr, alpha=0.3)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.3)
73/65:
plt.scatter(range(len(arr)), arr, alpha=0.3)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.3)
73/66:
plt.scatter(range(len(arr)), arr, alpha=0.1)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.1)
73/67:
plt.scatter(range(len(arr)), arr, alpha=0.1)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0)
73/68:
plt.scatter(range(len(arr)), arr, alpha=0.1)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.01)
73/69:
plt.scatter(range(len(arr)), arr, alpha=0.9)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.01)
73/70:
plt.scatter(range(len(arr)), arr, alpha=0.9)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.1)
73/71:
plt.scatter(range(len(arr)), arr, alpha=0.9)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.05)
73/72:
plt.scatter(range(len(arr)), arr, alpha=1)
plt.scatter(range(len(arr)), player_data['p1_lies'][2:], alpha=0.05)
73/73:
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data.shift(1)
    player_data = player_data.dropna()
    np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])
73/74: rev_value_counts = data['p2_id'].value_counts().index
73/75:
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data.shift(1)
    player_data = player_data.dropna()
    np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])
73/76:
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data.shift(1)
    player_data = player_data.dropna()
    np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])
73/77:
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data.shift(1)
    player_data = player_data.dropna()
    print(player_data['p1_lies'])
    # np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])
73/78:
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data.shift(1)
    player_data = player_data.dropna()
    print(player_data['p1_lies'])
    break
    # np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])
73/79:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep']))
73/80: rev_rolling
73/81:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/82: rev_rolling
73/83:
if abs(rev_rolling[i]) > 0.8:
    print(rev_value_counts[i], rev_rolling[i])
73/84:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.8:
        print(rev_value_counts[i], rev_rolling[i])
73/85:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/86:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/87:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(3).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/88:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/89:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(4).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/90:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/91:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(2).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/92:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/93:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(5).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/94:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/95:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(6).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/96:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/97:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(7).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/98:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/99:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(8).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/100:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/101:
rev_rolling = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    player_data['p2_rep'] = player_data['p2_lies'].rolling(9).mean()
    player_data = player_data.dropna()
    player_data['p2_rep'] = player_data['p2_rep'].shift(1)
    player_data = player_data.dropna()
    # print(player_data['p1_lies'])
    # break
    rev_rolling.append(np.corrcoef(player_data['p1_lies'], player_data['p2_rep'])[0][1])
73/102:
for i in range(len(rev_value_counts)):
    if abs(rev_rolling[i]) > 0.7:
        print(rev_value_counts[i], rev_rolling[i])
73/103:
data_86 = data[(data['p1_id'] == 86) | (data['p2_id'] == 86)]
data_86.to_csv('data_86.csv')
73/104: reinitialize_data()
73/105:
switch_on_cheat = []
switch_on_trust = []
for player_id in rev_value_counts:
    player_games = data[data['p2_id']==player_id]
    count_switch = 0
    prev = player_games['p2_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p2_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p1_lies'].iloc[i-1] == 0:
                count_switch_after_prev_cheat += 1
            if player_games['p1_lies'].iloc[i-1] == 1:
                count_switch_after_prev_trust += 1
        prev = player_games['p2_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
73/106:
for i in range(len(rev_value_counts)):
    if 0.9 < switch_on_cheat[i]:
        print(rev_value_counts[i], ": ", switch_on_cheat[i])
73/107:
data_41 = data[(data['p1_id'] == 41) | (data['p2_id'] == 41)]
data_41.to_csv('data_41.csv')
73/108:
data_129 = data[(data['p1_id'] == 129) | (data['p2_id'] == 129)]
data_129.to_csv('data_129.csv')
73/109:
data_189 = data[(data['p1_id'] == 189) | (data['p2_id'] == 189)]
data_189.to_csv('data_189.csv')
73/110:
play_sticks = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    start = player_data['p1_lies'][0]
    count = 0
    same_count = 0
    for i in range(len(player_data)):
        count += 1
        if player_data['p1_lies'].iloc[i] == start:
            same_count += 1
    play_sticks.append(same_count/count)
73/111: value_counts = data['p1_id'].value_counts().index
73/112:
play_sticks = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    start = player_data['p1_lies'][0]
    count = 0
    same_count = 0
    for i in range(len(player_data)):
        count += 1
        if player_data['p1_lies'].iloc[i] == start:
            same_count += 1
    play_sticks.append(same_count/count)
73/113:
play_sticks = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    start = player_data['p1_lies'].iloc[0]
    count = 0
    same_count = 0
    for i in range(len(player_data)):
        count += 1
        if player_data['p1_lies'].iloc[i] == start:
            same_count += 1
    play_sticks.append(same_count/count)
73/114: play_sticks
73/115:
for i in range(len(value_counts)):
    if 0.7 < play_sticks[i]:
        print(value_counts[i], ": ", play_sticks[i])
73/116:
for i in range(len(value_counts)):
    if 0.8 < play_sticks[i]:
        print(value_counts[i], ": ", play_sticks[i])
73/117: value_counts.index(189)
73/118: list(value_counts).index(189)
73/119: play_sticks[59:52]
73/120: play_sticks[59]
73/121: play_sticks[60]
73/122: play_sticks[61]
73/123: play_sticks[60]
73/124:
for i in range(len(value_counts)):
    if 0.7 < play_sticks[i]:
        print(value_counts[i], ": ", play_sticks[i])
73/125:
true_sticks = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    start = player_data['p1_lies'].iloc[0]
    opp_cheated = False
    count = 0
    true_count = 0
    for i in range(len(player_data)):
        count += 1
        if not opp_cheated and player_data['p1_lies'].iloc[i] == 1:
            true_count += 1
        if opp_cheated and play_sticks['p1_lies'].iloc[i] == 0:
            true_count += 1
        if player_data['p2_lies'].iloc[i] == 0:
            opp_cheated = True
    true_sticks.append(true_count/count)
73/126:
true_sticks = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    opp_cheated = False
    count = 0
    true_count = 0
    for i in range(len(player_data)):
        count += 1
        if not opp_cheated and player_data['p1_lies'].iloc[i] == 1:
            true_count += 1
        if opp_cheated and play_sticks['p1_lies'].iloc[i] == 0:
            true_count += 1
        if player_data['p2_lies'].iloc[i] == 0:
            opp_cheated = True
    true_sticks.append(true_count/count)
73/127:
true_sticks = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    opp_cheated = False
    count = 0
    true_count = 0
    for i in range(len(player_data)):
        count += 1
        if not opp_cheated and player_data['p1_lies'].iloc[i] == 1:
            true_count += 1
        if opp_cheated and player_data['p1_lies'].iloc[i] == 0:
            true_count += 1
        if player_data['p2_lies'].iloc[i] == 0:
            opp_cheated = True
    true_sticks.append(true_count/count)
73/128: true_sticks
73/129:
for i in range(len(value_counts)):
    if true_sticks[i] > 0.8:
        print(value_counts[i], true_sticks[i])
73/130:
true_sticks = []
for player in rev_value_counts:
    player_data = data[data['p2_id'] == player]
    opp_cheated = False
    count = 0
    true_count = 0
    for i in range(len(player_data)):
        count += 1
        if not opp_cheated and player_data['p2_lies'].iloc[i] == 1:
            true_count += 1
        if opp_cheated and player_data['p2_lies'].iloc[i] == 0:
            true_count += 1
        if player_data['p1_lies'].iloc[i] == 0:
            opp_cheated = True
    true_sticks.append(true_count/count)
73/131:
for i in range(len(value_counts)):
    if true_sticks[i] > 0.8:
        print(value_counts[i], true_sticks[i])
73/132:
for i in range(len(rev_value_counts)):
    if true_sticks[i] > 0.8:
        print(value_counts[i], true_sticks[i])
73/133:
betryaed = []
for player in value_counts:
    player_data = data[data['p1_id'] == player]
    opp_cheated = False
    count = 0
    true_count = 0
    for i in range(len(player_data)):
        count += 1
        if not opp_cheated and player_data['p1_lies'].iloc[i] == 1:
            true_count += 1
        if opp_cheated and player_data['p1_lies'].iloc[i] == 0:
            true_count += 1
        if player_data['p2_lies'].iloc[i] == 0 and player_data['p1_lies'].iloc[i] == 1:
            opp_cheated = True
    betryaed.append(true_count/count)
73/134:
for i in range(len(value_counts)):
    if betryaed[i] > 0.8:
        print(value_counts[i], betryaed[i])
73/135:
data_155 = data[(data['p1_id'] == 155) | (data['p2_id'] == 155)]
data_155.to_csv('data_155.csv')
73/136:
data_74 = data[(data['p1_id'] == 74) | (data['p2_id'] == 74)]
data_74.to_csv('data_155.csv')
73/137:
for i in range(len(rev_value_counts)):
    if 0.1 > switch_on_cheat[i]:
        print(rev_value_counts[i], ": ", switch_on_cheat[i])
73/138:
switch_on_cheat = []
switch_on_trust = []
for player_id in rev_value_counts:
    player_games = data[data['p2_id']==player_id]
    count_switch = 0
    prev = player_games['p2_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p2_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p1_lies'].iloc[i-1] == 0:
                count_switch_after_prev_cheat += 1
            if player_games['p1_lies'].iloc[i-1] == 1:
                count_switch_after_prev_trust += 1
        prev = player_games['p2_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
73/139:
for i in range(len(rev_value_counts)):
    if 0.1 > switch_on_cheat[i]:
        print(rev_value_counts[i], ": ", switch_on_cheat[i])
73/140:
switch_on_cheat = []
switch_on_trust = []
for player_id in rev_value_counts:
    player_games = data[data['p2_id']==player_id]
    count_switch = 0
    prev = player_games['p2_lies'].iloc[0]
    count_switch_after_prev_cheat = 0
    count_switch_after_prev_trust = 0
    for i in range(len(player_games)):
        if player_games['p2_lies'].iloc[i] != prev:
            count_switch += 1
            if player_games['p1_lies'].iloc[i-1] == 0:
                count_switch_after_prev_cheat += 1
            if player_games['p1_lies'].iloc[i-1] == 1:
                count_switch_after_prev_trust += 1
        prev = player_games['p2_lies'].iloc[i]
    switch_on_cheat.append(count_switch_after_prev_cheat/count_switch)
    switch_on_trust.append(count_switch_after_prev_trust/count_switch)
73/141:
for i in range(len(rev_value_counts)):
    if 0.1 > switch_on_cheat[i]:
        print(rev_value_counts[i], ": ", switch_on_cheat[i])
74/1:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
from numpy.random import choice
74/2:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
from numpy.random import choice
74/3:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
from numpy.random import choice
74/4:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
74/5:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [] for x in range(40)
    for y in range(40)
}
74/6:
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key, value in actions.items():
        actions[key] = 1/len(actions)
74/7: rewards = np.ones((40,40)) * 5
74/8: print(rewards)
74/9:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [] for x in range(40)
    for y in range(40)
}
74/10:
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key, value in actions.items():
        actions[key] = 1/len(actions)
74/11:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key, value in actions.items():
        actions[key] = 1/sum(actions)
74/12:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key in actions:
        actions[key] /= sum(actions)
74/13:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 < 40:
        actions[0] = 0
    if state[0] >= 1:
        actions[1] = 0
    if state[1] + 1 < 40:
        actions[2] = 0
    if state[1] > 0:
        actions[3] = 0
    for key in actions:
        actions[key] /= sum(actions)
74/14:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    for key in actions:
        actions[key] /= sum(actions)
74/15:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    for i in range(4):
        actions[key] /= sum(actions)
74/16:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    for i in range(4):
        actions[i] /= sum(actions)
74/17: print(mdp)
74/18:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    sum = sum(actions)
    for i in range(4):
        actions[i] /= sum(actions)
74/19:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    action_sum = sum(actions)
    for i in range(4):
        actions[i] /= action_sum
74/20:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    action_sum = sum(actions)
    for i in range(4):
        actions[i] /= action_sum
74/21:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    action_sum = sum(actions)
    for i in range(4):
        actions[i] /= action_sum
74/22: sum
74/23: sum(actions)
75/1:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    action_sum = sum(actions)
    for i in range(4):
        actions[i] /= action_sum
75/2:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
75/3: rewards = np.ones((40,40)) * 5
75/4:
import matplotlib.pyplot as plt
from random import randint
import numpy as np
from numpy.random import choice
75/5: num_iters = 0
75/6: rewards = np.ones((40,40)) * 5
75/7: print(rewards)
75/8:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
75/9:
# mdp = state: [action: probability for action]
mdp = {
    (x, y): [1,1,1,1] for x in range(40)
    for y in range(40)
}
for state, actions in mdp.items():
    if state[0] + 1 >= 40:
        actions[0] = 0
    if state[0] - 1 < 0:
        actions[1] = 0
    if state[1] + 1 >= 40:
        actions[2] = 0
    if state[1] < 1:
        actions[3] = 0
    action_sum = sum(actions)
    for i in range(4):
        actions[i] /= action_sum
75/10:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append((6,1))
            else:
                actions.append((0,0))
            if state[0] >= 1:
                actions.append((6,1))
            else:
                actions.append((0,0))
            if state[1] + 1 < 40:
                actions.append((6,1))
            else:
                actions.append((0,0))
            if state[1] > 0:
                actions.append((6,1))
            else:
                actions.append((0,0))
            actions = np.array(actions, dtype=np.float64)
            self.mdp[state] = actions
75/11:
gamma = 0.9
epsilon = 0.1
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/12: players[0].mdp
75/13:
excluded = randint(2, 4)
excluded
75/14:
for i in range(4):
    if i == excluded:
        continue
    if i == 0:
        x = randint(4, 16)
        y = randint(4, 16)
    elif i == 1:
        x = randint(24, 36)
        y = randint(4, 16)
    elif i == 2:
        x = randint(4, 16)
        y = randint(24, 36)
    else:
        x = randint(24, 36)
        y = randint(24, 36)
    rewards[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
75/15:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/16:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 1000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/17: game()
75/18: players[0].mdp
75/19: game()
75/20: game()
75/21: game()
75/22: game()
75/23: game()
75/24: players[0].mdp
75/25:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]):
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/26:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/27:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]):
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/28: game()
75/29:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/30:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/31: game()
75/32: game()
75/33: game()
75/34: game()
75/35: game()
75/36:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/37:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/38: game()
75/39: game()
75/40: game()
75/41: game()
75/42: players[0].mdp
75/43:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/44:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/45: game()
75/46: players[0].mdp
75/47: game()
75/48: players[0].mdp
75/49: game()
75/50: players[0].mdp
75/51:
class Player():
    def __init__(self):
        self.position = [0, 0]
        self.can_move = True
        print(self.position)
        self.mdp = {(x, y): [] for x in range(40) for y in range(40)}
        for state, actions in self.mdp.items():
            if state[0] + 1 < 40:
                actions.append((2,1))
            else:
                actions.append((0,0))
            if state[0] >= 1:
                actions.append((2,1))
            else:
                actions.append((0,0))
            if state[1] + 1 < 40:
                actions.append((2,1))
            else:
                actions.append((0,0))
            if state[1] > 0:
                actions.append((2,1))
            else:
                actions.append((0,0))
            actions = np.array(actions, dtype=np.float64)
            self.mdp[state] = actions
75/52:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/53:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/54: game()
75/55: game()
75/56: game()
75/57: game()
75/58:
def set_rewards():
    result = np.zeros((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
75/59:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/60:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/61: game()
75/62: game()
75/63:
def set_rewards():
    result = np.zeros((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
75/64:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/65: game()
75/66: set_rewards()
75/67: plt.contourf(set_rewards())
75/68: game()
75/69: plt.contourf(set_rewards())
75/70:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] += np.ones((3,3)) * 995
    return result
75/71:
def set_rewards():
    result = np.zeros((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] = np.ones((3,3)) * 1000
    return result
75/72:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/73:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/74: game()
75/75: game()
75/76: game()
75/77: game()
75/78: plt.contourf(set_rewards())
75/79:
for play in range(100):
    game()
75/80: plt.contourf(set_rewards())
75/81:
plt.contourf(set_rewards())
plt.legend()
75/82: plt.contourf(set_rewards())
75/83: plt.contourf(set_rewards())
75/84: plt.contourf(set_rewards())
75/85:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] = np.ones((3,3)) * 995
    return result
75/86:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/87:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/88:
for play in range(100):
    game()
75/89: plt.contourf(set_rewards())
75/90: players[0].mdp
75/91:
for play in range(100):
    print(game())
75/92:
def set_rewards():
    result = np.ones((40,40))*5
    excluded = randint(1,3)
    for i in range(4):
        if i == excluded:
            continue
        if i == 0:
            x = randint(4, 16)
            y = randint(4, 16)
        elif i == 1:
            x = randint(24, 36)
            y = randint(4, 16)
        elif i == 2:
            x = randint(4, 16)
            y = randint(24, 36)
        else:
            x = randint(24, 36)
            y = randint(24, 36)
        # print(x,y)
        result[x-2:x+1,y-2:y+1] = np.ones((3,3)) * 1000
    return result
75/93:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/94:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.9
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
    return reward
75/95:
for play in range(1):
    print(game())
75/96:
for play in range(1):
    print(game())
75/97:
for play in range(1):
    print(game())
75/98:
for play in range(1):
    print(game())
75/99:
for play in range(1):
    print(game())
75/100:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.99
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]]
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/101:
for play in range(1):
    print(game())
75/102:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
75/103: returned_rewards
75/104:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.99
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.99
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/105:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
75/106: returned_rewards
75/107:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.99
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.99
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/108:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
75/109:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/110:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.99
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.99
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/111:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
75/112:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.99
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.99
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/113:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
75/114:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/115:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.99
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.99
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/116:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
75/117: returned_rewards
75/118: mean(returned_rewards[74:]) - mean(returned_rewards[:25])
75/119: np.mean(returned_rewards[74:]) - np.mean(returned_rewards[:25])
75/120: np.mean(returned_rewards[50:]) - np.mean(returned_rewards[:50])
75/121: np.mean(returned_rewards[74:]) - np.mean(returned_rewards[:24])
75/122:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.92
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.92
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/123:
players = []
for _ in range(8):
    player = Player()
    players.append(player)
75/124:
def game():
    states_and_actions = [[],[],[],[],[],[],[],[]]
    gamma = 0.92
    rewards = set_rewards()
    reward = 0
    for player in players:
        player.position = [0,0]
    print("Starting Game")
    num_moves = 0
    while any([player.can_move for player in players]) and num_moves < 10000:
        gamma *= 0.92
        num_moves += 1
        for i in range(8):
            player = players[i]
            x = player.position[0]
            y = player.position[1]
            if not player.can_move:
                continue
            new_list = np.array([val[0] for val in player.mdp[(x,y)]])
            new_list = new_list / np.sum(new_list)
            movement = choice(range(1, 5), p=new_list)
            if x > 0 and rewards[x-1][y] == 1000:
                movement = 2
            elif x < 39 and rewards[x+1][y] == 1000:
                movement = 1
            elif y > 0 and rewards[x][y-1] == 1000:
                movement = 4
            elif y < 39 and rewards[x][y+1] == 1000:
                movement = 3
            elif x < 39 and y < 39 and rewards[x+1][y+1] == 1000:
                movement = choice([1,3])
            elif x < 39 and y > 0 and rewards[x+1][y-1] == 1000:
                movement = choice([1,4])
            elif x > 0 and y > 0 and rewards[x-1][y-1] == 1000:
                movement = choice([2,4])
            elif x > 0 and y < 39 and rewards[x-1][y+1] == 1000:
                movement = choice([2,3])
            states_and_actions[i].append(((player.position[0], player.position[1]), movement-1))
            if movement == 1:
                player.position[0] += 1
            elif movement == 2:
                player.position[0] -= 1
            elif movement == 3:
                player.position[1] += 1
            else:
                player.position[1] -= 1
            reward += rewards[player.position[0], player.position[1]] * gamma
            if rewards[player.position[0], player.position[1]] == 1000:
                print(num_moves)
                # player.can_move = False
                rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3] = np.minimum(rewards[player.position[0] - 2:player.position[0] + 3, player.position[1] - 2:player.position[1] + 3], 5)
            rewards[player.position[0], player.position[1]] = 0
    for i in range(8):
        given_rew = reward/1000
        player = players[i]
        for state_action in states_and_actions[i][::-1]:
            Q = player.mdp[state_action[0]][state_action[1]][0]
            n = player.mdp[state_action[0]][state_action[1]][1]
            player.mdp[state_action[0]][state_action[1]][0] = (Q * n + given_rew)/(n+1)
            player.mdp[state_action[0]][state_action[1]][1] += 1
            given_rew *= gamma
    return reward
75/125:
returned_rewards = []
for play in range(100):
    returned_rewards.append(game())
76/1:
'''
    Distributions
'''

import numpy as np
from matplotlib import pyplot as plt
seed = 101
76/2: beta_list = np.random.beta(4,5,1000)
76/3: print(beta_list)
76/4: exponential_list = np.random.exponential(0.1,1000)
76/5: gamma_list = np.random.gamma(0.1,2)
76/6: laplace_list = np.random.laplace(0,3,1000)
76/7: poisson_list = np.random.poisson(3)
76/8:
'''
    Distributions
'''

import numpy as np
from matplotlib import pyplot as plt
np.random.seed(seed)
76/9: beta_list = np.random.beta(4,5,1000)
76/10: exponential_list = np.random.exponential(0.1,1000)
76/11: gamma_list = np.random.gamma(0.1,2)
76/12: laplace_list = np.random.laplace(0,3,1000)
76/13: poisson_list = np.random.poisson(3)
76/14: plt.plot(beta_list)
76/15: plt.hist(beta_list)
76/16: plt.hist(beta_list, steps=1)
76/17: exponential_list = np.random.exponential(0.1,1000000)
76/18: beta_list = np.random.beta(4,5,1000000)
76/19: plt.hist(beta_list)
76/20: plt.hist(beta_list,bins=55)
76/21:
plt.xlim(-5,50)
plt.hist(beta_list,bins=55)
76/22:
plt.xlim(-5,50)
plt.hist(beta_list*100,bins=55)
76/23:
plt.xlim(-5,50)
plt.hist(beta_list,bins=55)
76/24: print(max(beta_list))
76/25: print(min(beta_list))
76/26: beta_list = np.random.beta(4,5,10000)
76/27:
plt.xlim(-5,50)
plt.hist(100*beta_list,bins=55)
76/28: print(min(beta_list))
76/29: print(max(beta_list))
76/30: print(np.percentile(beta_list,75))
76/31: print(np.percentile(beta_list,80))
76/32: print(np.percentile(beta_list,90))
76/33: print(np.percentile(beta_list,100))
76/34:
plt.xlim(-5,50)
plt.hist(55*beta_list-5,bins=55)
76/35: beta_list = np.random.beta(4,5,1000000)
76/36:
plt.xlim(-5,50)
plt.hist(55*beta_list-5,bins=55)
76/37: normal_list = np.random.normal(0,3,1000000)
76/38:
plt.xlim(-1,11)
plt.plot(normal_list)
76/39:
plt.xlim(-1,11)
plt.hist(normal_list)
76/40: len(normal_list)
76/41:
plt.xlim(-1,11)
plt.hist(normal_list,bins=11)
76/42:
plt.xlim(-10,11)
plt.hist(normal_list,bins=21)
76/43:
# plt.xlim(-5,50)
plt.hist(55*beta_list-5,bins=55)
76/44:
plt.xlim(-10,11)
plt.hist(21*normal_list-10,bins=21)
76/45:
# plt.xlim(-10,11)
plt.hist(21*normal_list-10,bins=21)
76/46: print(max(normal_list))
76/47:
plt.xlim(-10,11)
plt.hist(normal_list,bins=21)
76/48:
plt.xlim(-1,50)
plt.hist(50*exponential_list-1,bins=100)
76/49:
plt.xlim(-1,50)
plt.hist(100*exponential_list,bins=100)
76/50:
plt.xlim(-1,50)
plt.hist(100*exponential_list,bins=51)
76/51:
plt.xlim(-1,50)
plt.hist(100*exponential_list,bins=100)
76/52:
plt.xlim(-1,50)
plt.hist(100*gamma_list,bins=100)
76/53:
plt.xlim(-1,50)
plt.hist(gamma_list,bins=100)
76/54:
# plt.xlim(-1,50)
plt.hist(gamma_list,bins=100)
76/55: gamma_list
76/56: exponential_list = np.random.exponential(0.1,1000000)
76/57:
plt.xlim(-1,50)
plt.hist(100*exponential_list,bins=100)
76/58: gamma_list = np.random.gamma(0.1,2,1000000)
76/59:
# plt.xlim(-1,50)
plt.hist(gamma_list,bins=100)
76/60:
# plt.xlim(-1,50)
plt.hist(gamma_list,bins=100,orientation='horizontal')
76/61:
# plt.xlim(-1,50)
plt.hist(100*gamma_list,bins=100,orientation='horizontal')
76/62:
# plt.xlim(-1,50)
plt.hist(gamma_list,bins=100,orientation='horizontal')
76/63: max(gamma_list)
76/64:
# plt.xlim(-1,50)
plt.hist(50*gamma_list-1,bins=100,orientation='horizontal')
76/65:
plt.xlim(-1,50)
plt.hist(50*gamma_list-1,bins=100,orientation='horizontal')
76/66:
plt.ylim(-1,50)
plt.hist(50*gamma_list-1,bins=100,orientation='horizontal')
76/67: plt.hist(laplace_list,bins=100)
76/68:
'''
    Distributions
'''

import numpy as np
from matplotlib import pyplot as plt
np.random.seed(101)
76/69: laplace_list = np.random.laplace(0,0,1000)
76/70: plt.hist(laplace_list,bins=100)
76/71: laplace_list = np.random.laplace(0.5,0,1000000)
76/72: plt.hist(laplace_list,bins=100)
76/73: laplace_list
76/74: max(laplace_list)
76/75: laplace_list = np.random.laplace(0,0.5,1000000)
76/76: plt.hist(laplace_list,bins=100)
76/77:
plt.xlim(-1,50)
plt.hist(100*laplace_list,bins=100)
76/78:
plt.xlim(-1,50)
plt.hist(100*laplace_list,bins=2000)
76/79:
plt.xlim(-1,50)
plt.hist(100*laplace_list,bins=20000)
76/80:
plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='step')
76/81:
plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='step',bins=100)
76/82:
plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='step',bins=2000)
76/83:
plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='bar',bins=2000)
76/84:
plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='bar',bins=50, range=(-1,50))
76/85:
# plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='bar',bins=50, range=(-1,50))
76/86:
# plt.xlim(-1,50)
plt.hist(100*laplace_list,histtype='bar',bins=51, range=(-1,50))
76/87:
plt.ylim(-1,50)
plt.hist(50*gamma_list-1,bins=51,orientation='horizontal', range=(-1,50))
76/88:
plt.ylim(-1,50)
plt.hist(gamma_list,bins=51,orientation='horizontal', range=(-1,50))
76/89:
plt.ylim(-1,50)
plt.hist(100*gamma_list,bins=51,orientation='horizontal', range=(-1,50))
76/90:
plt.xlim(-1,50)
plt.hist(100*exponential_list,bins=51, range=(-1,50))
76/91:
# plt.xlim(-10,11)
plt.hist(normal_list,bins=21,range=(-10,11))
76/92:
# plt.xlim(-5,50)
plt.hist(100*beta_list,bins=55,range=(-5,50))
76/93: beta_list = np.random.beta(4,20,1000000)
76/94:
# plt.xlim(-5,50)
plt.hist(100*beta_list,bins=55,range=(-5,50))
76/95: plt.hist(poisson_list,bins=12,range=(-1,11))
76/96: poisson_list = np.random.poisson(3,1000000)
76/97: plt.hist(poisson_list,bins=12,range=(-1,11))
78/1:
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
78/2: from sklearn.preprocessing import StandardScaler
78/3: import pandas
78/4: data=pandas.read_csv("./train-dataset/CY_NCA25-1_1-no1.csv")
78/5: data.head()
78/6: data.head()
78/7: from matplotlib import pyplot as plt
78/8: import numpy as np
78/9: plt.plot(np.arange(0, len(data)), data['time/s'])
78/10: plt.plot(np.arange(0, len(data)), data['Ecell/V'])
78/11: max(data['Ecell/V'])
78/12: local_maxima=data[data['Ecell/V']>4]
78/13: local_maxima
78/14: local_maxima_indices= [data['Ecell/V'][i]>data['Ecell/V'][i-1] and data['Ecell/V'][i] > data['Ecell/V'][i+1] for i in range(1, len(data)-1)]
78/15: local_maxima_indices
78/16: data[local_maxima_indices].head()
78/17: data[[False] + local_maxima_indices + [False]].head()
78/18: maximas=data[[False] + local_maxima_indices + [False]].head()
78/19: plt.plot(np.arange(0, len(maximas)), maximas['time/s'])
78/20: max(maximas['time/s'])
78/21: len(maximas)
78/22: maximas=data[[False] + local_maxima_indices + [False]]
78/23: plt.plot(np.arange(0, len(maximas)), maximas['time/s'])
78/24: max(maximas['time/s'])
78/25: plt.plot(np.arange(0, len(data)), data['control/V'])
78/26: data['control/V'][0]
78/27: plt.plot(np.arange(0, len(data)), data['control/V/mA'])
78/28: data['control/V'][0]
78/29: data['control/V/mA'][0]
78/30: np.corrcoef(data['control/V/mA'], data['Ecell/V'])
78/31: np.corrcoef(data['control/V'], data['Ecell/V'])
78/32: maximas=data.groupby('cycle number').max('Ecell/V')
78/33: maximas
78/34: len(maximas)
78/35: plt.plot(np.arange(len(data)),data['Q discharge/mA.h'])
78/36: X=data['control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA']
78/37: X=data['control/V/mA']
78/38: X=data['control/V/mA', 'Ecell/V']
78/39: X=data[['control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA']]
78/40: scaled_X=StandardScaler().fit_transform(X)
78/41: Y=data['Q discharge/mA.h']
78/42: scaled_Y=StandardScaler().fit_transform(Y.values.reshape(-1, 1))
78/43: model=linear_model.LinearRegression()
78/44: model.train(scaled_X, scaled_Y)
78/45: model.fit(scaled_X, scaled_Y)
78/46: model.coef_
78/47: np.corrcoef(data['control/V'],data['control/V/mA'])
78/48: np.corrcoef(data['control/V'],data['control/mA'])
78/49: plt.plot(np.arange(len(data)), model.predict(scaled_X))
78/50: plt.plot(np.arange(len(data)), scaled_Y)
78/51: plt.plot(np.arange(len(data)), data['I/mA'])
78/52: plt.plot(np.arange(len(data)), data['<I>/mA'])
78/53:
currents=np.array(data['<I>/mA'])
times=np.array(data['time/s'])
78/54: integral_currents=currents[1:]*(times[1:]-times[:-1])
78/55: plt.plot(np.arange(len(integral_currents)), integral_currents)
78/56: discharging_X=scaled_X[data['control/V/mA']<0]
78/57: discharging_Y=scaled_Y[data['control/V/mA']<0]
78/58:
model2=linear_model.LinearRegression()
model2.fit(discharging_X, discharging_Y)
78/59: plt.plot(np.arange(len(data)), model2.predict(scaled_X))
78/60: plt.plot(np.arange(len(data)), model2.predict(discharging_X))
78/61: plt.plot(np.arange(len(discharging_X)), model2.predict(discharging_X))
78/62: plt.plot(np.arange(len(discharging_X)), discharging_Y)
78/63: discharging_X=scaled_X[data['control/V/mA']<-500]
78/64: discharging_Y=scaled_Y[data['control/V/mA']<-500]
78/65:
model2=linear_model.LinearRegression()
model2.fit(discharging_X, discharging_Y)
78/66: plt.plot(np.arange(len(discharging_X)), model2.predict(discharging_X))
78/67: plt.plot(np.arange(len(discharging_X)), discharging_Y)
78/68: X=data[['control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/69: discharging_X=scaled_X[data['control/V/mA']<-500]
78/70: discharging_Y=scaled_Y[data['control/V/mA']<-500]
78/71:
model2=linear_model.LinearRegression()
model2.fit(discharging_X, discharging_Y)
78/72: plt.plot(np.arange(len(discharging_X)), model2.predict(discharging_X))
78/73: plt.plot(np.arange(len(discharging_X)), discharging_Y)
78/74: model2.coef_
78/75: scaled_X=StandardScaler().fit_transform(X)
78/76: X=data[['control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/77: scaled_X=StandardScaler().fit_transform(X)
78/78: discharging_X=scaled_X[data['control/V/mA']<-500]
78/79: discharging_Y=scaled_Y[data['control/V/mA']<-500]
78/80:
model2=linear_model.LinearRegression()
model2.fit(discharging_X, discharging_Y)
78/81: plt.plot(np.arange(len(discharging_X)), model2.predict(discharging_X))
78/82: plt.plot(np.arange(len(discharging_X)), discharging_Y)
78/83:
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
78/84: from sklearn.preprocessing import StandardScaler
78/85: import pandas
78/86: data=pandas.read_csv("./train-dataset/CY_NCA25-1_1-no1.csv")
78/87: data.head()
78/88: from matplotlib import pyplot as plt
78/89: import numpy as np
78/90: plt.plot(np.arange(0, len(data)), data['time/s'])
78/91: plt.plot(np.arange(0, len(data)), data['Ecell/V'])
78/92: max(data['Ecell/V'])
78/93: local_maxima_indices= [data['Ecell/V'][i]>data['Ecell/V'][i-1] and data['Ecell/V'][i] > data['Ecell/V'][i+1] for i in range(1, len(data)-1)]
78/94: local_maxima_indices
78/95: maximas=data[[False] + local_maxima_indices + [False]]
78/96: plt.plot(np.arange(0, len(maximas)), maximas['time/s'])
78/97: len(maximas)
78/98: plt.plot(np.arange(0, len(data)), data['control/V/mA'])
78/99: np.corrcoef(data['control/V'], data['Ecell/V'])
78/100: maximas=data.groupby('cycle number').max('Ecell/V')
78/101: maximas
78/102: plt.plot(np.arange(len(data)),data['Q discharge/mA.h'])
78/103: X=data[['control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/104: scaled_X=StandardScaler().fit_transform(X)
78/105: Y=data['Q discharge/mA.h']
78/106: scaled_Y=StandardScaler().fit_transform(Y.values.reshape(-1, 1))
78/107: model=linear_model.LinearRegression()
78/108: model.fit(scaled_X, scaled_Y)
78/109: model.coef_
78/110: plt.plot(np.arange(len(data)), model.predict(scaled_X))
78/111: plt.plot(np.arange(len(data)), scaled_Y)
78/112: plt.plot(np.arange(len(data)), data['<I>/mA'])
78/113:
currents=np.array(data['<I>/mA'])
times=np.array(data['time/s'])
78/114: integral_currents=currents[1:]*(times[1:]-times[:-1])
78/115: plt.plot(np.arange(len(integral_currents)), integral_currents)
78/116: discharging_X=scaled_X[data['control/V/mA']<-500]
78/117: discharging_Y=scaled_Y[data['control/V/mA']<-500]
78/118:
model2=linear_model.LinearRegression()
model2.fit(discharging_X, discharging_Y)
78/119: plt.plot(np.arange(len(discharging_X)), model2.predict(discharging_X))
78/120: plt.plot(np.arange(len(discharging_X)), discharging_Y)
78/121: quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X)
78/122:
model3=linear_model.LinearRegression()
model3.fit(quadratic_coeffs, scaled_Y)
78/123: plt.plot(np.arange(len(data)), model3.predict(quadratic_coeffs))
78/124: X=data[['time/s','control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/125: scaled_X=StandardScaler().fit_transform(X)
78/126: discharging_X=scaled_X[data['control/V/mA']<-500]
78/127: discharging_Y=scaled_Y[data['control/V/mA']<-500]
78/128:
model2=linear_model.LinearRegression()
model2.fit(discharging_X, discharging_Y)
78/129: plt.plot(np.arange(len(discharging_X)), model2.predict(discharging_X))
78/130: plt.plot(np.arange(len(discharging_X)), discharging_Y)
78/131: quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X)
78/132:
model3=linear_model.LinearRegression()
model3.fit(quadratic_coeffs, scaled_Y)
78/133: plt.plot(np.arange(len(data)), model3.predict(quadratic_coeffs))
78/134: quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X.drop('time/s', axis=1))
78/135: quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X)
78/136: X=data[['control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/137: scaled_X=StandardScaler().fit_transform(X)
78/138: quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X)
78/139:
model3=linear_model.LinearRegression()
model3.fit(quadratic_coeffs, scaled_Y)
78/140: plt.plot(np.arange(len(data)), model3.predict(quadratic_coeffs))
78/141: plt.plot(np.arange(len(data)), data['Q discharge/mA.h']-data['Q charge/mA.h'])
78/142: plt.plot(np.arange(len(integral_currents)), np.cumsum(integral_currents))
78/143: plt.plot(np.arange(len(integral_currents)), integral_currents)
78/144: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:],integral_currents)
78/145: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1],integral_currents)
78/146: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1],np.cumsum(integral_currents))
78/147: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:],np.cumsum(integral_currents))
78/148: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h']),integral_currents)
78/149: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:],integral_currents)
78/150: np.corrcoef((data['Q discharge/mA.h'])[1:],integral_currents)
78/151: integral_currents=currents[:-1]*(times[1:]-times[:-1])
78/152: np.corrcoef((data['Q discharge/mA.h'])[1:],integral_currents)
78/153: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:],integral_currents)
78/154: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1],integral_currents)
78/155: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][:600],np.cumsum(integral_currents)[:600])
78/156: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][:800],np.cumsum(integral_currents)[:800])
78/157: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][:855],np.cumsum(integral_currents)[:855])
78/158: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][855:1600],np.cumsum(integral_currents)[855:1600])
78/159: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][:400],np.cumsum(integral_currents)[:400])
78/160: np.corrcoef((data['Q discharge/mA.h'])[:-1][:400],np.cumsum(integral_currents)[:400])
78/161: np.corrcoef((data['Q discharge/mA.h'])[:-1][:400],np.cumsum(integral_currents)[:400])
78/162: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][:400],np.cumsum(integral_currents)[:400])
78/163: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][400:855],np.cumsum(integral_currents)[400:855])
78/164: np.corrcoef((data['Q discharge/mA.h'])[:-1][400:855],np.cumsum(integral_currents)[400:855])
78/165: currents
78/166: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][855:1600],np.cumsum(integral_currents)[855:1600])
78/167: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][855:1600],np.cumsum(integral_currents)[855:1600]-np.cumsum(integral_currents)[855])
78/168: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][855:1600],np.cumsum(integral_currents)[855:1600]-np.cumsum(integral_currents)[855])
78/169: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1][855:1600],np.cumsum(integral_currents)[855:1600])
78/170:
def my_cumulative_func(arr):
    # if the value of the array is zero, reset the cumulative function to 0, else add the value to the previous value
    result = np.array()
    for i in range(len(arr)):
        if arr[i] == 0:
            result.append(0)
        else:
            result.append(result[i-1]+arr[i])
    return np.cumsum(arr)
78/171: my_cumulative_func(integral_currents)
78/172:
def my_cumulative_func(arr):
    # if the value of the array is zero, reset the cumulative function to 0, else add the value to the previous value
    result = []
    for i in range(len(arr)):
        if arr[i] == 0:
            result.append(0)
        else:
            result.append(result[i-1]+arr[i])
    return np.array(result)
78/173: my_cumulative_func(integral_currents)
78/174:
def my_cumulative_func(arr):
    # if the value of the array is zero, reset the cumulative function to 0, else add the value to the previous value
    result = []
    for i in range(len(arr)):
        if arr[i] == 0:
            result.append(0)
        else:
            if i == 0:
                result.append(arr[i])
            else:
                result.append(result[i-1]+arr[i])
    return np.array(result)
78/175: my_cumulative_func(integral_currents)
78/176: cumulative_currents=my_cumulative_func(integral_currents)
78/177: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1],cumulative_currents)
78/178: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:],cumulative_currents)
78/179: plt.plot(np.arange(len(data)-1), cumulative_currents)
78/180: plt.plot(np.arange(len(data)-1), data['Q discharge/mA.h']-data['Q charge/mA.h'])
78/181: plt.plot(np.arange(len(data)), data['Q discharge/mA.h']-data['Q charge/mA.h'])
78/182: plt.plot(np.arange(len(data)), data['Q discharge/mA.h']-data['Q charge/mA.h'])
78/183: model4=linear_model.LinearRegression()
78/184: model_data=data['Q charge/mA.h']+pandas.Series(cumulative_currents)
78/185: model_data.head()
78/186: model_data=data['Q charge/mA.h']
78/187: model_data['cumulative_currents']=cumulative_currents
78/188: model_data.head()
78/189: len(cumulative_currents)-len(data)
78/190: model_data.head
78/191: model_data=data['Q charge/mA.h']
78/192: model_data=data[['Q charge/mA.h']]
78/193: model_data['cumulative_currents']=cumulative_currents
78/194: model_data.head
78/195: model_data['cumulative_currents']=[0]+cumulative_currents
78/196: model_data['cumulative_currents']=[0]+cumulative_currents
78/197: model_data['cumulative_currents']=[0]*len(data)
78/198: model_data.head
78/199: model_data['cumulative_currents']=[0]*len(cumulative_currents)+[0]
78/200: model_data['cumulative_currents']=[0]+cumulative_currents.tolist()
78/201: model_data.head
78/202: scaled_model_data=StandardScaler().fit_transform(model_data)
78/203: model4.fit(scaled_model_data, scaled_Y)
78/204: plt.plot(np.arange(len(data)), model4.predict(scaled_model_data))
78/205: plt.plot(np.arange(len(data)), scaled_Y)
78/206: from sklearn.metrics import mean_squared_error
78/207: mean_squared_error(scaled_Y, model4.predict(scaled_model_data))
78/208: mean_squared_error(scaled_Y, model3.predict(quadratic_coeffs))
78/209: model4.coef_
78/210: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:],cumulative_currents)
78/211: (data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:]/cumulative_currents
78/212: cumulative_currents/(data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:]
78/213: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:], cumulative_currents)
78/214: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[:-1], cumulative_currents)
78/215: np.corrcoef((data['Q discharge/mA.h']-data['Q charge/mA.h'])[1:], cumulative_currents)
78/216: scaled_model_data
78/217: model_data
78/218: scaled_model_data
78/219: scaled_Y
78/220: model5=linear_model.LinearRegression()
78/221: model5=linear_model.LinearRegression()
78/222: model5=linear_model.LinearRegression()
78/223: new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(model_data)
78/224: model5.train(new_quadratic_coeffs,scaled_Y)
78/225: model5.fit(new_quadratic_coeffs,scaled_Y)
78/226: plt.plot(scaled_Y,model5.predict(new_quadratic_coeffs))
78/227: plt.plot(scaled_Y,model4.predict(scaled_model_data))
78/228: plt.scatter(scaled_Y,model4.predict(scaled_model_data))
78/229: plt.scatter(scaled_Y,model3.predict(quadratic_coeffs))
78/230: X=data[['time/s','control/V/mA', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/231: scaled_X=StandardScaler().fit_transform(X)
78/232: quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X)
78/233:
model3=linear_model.LinearRegression()
model3.fit(quadratic_coeffs, scaled_Y)
78/234: plt.scatter(scaled_Y,model3.predict(quadratic_coeffs))
78/235: mean_squared_error(scaled_Y,model3.predict(quadratic_coeffs))
78/236: lasso_model=linear_model.LassoCV()
78/237:
X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
X['integral_current']=[0]+cumulative_currents.to_list()
78/238:
X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
X['integral_current']=[0]+cumulative_currents
78/239:
X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
X['integral_current']=[0]+list(cumulative_currents)
78/240: scaled_X=StandardScaler().fit_transform(X)
78/241: new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(scaled_X)
78/242: scaled_X=StandardScaler().fit_transform(new_quadratic_coeffs)
78/243: model3.fit(scaled_X,scaled_Y)
78/244: plt.scatter(scaled_Y,model3.predict(scaled_X))
78/245: mean_squared_error(model3.predict(scaled_X), scaled_Y)
78/246: model3.coef_
78/247: lasso_model=linear_model.LassoCV(cv=5)
78/248: lasso_model.fit(scaled_X,scaled_Y)
78/249: plt.scatter(scaled_Y,lasso_model.predict(scaled_X))
78/250: mean_squared_error(scaled_Y,lasso_model.predict(scaled_X))
78/251: lasso_model.alpha_
78/252: data.head()
78/253: data_neg_i=data[data['<I>/mA']<0]
78/254: data_neg_i['<I>/mA']=data_neg_i.rolling(2).mean()
78/255: data_neg_i['<I>/mA']=[np.NaN]+data_neg_i.rolling(2).mean()
78/256: data_neg_i['<I>/mA']=[np.NaN]+data_neg_i.rolling(2).mean().to_list()
78/257: data_neg_i['<I>/mA']=data_neg_i.rolling(2).mean()['<I>/mA']
78/258: data_neg_i
78/259: data_neg_i.dropna(inplace=True)
78/260: data_neg_i.head()
78/261: data_neg_i=data[data['<I>/mA']<0]
78/262: negative_integrals=data_neg_i['<I>/mA'][1:]*(data_neg_i['time/s'][1:]-data_neg_i['time/s'][:-1])
78/263: negative_integrals
78/264: data_neg_i.head()
78/265: negative_integrals=np.array(data_neg_i['<I>/mA'][1:])*(np.array(data_neg_i['time/s'][1:])-np.array(data_neg_i['time/s'][:-1]))
78/266: negative_integrals
78/267: np.corrcoef(negative_integrals,data_neg_i['Q discharge/mA.h'])
78/268: np.corrcoef(negative_integrals,data_neg_i['Q discharge/mA.h'][1:])
78/269: np.corrcoef(negative_integrals,data_neg_i['Q discharge/mA.h'][:-1])
78/270: data_neg_i.head()
78/271: plt.plot(negative_integrals)
78/272: data['negative_curr_integral']=[0]+list(data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1]))
78/273: neg=data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1])
78/274: neg=(data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1])
78/275: len(neg)
78/276: neg=(data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1]))
78/277: len(neg)
78/278: neg=[0]+(data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1]))
78/279: len(neg)
78/280: neg=[0]+list(data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1]))
78/281: len(neg)
78/282: neg
78/283: [[0]]+neg
78/284: [0]+list(neg)
78/285: neg
78/286: neg=[0]+list((data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1])))
78/287: neg
78/288: data['negative_curr_integral']=[0]+list((data['<I>/mA'][1:])*(np.array(data['time/s'][1:])-np.array(data['time/s'][:-1])))
78/289: data_neg_i=data[data['<I>/mA']<0]
78/290: np.corrcoef(data_neg_i['negative_curr_integral'],data_neg_i['Q discharge/mA.h'])
78/291: data_neg_i
78/292: np.corrcoef(data_neg_i['negative_curr_integral'][1:],data_neg_i['Q discharge/mA.h'][1:]-data_neg_i['Q discharge/mA.h'][:-1])
78/293: np.corrcoef(data_neg_i['negative_curr_integral'][1:],np.array(data_neg_i['Q discharge/mA.h'][1:])-np.array(data_neg_i['Q discharge/mA.h'][:-1]))
78/294: plt.plot(data_neg_i['negative_curr_integral'])
78/295: plt.plot(np.array(data_neg_i['Q discharge/mA.h'][1:])-np.array(data_neg_i['Q discharge/mA.h'][:-1]))
78/296: plt.plot(np.array(data_neg_i['Q discharge/mA.h'][1:])-np.array(data_neg_i['Q discharge/mA.h'][:-1]),data_neg_i['negative_curr_integral'])
78/297: plt.plot(np.array(data_neg_i['Q discharge/mA.h'][1:])-np.array(data_neg_i['Q discharge/mA.h'][:-1]),data_neg_i['negative_curr_integral'][1:])
78/298: plt.plot(np.array(data_neg_i['Q discharge/mA.h'][1:])-np.array(data_neg_i['Q discharge/mA.h'][:-1]),data_neg_i['negative_curr_integral'][1:]/3600)
78/299: data_neg_i
78/300: data_neg_i['negative_curr_integral']=data_neg_i['negative_curr_integral']/3600
78/301: data_neg_i
78/302: data_neg_i['Q_difference']=[0]+list(np.array(data_neg_i['Q discharge/mA.h'][1:])-np.array(data_neg_i['Q discharge/mA.h'][:-1]))
78/303: data_neg_i
78/304: np.corrcoef(data_neg_i['negative_curr_integral'][1:],data_neg_i['Q_difference'][1:])
78/305: np.corrcoef(data_neg_i['negative_curr_integral'],data_neg_i['Q_difference'])
78/306: plt.plot(data_neg_i['negative_curr_integral'])
78/307: plt.plot(data_neg_i['negative_curr_integral']+data_neg_i['Q_difference'])
78/308: data_neg_i[5:10]
78/309:
data_neg_i[10:15
           ]
78/310: data_neg_i[100:115]
78/311: data_neg_i[5000:5015]
78/312: np.argmin(data_neg_i['negative_curr_integral']+data_neg_i['Q_difference'])
78/313: data_neg_i[844]
78/314: data_neg_i
78/315: data_neg_i.index
78/316: data_neg_i.index[844]
78/317: data_neg_i[3240]
78/318: data_neg_i.index[844]
78/319: data_neg_i[3240]
78/320: len(data_neg_i)
78/321: len(3240)
78/322: data_neg_i[3240]
78/323: data_neg_i[3241]
78/324: data_neg_i.iloc[3240]
78/325: np.min(data_neg_i['negative_curr_integral']+data_neg_i['Q_difference'])
78/326: data_neg_i.iloc[844]
78/327: data_neg_i
78/328: grouped_data=data.groupby('cycle number').max('Q discharge/mA.h')
78/329: grouped_data
78/330: grouped_data=data.groupby('cycle number').max('Q discharge/mA.h')['Q discharge/mA.h']
78/331: grouped_data
78/332: np.sum(grouped_data)
78/333: np.sum(grouped_data)*3600
78/334: grouped_data=data.groupby('cycle number').max('Q discharge/mA.h')['Q discharge/mA.h']
78/335: np.sum(grouped_data)
78/336: np.sum(grouped_data)*3600
78/337:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    cumulative_currents=my_cumulative_func(X['<I>/mA'][1:]*(np.array(X['time/s'][1:])-np.array(X['time/s'][:-1])))
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    scaled_Y=data['Q discharge/mA.h']
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/338:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    cumulative_currents=my_cumulative_func(np.array(X['<I>/mA'][1:]*(np.array(X['time/s'][1:])-np.array(X['time/s'][:-1]))))
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    scaled_Y=data['Q discharge/mA.h']
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/339: data=pandas.read_csv("./train-dataset/CY_NCA25-1_1-no1.csv")
78/340: X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
78/341:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+cumulative_currents.to_list()
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaled_X=StandardScaler().fit_transform(new_quadratic_coeffs)
    scaled_Y=StandardScaler().fit_transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    # scaler=StandardScaler()
    # scaler.fit(new_quadratic_coeffs)
    # scaled_X=scaler.transform(new_quadratic_coeffs)
    # scaled_Y=data['Q discharge/mA.h']
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/342:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaled_X=StandardScaler().fit_transform(new_quadratic_coeffs)
    scaled_Y=StandardScaler().fit_transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    # scaler=StandardScaler()
    # scaler.fit(new_quadratic_coeffs)
    # scaled_X=scaler.transform(new_quadratic_coeffs)
    # scaled_Y=data['Q discharge/mA.h']
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/343:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    scaled_Y=StandardScaler().fit_transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    # scaler=StandardScaler()
    # scaler.fit(new_quadratic_coeffs)
    # scaled_X=scaler.transform(new_quadratic_coeffs)
    # scaled_Y=data['Q discharge/mA.h']
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/344:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    scaled_Y=StandardScaler().fit_transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
78/345:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    scaled_Y=data['Q discharge/mA.h']
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/346:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    scaled_X_test=scaler.transform(X_test)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    with open(f"{filename[:-4]}_result.csv", "w") as file:
        file.write(prediction)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/347:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(X_test)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    with open(f"{filename[:-4]}_result.csv", "w") as file:
        file.write(prediction)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/348:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    with open(f"{filename[:-4]}_result.csv", "w") as file:
        file.write(prediction)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/349:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    result=pandas.DataFrame(prediction)
    result.to_csv(f"{filename[:-4]}_result.csv", index=True)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/350: result_data=pandas.read_csv("/home/aditya/Aditya/IIT Bombay/Freshman Year/Energius/train-dataset/CY_NCA45-05_1-no25_result.csv")
78/351: plt.plot(result_data)
78/352: result_data.head()
78/353:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    result=pandas.DataFrame(prediction,columns=['Q discharge/mA.h'])
    result.to_csv(f"{filename[:-4]}_result.csv", index=True)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/354: result_data.head()
78/355: result_data=pandas.read_csv("/home/aditya/Aditya/IIT Bombay/Freshman Year/Energius/train-dataset/CY_NCA45-05_1-no25_result.csv")
78/356: result_data.head()
78/357: plt.plot(result['Q discharge/mA.h'])
78/358: plt.plot(result['Q discharge/mA.h'][:1000000])
78/359: plt.plot(result['Q discharge/mA.h'][:100000])
78/360: plt.plot(result['Q discharge/mA.h'][:10000])
78/361: plt.plot(result['Q discharge/mA.h'][:1000000])
78/362: plt.plot(result['Q discharge/mA.h'][:100000])
78/363: data=pandas.read_csv("/home/aditya/Aditya/IIT Bombay/Freshman Year/Energius/train-dataset/CY_NCA45-05_1-no25.csv")
78/364: plt.plot(data['Q discharge/mA.h'])
78/365: plt.plot(data['Q discharge/mA.h'][:1000000])
78/366: plt.plot(data['Q discharge/mA.h'][:100000])
78/367:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    indices=np.arange(1,len(prediction)+1)
    result=pandas.DataFrame({'index':indices,'Q discharge/mA.h':prediction})
    result.to_csv(f"{filename[:-4]}_result.csv", index=False)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/368: plt.plot(data['Q discharge/mA.h'][:10000])
78/369: plt.plot(result['Q discharge/mA.h'][:10000])
78/370:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    indices=np.arange(1,len(prediction)+1)
    result=pandas.DataFrame({'Q discharge/mA.h':prediction},index=indices)
    result.to_csv(f"{filename[:-4]}_result.csv")
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/371:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    indices=np.arange(1,len(prediction)+1)
    result=pandas.DataFrame({'Q discharge/mA.h':prediction.flatten()},index=indices)
    result.to_csv(f"{filename[:-4]}_result.csv")
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/372:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    indices=np.arange(1,len(prediction)+1)
    result=pandas.DataFrame({'index':indices,'Q discharge/mA.h':prediction.flatten()},index=np.arange(len(prediction)))
    result.to_csv(f"{filename[:-4]}_result.csv",index=False)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
78/373: result_data=pandas.read_csv("/home/aditya/Aditya/IIT Bombay/Freshman Year/Energius/train-dataset/CY_NCA45-05_1-no25_result.csv")
78/374: result_data.head()
78/375: plt.plot(result['Q discharge/mA.h'][:10000])
78/376: plt.plot(result['Q discharge/mA.h'][:100000])
78/377: plt.plot(data['Q discharge/mA.h'][:100000])
78/378: plt.plot(result['Q discharge/mA.h'][:1000000])
78/379: plt.plot(result['Q discharge/mA.h'][:10000000])
78/380: plt.plot(result['Q discharge/mA.h'][:1000000])
78/381: from sklearn.model_selection import train_test_split
78/382:
y=data['Q discharge/mA.h']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
78/383:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X,y,X_test,y_test=train_test_split(data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']], data['Q discharge/mA.h'], test_size=0.3, random_state=42)
    # X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(y.values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(y.values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    scaled_Y_test=y_scaler.transform(y_test)
    print(mean_squared_error(prediction,scaled_Y_test))
    prediction=y_scaler.inverse_transform(prediction)
    print(mean_squared_error(prediction,y_test))
    break
78/384:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X,X_test,y_train,y_test=train_test_split(data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']], data['Q discharge/mA.h'], test_size=0.3, random_state=42)
    # X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(y.values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(y.values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    scaled_Y_test=y_scaler.transform(y_test)
    print(mean_squared_error(prediction,scaled_Y_test))
    prediction=y_scaler.inverse_transform(prediction)
    print(mean_squared_error(prediction,y_test))
    break
78/385:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X,X_test,y,y_test=train_test_split(data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']], data['Q discharge/mA.h'], test_size=0.3, random_state=42)
    # X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(y.values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(y.values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    scaled_Y_test=y_scaler.transform(y_test)
    print(mean_squared_error(prediction,scaled_Y_test))
    prediction=y_scaler.inverse_transform(prediction)
    print(mean_squared_error(prediction,y_test))
    break
78/386:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X,X_test,y,y_test=train_test_split(data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']], data['Q discharge/mA.h'], test_size=0.3, random_state=42)
    # X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(y.values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(y.values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    scaled_Y_test=y_scaler.transform(y_test.values.reshape(-1,1))
    print(mean_squared_error(prediction,scaled_Y_test))
    prediction=y_scaler.inverse_transform(prediction)
    print(mean_squared_error(prediction,y_test))
    break
78/387: from sklearn.metrics import mean_squared_error, r2_score
78/388:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X,X_test,y,y_test=train_test_split(data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']], data['Q discharge/mA.h'], test_size=0.3, random_state=42)
    # X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(y.values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(y.values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    scaled_Y_test=y_scaler.transform(y_test.values.reshape(-1,1))
    print(r2_score(prediction,scaled_Y_test))
    prediction=y_scaler.inverse_transform(prediction)
    print(r2_score(prediction,y_test))
    break
78/389:
for filename in ["./train-dataset/CY_NCA45-05_1-no25.csv","./train-dataset/CY_NCA25-025_1-no4.csv","./train-dataset/CY_NCM35-05_1-no3.csv","./train-dataset/CY_NCA45-05_1-no9.csv","./train-dataset/CY_NCA45-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no13.csv","./train-dataset/CY_NCA45-05_1-no21.csv","./train-dataset/CY_NCA45-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no10.csv","./train-dataset/CY_NCA45-05_1-no22.csv","./train-dataset/CY_NCA45-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no27.csv","./train-dataset/CY_NCA25-1_1-no4.csv","./train-dataset/CY_NCA45-05_1-no15.csv","./train-dataset/CY_NCA25-05_1-no11.csv","./train-dataset/CY_NCA45-05_1-no3.csv","./train-dataset/CY_NCA25-1_1-no2.csv","./train-dataset/CY_NCM35-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no24.csv","./train-dataset/CY_NCM25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no14.csv","./train-dataset/CY_NCA25-05_1-no5.csv","./train-dataset/CY_NCA45-05_1-no16.csv","./train-dataset/CY_NCM45-05_1-no24.csv","./train-dataset/CY_NCA25-1_1-no9.csv","./train-dataset/CY_NCA25-025_1-no3.csv","./train-dataset/CY_NCA25-025_1-no6.csv","./train-dataset/CY_NCA35-05_1-no3.csv","./train-dataset/CY_NCA25-025_1-no5.csv","./train-dataset/CY_NCA25-05_1-no9.csv","./train-dataset/CY_NCA25-05_1-no7.csv","./train-dataset/CY_NCA45-05_1-no12.csv","./train-dataset/CY_NCA45-05_1-no18.csv","./train-dataset/CY_NCA35-05_1-no1.csv","./train-dataset/CY_NCM25-05_1-no1.csv","./train-dataset/CY_NCA45-05_1-no17.csv","./train-dataset/CY_NCA25-05_1-no14.csv","./train-dataset/CY_NCA25-1_1-no7.csv","./train-dataset/CY_NCA45-05_1-no13.csv","./train-dataset/CY_NCA25-1_1-no8.csv","./train-dataset/CY_NCA25-05_1-no18.csv","./train-dataset/CY_NCA25-1_1-no5.csv","./train-dataset/CY_NCA25-1_1-no1.csv","./train-dataset/CY_NCA35-05_1-no2.csv","./train-dataset/CY_NCA25-05_1-no3.csv","./train-dataset/CY_NCA25-05_1-no6.csv","./train-dataset/CY_NCA25-025_1-no7.csv","./train-dataset/CY_NCA25-05_1-no16.csv","./train-dataset/CY_NCA25-1_1-no6.csv","./train-dataset/CY_NCA25-05_1-no12.csv","./train-dataset/CY_NCA25-05_1-no19.csv","./train-dataset/CY_NCA45-05_1-no4.csv","./train-dataset/CY_NCA45-05_1-no6.csv","./train-dataset/CY_NCA45-05_1-no26.csv","./train-dataset/CY_NCA45-05_1-no5.csv","./train-dataset/CY_NCA25-05_1-no4.csv","./train-dataset/CY_NCA25-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no23.csv","./train-dataset/CY_NCA25-05_1-no15.csv","./train-dataset/CY_NCA25-025_1-no2.csv","./train-dataset/CY_NCA45-05_1-no2.csv","./train-dataset/CY_NCA45-05_1-no20.csv","./train-dataset/CY_NCA25-05_1-no1.csv","./train-dataset/CY_NCA25-05_1-no8.csv","./train-dataset/CY_NCA25-05_1-no17.csv","./train-dataset/CY_NCA25-025_1-no1.csv","./train-dataset/CY_NCA45-05_1-no19.csv","./train-dataset/CY_NCA25-1_1-no3.csv","./train-dataset/CY_NCA45-05_1-no10.csv","./train-dataset/CY_NCM45-05_1-no25.csv","./train-dataset/CY_NCA45-05_1-no28.csv"]:
    data=pandas.read_csv(filename)
    X=data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X['<I>/mA'])
    times=np.array(X['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X['integral_current']=[0]+list(cumulative_currents)
    new_quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X)
    scaler=StandardScaler()
    scaler.fit(new_quadratic_coeffs)
    scaled_X=scaler.transform(new_quadratic_coeffs)
    y_scaler=StandardScaler().fit(data['Q discharge/mA.h'].values.reshape(-1, 1))
    scaled_Y=y_scaler.transform(data['Q discharge/mA.h'].values.reshape(-1, 1))
    model=linear_model.LinearRegression()
    model.fit(scaled_X,scaled_Y)
    test_data=pandas.read_csv("test.csv")
    X_test=test_data[['time/s', 'Ecell/V', '<I>/mA', 'Q charge/mA.h', 'control/V', 'control/mA', 'cycle number']]
    currents=np.array(X_test['<I>/mA'])
    times=np.array(X_test['time/s'])
    integral_currents=currents[:-1]*(times[1:]-times[:-1])
    cumulative_currents=my_cumulative_func(integral_currents)
    X_test['integral_current']=[0]+list(cumulative_currents)
    quadratic_coeffs=PolynomialFeatures(degree=2).fit_transform(X_test)
    scaled_X_test=scaler.transform(quadratic_coeffs)
    prediction=model.predict(scaled_X_test)
    prediction=y_scaler.inverse_transform(prediction)
    indices=np.arange(len(prediction))
    result=pandas.DataFrame({'index':indices,'Q discharge/mA.h':prediction.flatten()},index=np.arange(len(prediction)))
    result.to_csv(f"{filename[:-4]}_result.csv",index=False)
    print(mean_squared_error(scaled_Y,model.predict(scaled_X)))
    break
79/1:
import matplotlib.pyplot as plt

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=2, ncols=2)

# Flatten the axes array to iterate over subplots
axes = axes.flatten()

# Plot on each subplot
for i, ax in enumerate(axes):
    ax.plot([i, i+1, i+2], [i+3, i+4, i+5])
    ax.set_title(f'Subplot {i+1}')

# Adjust layout to prevent overlap of titles
plt.tight_layout()

# Show the plot
plt.show()
79/2:
import matplotlib.pyplot as plt

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=2, ncols=2)

print(axes)
# Flatten the axes array to iterate over subplots
axes = axes.flatten()

# Plot on each subplot
for i, ax in enumerate(axes):
    ax.plot([i, i+1, i+2], [i+3, i+4, i+5])
    ax.set_title(f'Subplot {i+1}')

# Adjust layout to prevent overlap of titles
plt.tight_layout()

# Show the plot
plt.show()
79/3:
import matplotlib.pyplot as plt

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=2, ncols=2)

# Flatten the axes array to iterate over subplots
axes = axes.flatten()

# Plot on each subplot
for i, ax in enumerate(axes):
    ax.plot([i, i+1, i+2], [i+3, i+4, i+5])
    ax.set_title(f'Subplot {i+1}')

# Adjust layout to prevent overlap of titles
plt.tight_layout()

# Show the plot
plt.show()
79/4:
import matplotlib.pyplot as plt

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=2, ncols=2)

# Flatten the axes array to iterate over subplots
# axes = axes.flatten()

# Plot on each subplot
for i, ax in enumerate(axes):
    ax.plot([i, i+1, i+2], [i+3, i+4, i+5])
    ax.set_title(f'Subplot {i+1}')

# Adjust layout to prevent overlap of titles
plt.tight_layout()

# Show the plot
plt.show()
79/5:
import matplotlib.pyplot as plt

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=2, ncols=2)

# Flatten the axes array to iterate over subplots
axes = axes.flatten()

# Plot on each subplot
for i, ax in enumerate(axes):
    ax.plot([i, i+1, i+2], [i+3, i+4, i+5])
    ax.set_title(f'Subplot {i+1}')

# Adjust layout to prevent overlap of titles
plt.tight_layout()

# Show the plot
plt.show()
79/6:
import matplotlib.pyplot as plt

# Sample data
x = [1, 2, 3, 4, 5]
y = [2, 3, 5, 7, 11]

# Plot
plt.plot(x, y)
plt.xlabel('X Label')
plt.ylabel('Y Label')
plt.title('Plot with Custom Gridlines')

# Customize horizontal gridlines
plt.grid(True, axis='y', linestyle='--', linewidth=0.5, color='gray')

# Customize vertical gridlines
plt.grid(True, axis='x', linestyle=':', linewidth=0.5, color='blue')

plt.show()
79/7:
import matplotlib.pyplot as plt

# Sample data
x = [1, 2, 3, 4, 5]
y = [10, 100, 1000, 10000, 100000]

# Create a plot
plt.plot(x, y)

# Set logarithmic scale for both x and y axes
plt.xscale('log')
plt.yscale('log')

# Add labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Logarithmic Scale Plot')

# Show plot
plt.show()
79/8:
import matplotlib.pyplot as plt

# Sample data
x = [1, 2, 3, 4, 5]
y = [10, 100, 1000, 10000, 100000]

# Create a plot
plt.plot(x, y)

# Set logarithmic scale for both x and y axes
plt.xscale('log')
plt.yscale('log')

# Add labels and title
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Logarithmic Scale Plot')

# Show plot
plt.show()
79/9:
import matplotlib.pyplot as plt

plt.plot([1, 2, 3], [4, 5, 6])
plt.title('Plot Title')
plt.show()
79/10:
import matplotlib.pyplot as plt

plt.plot([1, 2, 3], [4, 5, 6])
plt.set_title('Updated Title')
plt.show()
79/11:
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.plot([1, 2, 3], [4, 5, 6])
ax.set_title('Subplot Title')
plt.show()
79/12:
import matplotlib.pyplot as plt

fig, ax = plt.subplots(nrows=2,ncols=2)
ax.plot([1, 2, 3], [4, 5, 6])
ax[0,0].set_title('Subplot Title')
plt.show()
79/13:
import matplotlib.pyplot as plt

fig, ax = plt.subplots(nrows=2,ncols=2)
ax[0,0].plot([1, 2, 3], [4, 5, 6])
ax[0,0].set_title('Subplot Title')
plt.show()
79/14:
import matplotlib.pyplot as plt

fig, ax = plt.subplots(nrows=2,ncols=2)
ax[0,0].plot([1, 2, 3], [4, 5, 6])
ax[0,0].set_title('Subplot Title')
plt.show()
79/15:
import matplotlib.pyplot as plt

fig = plt.figure()
axes = fig.subplots(nrows=2, ncols=2)

plt.show()
79/16:
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(16,12))
axes = fig.subplots(nrows=2, ncols=2)

plt.show()
79/17:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(m),(m)->(m)')

# Apply the vectorized function to arrays
x = np.array([1, 2, 3, 4])
y = np.array([2, 3, 4, 5, 6])  # Different length array
result = vectorized_func(x, y)
print(result)
79/18:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(m),(n)->(m)')

# Apply the vectorized function to arrays
x = np.array([1, 2, 3, 4])
y = np.array([2, 3, 4, 5, 6])  # Different length array
result = vectorized_func(x, y)
print(result)
79/19:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(m),(n)->(m)')

# Apply the vectorized function to arrays
x = np.array([1, 2, 3, 4])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/20:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(m),(m)->(m)')

# Apply the vectorized function to arrays
x = np.array([1, 2, 3, 4])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/21:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(m),(m)->(m)')

# Apply the vectorized function to arrays
x = np.array([1])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/22:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(n),(m)->(m)')

# Apply the vectorized function to arrays
x = np.array([1])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/23:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(n),(m)->(n)')

# Apply the vectorized function to arrays
x = np.array([1])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/24:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(n),(m)->(m)')

# Apply the vectorized function to arrays
x = np.array([1])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/25:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(n,),(m)->(m)')

# Apply the vectorized function to arrays
x = np.array([1])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
79/26:
import numpy as np

# Define a scalar function with two array inputs and output
def custom_operation(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    return x ** 2 * y

# Create a vectorized version of the function with specified signature
vectorized_func = np.vectorize(custom_operation, signature='(n,),(m,)->(m,)')

# Apply the vectorized function to arrays
x = np.array([1])
y = np.array([2, 3, 4, 5])  # Different length array
result = vectorized_func(x, y)
print(result)
81/1:
import json
with open('cs_projects.json') as f:
    lines = json.load(f)
81/2: print(lines)
81/3:
for line in lines:
    print line.get('name')
81/4:
for line in lines:
    print(line.get('name'))
81/5:
for line in lines:
    print(line.get('title'))
81/6: len(lines)
81/7: print(lines)
82/1:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('data/ipl.csv')
X = dataset.iloc[:,[7,8,9,12,13,14]].values
y = dataset.iloc[:, 15].values


# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)


# Training the dataset
from sklearn.linear_model import LogisticRegression
lin = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')
lin.fit(X_train,y_train)

# Testing the dataset on trained model
y_pred = lin.predict(X_test)
score = lin.score(X_test,y_test)*100
print("R square value:" , score)
82/2: import numpy as np
83/1:
new_prediction = lin.predict_proba(sc.transform(np.array([[100,2,10,23,52,201]])))
print("Prediction score:" , new_prediction[0][1]*100)
83/2:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('data/ipl.csv')
X = dataset.iloc[:,[7,8,9,12,13,14]].values
y = dataset.iloc[:, 15].values


# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)


# Training the dataset
from sklearn.linear_model import LogisticRegression
lin = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')
lin.fit(X_train,y_train)

# Testing the dataset on trained model
y_pred = lin.predict(X_test)
score = lin.score(X_test,y_test)*100
print("R square value:" , score)
83/3: import numpy as np
83/4:
new_prediction = lin.predict_proba(sc.transform(np.array([[100,2,10,23,52,201]])))
print("Prediction score:" , new_prediction[0][1]*100)
83/5:
import json
with open("data.json", "r") as file:
    data=json.load(file)
    data=data["innings"][1]
83/6: print(data)
83/7:
import json
with open("data.json", "r") as file:
    data=json.load(file)
    data=data["innings"][1]['overs']
83/8: print(data)
83/9:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
for over in data:
    ball_num+=1
    for ball in over['deliveries']:
        if players_map.get(ball["batter"]) is None:
            players_map[ball["batter"]]=player_num
            player_num+=1
        if players_map.get(ball["non_striker"]) is None:
            players_map[ball["non_striker"]]=player_num
            player_num+=1
        total_score+=ball["runs"]["total"]
        if ball.get('wickets') is not None:
            total_wickets+=len(ball['wickets'])
        new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
        break
    break
83/10: print new_prediction
83/11: print("Prediction score:" , new_prediction[0][1]*100)
83/12:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
for over in data[::-1]:
    ball_num+=1
    for ball in over['deliveries']:
        if players_map.get(ball["batter"]) is None:
            players_map[ball["batter"]]=player_num
            player_num+=1
        if players_map.get(ball["non_striker"]) is None:
            players_map[ball["non_striker"]]=player_num
            player_num+=1
        total_score+=ball["runs"]["total"]
        if ball.get('wickets') is not None:
            total_wickets+=len(ball['wickets'])
        new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
        break
    break
83/13: print("Prediction score:" , new_prediction[0][1]*100)
83/14: from matplotlib import pyplot as plt
83/15:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for over in data[::-1]:
    ball_num+=1
    for ball in over['deliveries']:
        if players_map.get(ball["batter"]) is None:
            players_map[ball["batter"]]=player_num
            player_num+=1
        if players_map.get(ball["non_striker"]) is None:
            players_map[ball["non_striker"]]=player_num
            player_num+=1
        total_score+=ball["runs"]["total"]
        if ball.get('wickets') is not None:
            total_wickets+=len(ball['wickets'])
        new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
        predictions.append(new_prediction[0][1]*100)
83/16: from matplotlib import pyplot as plt
83/17: plt.plot(predictions)
83/18: predictions
83/19: plt.scatter(predictions)
83/20: plt.scatter(predictions, np.arange(1,len(predictions)+1))
83/21: plt.scatter(np.arange(1,len(predictions)+1), predictions)
83/22: stats=pd.read_csv("stats.csv")
83/23: stats
83/24: stats=pd.read_csv("stats.csv", header=None)
83/25: stats
83/26:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for ball in stats:
    print(ball.get("Striker"))
    # if players_map.get(ball["Striker"]) is None:
    #     players_map[ball["Striker"]]=player_num
    #     player_num+=1
    # if players_map.get(ball["Non-striker"]) is None:
    #     players_map[ball["Non-striker"]]=player_num
    #     player_num+=1
    # new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["Striker"]],players_map[ball["Non-striker"]],145]])))
    # predictions.append(new_prediction[0][1]*100)
# for over in data[::-1]:
#     ball_num+=1
#     for ball in over['deliveries']:
#         if players_map.get(ball["batter"]) is None:
#             players_map[ball["batter"]]=player_num
#             player_num+=1
#         if players_map.get(ball["non_striker"]) is None:
#             players_map[ball["non_striker"]]=player_num
#             player_num+=1
#         total_score+=ball["runs"]["total"]
#         if ball.get('wickets') is not None:
#             total_wickets+=len(ball['wickets'])
#         new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
#         predictions.append(new_prediction[0][1]*100)
83/27:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for ball, value in stats.iterrows():
    print(ball.get("Striker"))
    # if players_map.get(ball["Striker"]) is None:
    #     players_map[ball["Striker"]]=player_num
    #     player_num+=1
    # if players_map.get(ball["Non-striker"]) is None:
    #     players_map[ball["Non-striker"]]=player_num
    #     player_num+=1
    # new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["Striker"]],players_map[ball["Non-striker"]],145]])))
    # predictions.append(new_prediction[0][1]*100)
# for over in data[::-1]:
#     ball_num+=1
#     for ball in over['deliveries']:
#         if players_map.get(ball["batter"]) is None:
#             players_map[ball["batter"]]=player_num
#             player_num+=1
#         if players_map.get(ball["non_striker"]) is None:
#             players_map[ball["non_striker"]]=player_num
#             player_num+=1
#         total_score+=ball["runs"]["total"]
#         if ball.get('wickets') is not None:
#             total_wickets+=len(ball['wickets'])
#         new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
#         predictions.append(new_prediction[0][1]*100)
83/28:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for ball, value in stats.iterrows():
    print(value.get("Striker"))
    # if players_map.get(ball["Striker"]) is None:
    #     players_map[ball["Striker"]]=player_num
    #     player_num+=1
    # if players_map.get(ball["Non-striker"]) is None:
    #     players_map[ball["Non-striker"]]=player_num
    #     player_num+=1
    # new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["Striker"]],players_map[ball["Non-striker"]],145]])))
    # predictions.append(new_prediction[0][1]*100)
# for over in data[::-1]:
#     ball_num+=1
#     for ball in over['deliveries']:
#         if players_map.get(ball["batter"]) is None:
#             players_map[ball["batter"]]=player_num
#             player_num+=1
#         if players_map.get(ball["non_striker"]) is None:
#             players_map[ball["non_striker"]]=player_num
#             player_num+=1
#         total_score+=ball["runs"]["total"]
#         if ball.get('wickets') is not None:
#             total_wickets+=len(ball['wickets'])
#         new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
#         predictions.append(new_prediction[0][1]*100)
83/29:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for ball, value in stats.iterrows():
    print(value)
    # if players_map.get(ball["Striker"]) is None:
    #     players_map[ball["Striker"]]=player_num
    #     player_num+=1
    # if players_map.get(ball["Non-striker"]) is None:
    #     players_map[ball["Non-striker"]]=player_num
    #     player_num+=1
    # new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["Striker"]],players_map[ball["Non-striker"]],145]])))
    # predictions.append(new_prediction[0][1]*100)
# for over in data[::-1]:
#     ball_num+=1
#     for ball in over['deliveries']:
#         if players_map.get(ball["batter"]) is None:
#             players_map[ball["batter"]]=player_num
#             player_num+=1
#         if players_map.get(ball["non_striker"]) is None:
#             players_map[ball["non_striker"]]=player_num
#             player_num+=1
#         total_score+=ball["runs"]["total"]
#         if ball.get('wickets') is not None:
#             total_wickets+=len(ball['wickets'])
#         new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
#         predictions.append(new_prediction[0][1]*100)
83/30: stats=pd.read_csv("stats.csv", header=["Runs", "Striker", "Non-striker", "Wickets", "Total"])
83/31: stats=pd.read_csv("stats.csv")
83/32:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for ball, value in stats.iterrows():
    print(value)
    # if players_map.get(ball["Striker"]) is None:
    #     players_map[ball["Striker"]]=player_num
    #     player_num+=1
    # if players_map.get(ball["Non-striker"]) is None:
    #     players_map[ball["Non-striker"]]=player_num
    #     player_num+=1
    # new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["Striker"]],players_map[ball["Non-striker"]],145]])))
    # predictions.append(new_prediction[0][1]*100)
# for over in data[::-1]:
#     ball_num+=1
#     for ball in over['deliveries']:
#         if players_map.get(ball["batter"]) is None:
#             players_map[ball["batter"]]=player_num
#             player_num+=1
#         if players_map.get(ball["non_striker"]) is None:
#             players_map[ball["non_striker"]]=player_num
#             player_num+=1
#         total_score+=ball["runs"]["total"]
#         if ball.get('wickets') is not None:
#             total_wickets+=len(ball['wickets'])
#         new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
#         predictions.append(new_prediction[0][1]*100)
83/33: stats
83/34:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
predictions=[]
for ball, value in stats.iterrows():
    if players_map.get(value["Striker"]) is None:
        players_map[value["Striker"]]=player_num
        player_num+=1
    if players_map.get(value["Non-striker"]) is None:
        players_map[value["Non-striker"]]=player_num
        player_num+=1
    new_prediction = lin.predict_proba(sc.transform(np.array([[value["Total"],value["Wickets"],float(f"{ball//6}.{ball%6}"),players_map[value["Striker"]],players_map[value["Non-striker"]],201]])))
    predictions.append(new_prediction[0][1]*100)
# for over in data[::-1]:
#     ball_num+=1
#     for ball in over['deliveries']:
#         if players_map.get(ball["batter"]) is None:
#             players_map[ball["batter"]]=player_num
#             player_num+=1
#         if players_map.get(ball["non_striker"]) is None:
#             players_map[ball["non_striker"]]=player_num
#             player_num+=1
#         total_score+=ball["runs"]["total"]
#         if ball.get('wickets') is not None:
#             total_wickets+=len(ball['wickets'])
#         new_prediction = lin.predict_proba(sc.transform(np.array([[total_score,total_wickets,float(f"{ball_num//6}.{ball_num%6}"),players_map[ball["batter"]],players_map[ball["non_striker"]],145]])))
#         predictions.append(new_prediction[0][1]*100)
83/35: predictions
83/36: plt.plot(np.arange(1,len(predictions)+1), predictions)
83/37:
for i in range(0,len(predictions,6)):
    print(f"Over {i//6}: {predictions[i]}")
83/38:
for i in range(0,len(predictions), 6):
    print(f"Over {i//6}: {predictions[i]}")
83/39:
for i in range(0,len(predictions), 6):
    print(f"Over {i//6}: {predictions[i+6]}")
83/40:
for i in range(6,len(predictions), 6):
    print(f"Over {i//6}: {predictions[i-1]}")
83/41: values=[11.4, 10.32, 5.66, 8.02, 5.88, 14.51, 9.32, 17.33, 11.89, 4.5, 1.94, 5.25, 3.14, 1.06, 0.28, 0.15, 0.14, 0.22, 0.56]
83/42: plt.plot(np.arange(1,len(predictions)+1), predictions, range(6,len(predictions),6), values)
83/43:
total_score=0
total_wickets=0
players_map={}
player_num=1
ball_num=0
new_predictions=[]
for ball, value in stats.iterrows():
    if players_map.get(value["Striker"]) is None:
        players_map[value["Striker"]]=player_num
        player_num+=1
    if players_map.get(value["Non-striker"]) is None:
        players_map[value["Non-striker"]]=player_num
        player_num+=1
    new_prediction = lin.predict_proba(sc.transform(np.array([[value["Total"],value["Wickets"],float(f"{ball//6}.{ball%6}"),players_map[value["Striker"]],players_map[value["Non-striker"]],219]])))
    new_predictions.append(new_prediction[0][1]*100)
83/44: plt.plot(np.arange(1,len(new_predictions)+1), new_predictions, range(6,len(predictions),6), values)
83/45: plt.plot(range(6,len(predictions),6), values)
83/46: plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/47:
plt.style('ggplot')
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/48:
plt.style.use('ggplot')
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/49:
plt.style.use('ggplot')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/50:
plt.style.use('solarize_light2')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/51:
plt.style.use('Solarize-Light2')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/52: plt.style.available
83/53:
plt.style.use('Solarize_Light2')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/54:
plt.style.use('seaborn-v0_8-deep')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/55:
plt.style.use('fivethirtyeight')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/56:
plt.style.use('ggplot')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/57:
plt.style.use('seaborn-v0_8-poster')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/58:
plt.style.use('_mpl-gallery')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/59:
plt.style.use('grayscale')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/60:
plt.style.use('seaborn-v0_8-muted')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/61:
plt.style.use('seaborn-v0_8-colorblind')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/62:
plt.style.use('fast')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/63:
plt.style.use('ggplot')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/64:
plt.style.use('Solarize_Light2')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/65:
plt.style.use('fast')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/66:
plt.style.use('Solarize_Light2')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/67:
plt.style.use('Solarize_Light2')
plt.figure.figsize(16,12)
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/68:
plt.style.use('ggplot')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/69:
plt.style.use('fivethirtyeight')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/70:
plt.style.use('fivethirtyeight')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/71:
plt.style.use('fivethirtyeight')
plt.figure(16,12)
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/72:
plt.style.use('fivethirtyeight')
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/73:
plt.style.use('fivethirtyeight')
plt.figsize(16, 12)
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/74:
plt.style.use('fivethirtyeight')
plt.figure(figsize=(16,12))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/75:
plt.style.use('fivethirtyeight')
plt.figure(figsize=(12,10))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/76:
plt.style.use('fivethirtyeight')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/77:
plt.style.use('classic')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/78:
plt.style.use('seaborn-v0_8-darkgrid')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/79:
plt.style.use('dark_background')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/80:
plt.style.use('_classic_test_patch')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/81:

plt.style.use('_mpl-gallery')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/82:

plt.style.use('seaborn-v0_8')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/83:

plt.style.use('ggplot')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/84:

plt.style.use('seaborn-v0_8-muted')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/85:

plt.style.use('Solarize_Light2')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/86:

plt.style.use('seaborn-v0_8-deep')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/87:

plt.style.use('fast')
plt.figure(figsize=(8,5))
plt.title("RCB's Probability of Qualifying")
plt.xlabel("Balls")
plt.ylabel("Probability")
plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/88:
for styling in plt.style.available:
    plt.style.use(f'{style}')
    plt.figure(figsize=(8,5))
    plt.title("RCB's Probability of Qualifying")
    plt.xlabel("Balls")
    plt.ylabel("Probability")
    plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/89:
for styling in plt.style.available:
    plt.style.use(f'{styling}')
    plt.figure(figsize=(8,5))
    plt.title("RCB's Probability of Qualifying")
    plt.xlabel("Balls")
    plt.ylabel("Probability")
    plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='red', label="RCB's Probability of qualifying")
83/90:
for styling in plt.style.available:
    plt.style.use(f'{styling}')
    plt.figure(figsize=(8,5))
    plt.title("RCB's Probability of Qualifying")
    plt.xlabel("Balls")
    plt.ylabel("Probability")
    plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/91:
for styling in plt.style.available:
    plt.style.use(f'{styling}')
    plt.title("RCB's Probability of Qualifying")
    plt.xlabel("Balls")
    plt.ylabel("Probability")
    plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
83/92:
for styling in plt.style.available:
    plt.style.use(f'{styling}')
    plt.figure(figsize=(8,5))
    plt.title("RCB's Probability of Qualifying")
    plt.xlabel("Balls")
    plt.ylabel("Probability")
    plt.plot(np.arange(1,len(new_predictions)+1), [100-x for x in new_predictions], color='orange', label="RCB's Probability of qualifying")
85/1:
# mdp = {
#    state: action: [(probability, next_state, reward, is_done)]
# }
85/2: mdp={}
85/3: string='BBBB........WWWW'
85/4:
n=4
def find_possible_positions(state, player):
    global n
    result=[]
    if player==1:
        for i in range(n*n):
            if state[i]=="W":
                if i-4 < 0:
                    continue
                if i-4 >= 0 and state[i-4]==".":
                    s=state
                    s[i]="."
                    s[i-4]="W"
                    result.append(s)
                if (i-3)//4 == (i-4)//4 and state[i-3]=="B":
                    s=state
                    s[i]="."
                    s[i-3]="W"
                    result.append(s)
                if (i-5)//4 == (i-4)//4 and state[i-5]=="B":
                    s=state
                    s[i]="."
                    s[i-5]="B"
                    result.append(s)
    return result
85/5:
n=4
def find_possible_positions(state, player):
    global n
    result=[]
    if player==1:
        char1="W"
        char2="B"
        sign=-1
    else:
        char1="B"
        char2="W"
        sign=1
    for i in range(n*n):
        if state[i]==char1:
            if i+sign*4 < 0:
                continue
            if i+sign*4 >= 0 and state[i+sign*4]==".":
                s=state
                s[i]="."
                s[i+sign*4]=char1
                result.append(s)
            if (i+sign*3)//4 == (i+sign*4)//4 and state[i+sign*3]==char2:
                s=state
                s[i]="."
                s[i+sign*3]=char1
                result.append(s)
            if (i+sign*5)//4 == (i+sign*4)//4 and state[i+sign*5]==char2:
                s=state
                s[i]="."
                s[i-5]=char1
                result.append(s)
    return result
85/6:
n=4
def find_possible_positions(state, player):
    global n
    result=[]
    if player==1:
        char1="W"
        char2="B"
        sign=-1
    else:
        char1="B"
        char2="W"
        sign=1
    for i in range(n*n):
        if state[i]==char1:
            if i+sign*n < 0:
                continue
            if i+sign*n >= 0 and state[i+sign*n]==".":
                s=state
                s[i]="."
                s[i+sign*n]=char1
                result.append(s)
            if (i+sign*(n-1))//n == (i+sign*n)//n and state[i+sign*(n-1)]==char2:
                s=state
                s[i]="."
                s[i+sign*(n-1)]=char1
                result.append(s)
            if (i+sign*(n+1))//n == (i+sign*n)//n and state[i+sign*(n+1)]==char2:
                s=state
                s[i]="."
                s[i+sign*(n+1)]=char1
                result.append(s)
    return result
85/7:
from random import random
import numpy as np
85/8:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random<epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
85/9:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random<epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=(mdp1[states[i]][states[i+1]][0]+reward, mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=(mdp2[states[i]][states[i+1]][0]+reward, mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/10: game()
85/11:
mdp1={}
mdp2={}
85/12: game()
85/13:
n=4
def find_possible_positions(state, player):
    global n
    result=[]
    if player==1:
        char1="W"
        char2="B"
        sign=-1
    else:
        char1="B"
        char2="W"
        sign=1
    for i in range(n*n):
        if state[i]==char1:
            if i+sign*n < 0:
                continue
            if i+sign*n >= 0 and state[i+sign*n]==".":
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*n]+char1+s[i+sign*n+1:]
                result.append(s)
            if (i+sign*(n-1))//n == (i+sign*n)//n and state[i+sign*(n-1)]==char2:
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*(n-1)]+char1+s[i+sign*(n-1)+1:]
                result.append(s)
            if (i+sign*(n+1))//n == (i+sign*n)//n and state[i+sign*(n+1)]==char2:
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*(n+1)]+char1+s[i+sign*(n+1)+1:]
                result.append(s)
    return result
85/14:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random<epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=(mdp1[states[i]][states[i+1]][0]+reward, mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=(mdp2[states[i]][states[i+1]][0]+reward, mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/15: game()
85/16:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values(), key=lambda x: x[0]/x[1])]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=(mdp1[states[i]][states[i+1]][0]+reward, mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=(mdp2[states[i]][states[i+1]][0]+reward, mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/17: game()
85/18:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]+reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/19: game()
85/20:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]+reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/21: game()
85/22:
mdp1={}
mdp2={}
85/23: find_possible_positions(string, 1)
85/24:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]+reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/25: game()
85/26:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]+reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/27: game()
85/28: mdp1
85/29:
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]+reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/30:
for i in range(10000):
    game()
85/31:
print("MDP1")
for state in mdp1:
    print(state)
    print(mdp1[state])

print("MDP2")
for state in mdp2:
    print(state)
    print(mdp2[state])
85/32:
print("MDP1")
for state in mdp1:
    print(state)
    print(list(mdp1[state].keys())[np.argmax(mdp1[state].values())])

print("MDP2")
for state in mdp2:
    print(state)
    print(list(mdp2[state].keys())[np.argmax(mdp2[state].values())])
85/33:
print("MDP1")
for state in mdp1:
    print(state)
    print(list(mdp1[state].keys())[np.argmax(mdp1[state].values())])

print("MDP2")
for state in mdp2:
    print(state, list(mdp2[state].keys())[np.argmax(mdp2[state].values())])
85/34:
print("MDP1")
for state in mdp1:
    print(state, list(mdp1[state].keys())[np.argmax(mdp1[state].values())])

print("MDP2")
for state in mdp2:
    print(state, list(mdp2[state].keys())[np.argmax(mdp2[state].values())])
85/35:
print("MDP1")
for state in mdp1:
    if len(mdp1[state])>0:
        print(state, list(mdp1[state].keys())[np.argmax(mdp1[state].values())])

print("MDP2")
for state in mdp2:
    if len(mdp2[state])>0:
        print(state, list(mdp2[state].keys())[np.argmax(mdp2[state].values())])
85/36:
rewards=[]
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    rewards.append(reward)
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]+reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/37:
for i in range(10000):
    game()
85/38: print(rewards[-100:])
85/39:
for i in range(100000000):
    game()
85/40:
mdp1={}
mdp2={}
85/41:
rewards=[]
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    rewards.append(reward)
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]-reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/42:
for i in range(100000):
    game()
85/43: print(rewards[-100:])
85/44: print(rewards[-10:])
85/45:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
85/46:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
85/47: not_practice_game()
85/48:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
85/49: not_practice_game()
85/50: mdp1
85/51:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    print("HERE")
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
85/52:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    print("HERE")
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
85/53: not_practice_game()
85/54:
rewards=[]
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
    rewards.append(reward)
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]-reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/55:
mdp1={}
mdp2={}
85/56:
for i in range(10000000):
    game()
85/57: print(rewards[-10:])
85/58: print(rewards[-100:])
85/59: not_practice_game()
85/60: mdp1["B.BB.B..W....WWW"]
85/61: mdp2["B.BB.B..WW....WW"]
85/62: mdp2["B.BB....BW....WW"]
85/63: mdp1["B.BB....BW....WW"]
85/64: mdp2["B.BB.W..B.....WW"]
85/65:
mdp1={}
mdp2={}
85/66:
rewards=[]
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]-reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/67:
for i in range(1000000):
    game()
85/68: print(rewards[-100:])
85/69: print(rewards[-100:])
85/70: not_practice_game()
85/71: mdp1["B.BB.B..W....WWW"]
85/72: mdp2["B.BB.W..B.....WW"]
85/73:
rewards=[]
def game(epsilon=0.1, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]-reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/74:
mdp1={}
mdp2={}
85/75:
for i in range(1000000):
    game()
85/76: print(rewards[-100:])
85/77: not_practice_game()
85/78: not_practice_game()
85/79: not_practice_game()
85/80: mdp2["B.BB.W..B.....WW"]
85/81:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    print("HERE")
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/82: not_practice_game()
85/83:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    print("HERE")
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/84:
for i in range(100):
    not_practice_game()
85/85:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax(mdp1[current_state].values())]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax(mdp2[current_state].values())]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/86:
for i in range(100):
    not_practice_game()
85/87: mdp1["B.BB.B..W....WWW"]
85/88: mdp1["BBBB........WWWW"]
85/89: mdp2["BBBB....W....WWW"]
85/90:
def not_practice_game():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/91:
for i in range(100):
    not_practice_game()
85/92: len(mdp1)
85/93: len(mdp2)
85/94: optimal_white_mdp={}
85/95:
def optimal_play_by_white():
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if optimal_white_mdp.get(current_state) is None:
            if len(mdp1.get(current_state))==0:
                optimal_white_mdp[current_state]={}
                reward=0
                break
            optimal_white_mdp[current_state]=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        if len(mdp1[current_state]) == 0:
            break
        current_state=optimal_white_mdp[current_state]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=np.random.choice(list(mdp2[current_state].keys()))
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    return reward
85/96:
for i in range(10):
    print(optimal_play_by_white())
85/97: optimal_white_mdp
85/98:
for i in range(100000):
    optimal_play_by_white()
85/99: len(optimal_white_mdp)
85/100:
for i in range(1000000):
    optimal_play_by_white()
85/101: len(optimal_white_mdp)
85/102: len(mdp1)
85/103: len(mdp2)
85/104: len(mdp1)
85/105:
def optimal_play_by_white():
    global mdp1, mdp2, optimal_white_mdp
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if optimal_white_mdp.get(current_state) is None:
            if len(mdp1.get(current_state))==0:
                optimal_white_mdp[current_state]={}
                reward=0
                break
            optimal_white_mdp[current_state]=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        if len(mdp1[current_state]) == 0:
            break
        current_state=optimal_white_mdp[current_state]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=np.random.choice(list(mdp2[current_state].keys()))
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    return reward
85/106:
for i in range(1000000):
    optimal_play_by_white()
85/107: len(optimal_white_mdp)
85/108: optimal_black_mdp={}
85/109:
def optimal_play_by_black():
    global mdp1, mdp2, optimal_black_mdp
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        current_state=np.random.choice(list(mdp1[current_state].keys()))
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if optimal_black_mdp.get(current_state) is None:
            if len(mdp2.get(current_state))==0:
                optimal_black_mdp[current_state]={}
                reward=0
                break
            optimal_black_mdp[current_state]=list(mdp2[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        if len(mdp2[current_state]) == 0:
            break
        current_state=optimal_black_mdp[current_state]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    return reward
85/110:
for i in range(1000000):
    optimal_play_by_black()
85/111: len(optimal_black_mdp)
85/112:
import json
with open("optimal_white_mdp.json", "w") as f:
    json.dump(optimal_white_mdp, f)
85/113:
with open("optimal_black_mdp.json", "w") as f:
    json.dump(optimal_black_mdp, f)
85/114:
import json
with open("optimal_white_mdp.json", "w") as f:
    json.dump(optimal_white_mdp, f, indent=4)
85/115:
with open("optimal_black_mdp.json", "w") as f:
    json.dump(optimal_black_mdp, f, indent=4)
85/116: len(mdp1)
85/117: len(mdp2)
85/118:
for i in range(10000000):
    game()
85/119: len(mdp1)
85/120: len(mdp2)
85/121:
rewards=[]
def game(epsilon=0.25, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    for i in range(len(states)-1):
        if i%2==0:
            mdp1[states[i]][states[i+1]]=((mdp1[states[i]][states[i+1]][0]*mdp1[states[i]][states[i+1]][1]+reward)/(mdp1[states[i]][states[i+1]][1]+1), mdp1[states[i]][states[i+1]][1]+1)
        else:
            mdp2[states[i]][states[i+1]]=((mdp2[states[i]][states[i+1]][0]*mdp2[states[i]][states[i+1]][1]-reward)/(mdp2[states[i]][states[i+1]][1]+1), mdp2[states[i]][states[i+1]][1]+1)
        reward*=gamma
85/122:
for i in range(100000000):
    game()
85/123: print(rewards[-100:])
85/124: len(mdp1)
85/125: len(mdp2)
85/126:
for i in range(10000000):
    optimal_play_by_white()
85/127: len(optimal_white_mdp)
85/128:
for i in range(10000000):
    optimal_play_by_white()
85/129: len(optimal_white_mdp)
85/130:
for i in range(100000000):
    optimal_play_by_black()
85/131: len(optimal_black_mdp)
85/132:
with open("optimal_black_mdp_longer.json", "w") as f:
    json.dump(optimal_black_mdp, f, indent=4)
85/133:
for i in range(1000000):
    game()
85/134:
mdp1={}
mdp2={}
85/135:
for i in range(100000):
    game()
85/136: print(rewards[-100:])
85/137: print(mpd1)
85/138: print(mdp1)
85/139: print(mdp1['B..B.W......W.WW'])
85/140: print(mdp1['B..B.W......W.WW'])
85/141: print(mdp1['B..B.W..W.....WW'])
85/142:
for key, value in mdp1.items():
    for key2, value2 in value.items():
        if value2[1]>90:
            print(key, key2, value2)
85/143:
for key, value in mdp1.items():
    for key2, value2 in value.items():
        if value2[0]>90:
            print(key, key2, value2)
85/144:
for key, value in mdp1.items():
    for key2, value2 in value.items():
        if value2[0]>20:
            print(key, key2, value2)
85/145:
rewards=[]
def game(epsilon=0.25, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if i%2==0:
            mdp1[states[length-2-i]][states[length-1-i]]=((mdp1[states[length-2-i]][states[length-i-1]][0]*mdp1[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1[states[length-2-i]][states[length-i-1]][1]+1), mdp1[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2[states[length-2-i]][states[length-1-i]]=((mdp2[states[length-2-i]][states[length-i-1]][0]*mdp2[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2[states[length-2-i]][states[length-i-1]][1]+1), mdp2[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/146:
for i in range(100000):
    game()
85/147:
for i in range(100000):
    game()
85/148:
mdp1={}
mdp2={}
85/149:
for i in range(100000):
    game()
85/150:
rewards=[]
def game(epsilon=0.25, gamma=0.9):
    global mdp1, mdp2
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1[current_state]={}
                reward=0
                break
            mdp1[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1[current_state].keys()))
        else:
            current_state=list(mdp1[current_state].keys())[np.argmax([v[0] for v in mdp1[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2[current_state]={}
                reward=0
                break
            mdp2[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2[current_state].keys()))
        else:
            current_state=list(mdp2[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1[states[length-2-i]][states[length-1-i]]=((mdp1[states[length-2-i]][states[length-i-1]][0]*mdp1[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1[states[length-2-i]][states[length-i-1]][1]+1), mdp1[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2[states[length-2-i]][states[length-1-i]]=((mdp2[states[length-2-i]][states[length-i-1]][0]*mdp2[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2[states[length-2-i]][states[length-i-1]][1]+1), mdp2[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/151:
for i in range(100000):
    game()
85/152:
for key, value in mdp1.items():
    for key2, value2 in value.items():
        if value2[0]>20:
            print(key, key2, value2)
85/153:
for key, value in mdp1.items():
    for key2, value2 in value.items():
        if value2[0]>50:
            print(key, key2, value2)
85/154:
mdp1={}
mdp2={}
85/155:
n=4
def find_possible_positions(state, player):
    global n
    result=[]
    if player==1:
        char1="W"
        char2="B"
        sign=-1
    else:
        char1="B"
        char2="W"
        sign=1
    for i in range(n*n):
        if state[i]==char1:
            if i+sign*n < 0:
                continue
            if i+sign*n >= 0 and state[i+sign*n]==".":
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*n]+char1+s[i+sign*n+1:]
                result.append(s)
            if (i+sign*(n-1))//n == (i+sign*n)//n and state[i+sign*(n-1)]==char2:
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*(n-1)]+char1+s[i+sign*(n-1)+1:]
                result.append(s)
            if (i+sign*(n+1))//n == (i+sign*n)//n and state[i+sign*(n+1)]==char2:
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*(n+1)]+char1+s[i+sign*(n+1)+1:]
                result.append(s)
    return result
85/156:
for i in range(100000):
    game()
85/157:
for key, value in mdp1.items():
    for key2, value2 in value.items():
        if value2[0]>50:
            print(key, key2, value2)
85/158:
for i in range(100000000):
    game()
85/159: len(mdp1)
85/160: len(mdp1)
85/161: len(mdp2)
85/162:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/163:
for i in range(10000):
    n_game()
85/164:
mdp1_5={}
mdp2_5={}
85/165:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/166:
for i in range(10000):
    n_game()
85/167: print(rewards[-100:])
85/168:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/169:
for i in range(10000):
    n_game()
85/170: print(rewards[-100:])
85/171: print(mdp1)
85/172: print(mdp1_5)
85/173:
mdp1_5={}
mdp2_5={}
85/174:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/175:
for i in range(10000):
    n_game()
85/176:
n=4
def find_possible_positions(state, player, n=4):
    # global n
    result=[]
    if player==1:
        char1="W"
        char2="B"
        sign=-1
    else:
        char1="B"
        char2="W"
        sign=1
    for i in range(n*n):
        if state[i]==char1:
            if i+sign*n < 0:
                continue
            if i+sign*n >= 0 and state[i+sign*n]==".":
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*n]+char1+s[i+sign*n+1:]
                result.append(s)
            if (i+sign*(n-1))//n == (i+sign*n)//n and state[i+sign*(n-1)]==char2:
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*(n-1)]+char1+s[i+sign*(n-1)+1:]
                result.append(s)
            if (i+sign*(n+1))//n == (i+sign*n)//n and state[i+sign*(n+1)]==char2:
                s=state
                s=s[:i]+"."+s[i+1:]
                s=s[:i+sign*(n+1)]+char1+s[i+sign*(n+1)+1:]
                result.append(s)
    return result
85/177:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/178:
for i in range(10000):
    n_game()
85/179: print(rewards[-100:])
85/180: print(mdp1_5)
85/181: print(mdp1_5)
85/182: print(mdp1_5)
85/183: mdp2_5
85/184: mdp1_5
85/185: print(mdp1_5)
85/186: mdp1_5
85/187:
for i in range(1000000):
    n_game()
85/188: print(rewards[-100:])
85/189:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    return reward
85/190:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/191:
for i in range(100):
    n_not_practice_game()
85/192:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2[current_state].values()])]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/193:
for i in range(100):
    n_not_practice_game()
85/194:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/195:
for i in range(100):
    n_not_practice_game()
85/196: mdp1_5['BBBBB...............WWWWW']
85/197: mdp2_5['BBBBB.............W.WWW.W']
85/198: mdp1_5['BBBB.....B........W.WWW.W']
85/199: mdp2_5['BBBB.....B.......WW.WW..W']
85/200: mdp1_5['BBB.....BB.......WW.WW..W']
85/201: mdp2_5['BBB.....BB.......WWWWW...']
85/202: mdp1_5['BB.....BBB.......WWWWW...']
85/203: mdp2_5['BB.....BBB......WWWWW....']
85/204: mdp1['BB.....B.B...B..WWWWW....']
85/205: mdp1_5['BB.....B.B...B..WWWWW....']
85/206: mdp2_5['BB.....B.B...W..W.WWW....']
85/207: len(mdp1_5)
85/208:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    if (len(rewards))>100:
        rewards=rewards[-100:]
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/209:
for i in range(1000000000):
    n_game()
85/210:
rewards=[]
def n_game(n=5, epsilon=0.25, gamma=0.9):
    global mdp1_5, mdp2_5, rewards
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    if (len(rewards))>100:
        rewards=rewards[-100:]
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/211:
for i in range(1000000000):
    n_game()
85/212: print(rewards[-100:])
85/213:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/214:
for i in range(100):
    n_not_practice_game()
85/215: mdp2_5['BBBBB.............................WWWWW']
85/216: mdp1_5['BBBBB.............................WWWWW']
85/217: mdp1_5['BBBBB...............WWWWW']
85/218: mdp1_5['BBBBB............W..WW.WW']
85/219: mdp2_5['BBBBB............W..WW.WW']
85/220:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
85/221:
for i in range(1):
    n_not_practice_game()
85/222: len(mdp1_5)
85/223:
for key, value in mdp1_5.items():
    for k, v in value.items():
        if v[0]>20:
            print(key, k, v)
    break
85/224: len(mdp2_5)
85/225:
rewards=[]
def n_game(n=5, epsilon=0.05, gamma=0.9):
    global mdp1_5, mdp2_5, rewards
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    if (len(rewards))>100:
        rewards=rewards[-100:]
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/226:
for i in range(100000000):
    n_game()
85/227: print(rewards[-100:])
85/228:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
85/229:
for i in range(1):
    n_not_practice_game()
85/230:
def n_not_practice_game(n=5):
    global mdp1_5, mdp2_5
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    reward=0
    while True:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=5)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        print(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=5)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        print(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    print(reward)
85/231:
for i in range(1):
    n_not_practice_game()
85/232: mdp1_5['BBBBB...............WWWWW']
85/233: mdp2_5['BBBBB............W..WW.WW']
85/234:
with open("mdp1_5.json", "w") as f:
    json.dump(mdp1_5, f, indent=4)
85/235:
with open("mdp2_5.json", "w") as f:
    json.dump(mdp2_5, f, indent=4)
85/236:
mdp1_5={}
mdp2_5={}
85/237:
rewards=[]
def n_game(n=5, epsilon=0.05, gamma=0.9):
    global mdp1_5, mdp2_5, rewards
    current_state="B"*n+"."*((n)*(n-2))+"W"*n
    states=[current_state]
    over=False
    reward=0
    while not over:
        if mdp1_5.get(current_state) is None:
            result=find_possible_positions(current_state, 1, n=n)
            if len(result)==0:
                mdp1_5[current_state]={}
                reward=0
                break
            mdp1_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp1_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp1_5[current_state].keys()))
        else:
            current_state=list(mdp1_5[current_state].keys())[np.argmax([v[0] for v in mdp1_5[current_state].values()])]
        states.append(current_state)
        for i in range(n):
            if current_state[i]=="W":
                reward=100
                break
        if reward!=0:
            break
        if mdp2_5.get(current_state) is None:
            result=find_possible_positions(current_state, 2, n=n)
            if len(result)==0:
                mdp2_5[current_state]={}
                reward=0
                break
            mdp2_5[current_state]={possible_state: (0,1) for possible_state in result}
        if len(mdp2_5[current_state]) == 0:
            break
        if random() < epsilon:
            current_state=np.random.choice(list(mdp2_5[current_state].keys()))
        else:
            current_state=list(mdp2_5[current_state].keys())[np.argmax([v[0] for v in mdp2_5[current_state].values()])]
        states.append(current_state)
        for i in range(n*n-n, n*n):
            if current_state[i]=="B":
                reward=-100
                break
        if reward!=0:
            break
    rewards.append(reward)
    if (len(rewards))>100:
        rewards=rewards[-100:]
    length=len(states)
    for i in range(length-1):
        if length%2==i%2:
            mdp1_5[states[length-2-i]][states[length-1-i]]=((mdp1_5[states[length-2-i]][states[length-i-1]][0]*mdp1_5[states[length-2-i]][states[length-1-i]][1]+reward)/(mdp1_5[states[length-2-i]][states[length-i-1]][1]+1), mdp1_5[states[length-2-i]][states[length-1-i]][1]+1)
        else:
            mdp2_5[states[length-2-i]][states[length-1-i]]=((mdp2_5[states[length-2-i]][states[length-i-1]][0]*mdp2_5[states[length-2-i]][states[length-1-i]][1]-reward)/(mdp2_5[states[length-2-i]][states[length-i-1]][1]+1), mdp2_5[states[length-2-i]][states[length-1-i]][1]+1)
        reward*=gamma
85/238:
for i in range(100000000):
    n_game()
85/239: print(rewards[-100:])
85/240:
for i in range(1):
    n_not_practice_game()
85/241:
for i in range(1000000000):
    n_game()
86/1:
n=4
m=7
mdp={'.'*(n*m): -2}
86/2:
import sys
sys.setrecursionlimit(10**10)
86/3:
import sys
sys.setrecursionlimit(10**8)
86/4:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
86/5:
def game(state, player):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generat
86/6:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
86/7: generate_states('.'*(n*m), n, m)
86/8: generate_states('.'*28, 4, 7)
86/9: generate_states(..................XX..XX..XX, 4, 7)
86/10: generate_states('..................XX..XX..XX', 4, 7)
86/11:
def game(state, player):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
86/12: game('.'*(1*2), 1)
86/13: game('.'*(1*2), 1, 1, 2)
86/14:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
86/15: game('.'*(1*2), 1, 1, 2)
86/16:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
86/17: game('.'*(1*2), 1, 1, 2)
86/18: game('.'*(2*2), 1, 1, 2)
87/1: game('.'*(1*4), 1, 1, 2)
87/2:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
87/3:
n=4
m=7
mdp={'.'*(n*m): -2}
87/4:
import sys
sys.setrecursionlimit(10**8)
87/5:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
87/6:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
87/7: game('.'*(1*4), 1, 1, 2)
88/1: game('.'*(1*2), 1, 1, 2)
88/2:
n=4
m=7
mdp={'.'*(n*m): -2}
88/3:
import sys
sys.setrecursionlimit(10**8)
88/4:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
88/5:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
88/6: game('.'*(1*2), 1, 1, 2)
88/7: generate_states('...')
88/8: generate_states('...', 1, 3)
88/9:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==1:
        if player==1:
            mdp[state]=-1
            return -11
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
88/10: generate_states('.XX', 1, 3)
88/11: generate_states('XXX', 1, 3)
88/12:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
89/1: game('.'*(1*3), 1, 1, 2)
89/2:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
89/3: game('.'*(1*3), 1, 1, 2)
89/4:
n=1
m=3
mdp={'.'*(n*m): -2}
89/5:
import sys
sys.setrecursionlimit(10**8)
89/6:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
89/7:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
89/8: game('.'*(1*3), 1, 1, 2)
90/1: game('.'*(1*3), 1, 1, 2)
90/2:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
90/3: game('.'*(1*3), 1, 1, 2)
90/4:
n=1
m=3
mdp={'.'*(n*m): -2}
90/5:
import sys
sys.setrecursionlimit(10**8)
90/6:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
90/7:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
91/1:
n=1
m=3
mdp={'.'*(n*m): -2}
91/2:
n=1
m=3
mdp={'.'*(n*m): -2}
91/3:
import sys
sys.setrecursionlimit(10**8)
91/4:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
91/5:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    print(substates)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
91/6: game('.'*(1*3), 1, 1, 2)
92/1:
n=1
m=3
mdp={'.'*(n*m): -2}
92/2:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    print(substates)
    return
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/3: game('.'*(1*3), 1, 1, 2)
92/4:
n=1
m=3
mdp={'.'*(n*m): -2}
92/5:
import sys
sys.setrecursionlimit(10**8)
92/6:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
92/7:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    print(substates)
    return
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/8: game('.'*(1*3), 1, 1, 2)
92/9: generate_states('...', 1, 3)
92/10: game('.'*(1*3), 1, 1, 3)
92/11:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    print(substates)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/12: game('.'*(1*3), 1, 1, 3)
92/13:
n_k=2
m_k=2
game('.'*(n_k*m_k), 1, n_k, m_k)
92/14:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/15:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            print("HERE", substate)
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/16:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/17:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    print(state, player, substates)
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/18:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/19:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    print(len(substates))
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/20:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/21:
def game(state, player, n, m):
    global mdp
    mdp={}
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/22:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/23:
def game(state, player, n, m):
    global mdp
    mdp={}
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    print(len(substates))
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/24:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/25:
def game(state, player, n, m):
    global mdp
    mdp={}
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=1
            return 1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/26:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/27:
def game(state, player, n, m):
    global mdp
    mdp={}
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
92/28:
n_k=2
m_k=2
game('...X', 1, n_k, m_k)
92/29:
n_k=1
m_k=2
game('...X', 1, n_k, m_k)
93/1:
import sys
sys.setrecursionlimit(10**8)
93/2:
n=1
m=3
mdp={'.'*(n*m): -2}
93/3:
import sys
sys.setrecursionlimit(10**8)
93/4:
def generate_states(state, n, m):
    results=[]
    for i in range(len(state)):
        if state[i]=='.':
            new_state=list(state)
            for j in range(i%n, n):
                for k in range(i//n, m):
                    if new_state[j+k*n]=='.':
                        new_state[j+k*n]='X'
                    else:
                        break
            results.append(''.join(new_state))
    return results
93/5:
def game(state, player, n, m):
    global mdp
    mdp={}
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
93/6:
n_k=1
m_k=2
game('.X', 1, n_k, m_k)
93/7:
n_k=1
m_k=2
game('..', 1, n_k, m_k)
93/8:
def game(state, player, n, m):
    global mdp
    mdp={}
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
93/9:
n_k=3
m_k=3
game('.'*(n_k*m_k), 1, n_k, m_k)
93/10:
n_k=4
m_k=7
game('.'*(n_k*m_k), 1, n_k, m_k)
93/11:
def game(state, player, n, m):
    global mdp
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
93/12:
n_k=3
m_k=3
mdp={}
game('.'*(n_k*m_k), 1, n_k, m_k)
93/13:
def game(state, player, n, m):
    global mdp
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
93/14:
n_k=3
m_k=3
mdp={}
game('.'*(n_k*m_k), 1, n_k, m_k)
93/15:
n_k=2
m_k=2
mdp={}
game('.'*(n_k*m_k), 1, n_k, m_k)
93/16:
def game(state, player, n, m):
    global mdp
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                print("HERE")
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                print("HERE")
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
93/17:
n_k=2
m_k=2
mdp={}
game('.'*(n_k*m_k), 1, n_k, m_k)
93/18:
def game(state, player, n, m):
    global mdp
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -2
    elif mdp[state] != -2:
        if player==2:
            return -mdp[state]
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        if player==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=-1
            return -1
    if player==1:
        score=-1
        for substate in substates:
            if game(substate, 2, n, m)==1:
                mdp[state]=1
                return 1
            score=max(score, mdp[substate])
        if score==-1:
            mdp[state]=-1
            return -1
        else:
            mdp[state]=0
            return 0
    else:
        score=1
        for substate in substates:
            if game(substate, 1, n, m)==-1:
                mdp[state]=-1
                return -1
            score=min(score, mdp[substate])
        if score==1:
            mdp[state]=1
            return 1
        else:
            mdp[state]=0
            return 0
93/19:
n_k=2
m_k=2
mdp={}
game('.'*(n_k*m_k), 1, n_k, m_k)
93/20:
def game(state, n, m):
    global mdp
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -1
    elif mdp[state] != -1:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        mdp[state]=1
        return 1
    for substate in substates:
        if game(substate, n, m)==0:
            mdp[state]=1
            return 1
    mdp[state]=0
    return 0
93/21:
n_k=2
m_k=2
mdp={}
game('.'*(n_k*m_k), 1, n_k, m_k)
93/22:
n_k=2
m_k=2
mdp={}
game('.'*(n_k*m_k), n_k, m_k)
93/23:
n_k=1
m_k=2
mdp={}
game('.'*(n_k*m_k), n_k, m_k)
93/24:
n_k=2
m_k=2
mdp={}
game('.'*(n_k*m_k), n_k, m_k)
93/25:
n_k=1
m_k=1
mdp={}
game('.'*(n_k*m_k), n_k, m_k)
93/26:
n_k=2
m_k=2
mdp={}
game('...X', n_k, m_k)
93/27:
def game(state, n, m):
    global mdp, winning_moves
    print(state)
    if mdp.get(state) is None:
        mdp[state] = -1
    elif mdp[state] != -1:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        mdp[state]=1
        return 1
    for substate in substates:
        if game(substate, n, m)==0:
            winning_moves[state]=substate
            mdp[state]=1
            return 1
    mdp[state]=0
    return 0
93/28:
n_k=2
m_k=2
mdp={}
winning_moves={}
game('.'*(n_k, m_k), n_k, m_k)
93/29:
n_k=2
m_k=2
mdp={}
winning_moves={}
game('.'*(n_k*m_k), n_k, m_k)
93/30: winning_moves
93/31:
n_k=4
m_k=7
mdp={}
winning_moves={}
game('.'*(n_k*m_k), n_k, m_k)
93/32: len(winning_moves)
93/33:
import json
with open('4x7_winning_moves.json', 'w') as f:
    json.dump(winning_moves, f)
93/34:
import json
with open('4x7_winning_moves.json', 'w') as f:
    json.dump(winning_moves, f, indent=4)
93/35:
def game(state, n, m):
    global mdp, winning_moves
    # print(state)
    if mdp.get(state) is None:
        mdp[state] = -1
    elif mdp[state] != -1:
        return mdp[state]
    substates=generate_states(state, n, m)
    if len(substates)==0:
        mdp[state]=1
        return 1
    for substate in substates:
        if game(substate, n, m)==0:
            winning_moves[state]=substate
            mdp[state]=1
            return 1
    mdp[state]=0
    return 0
93/36:
n_k=4
m_k=5
mdp={}
winning_moves={}
game('.'*(n_k*m_k), n_k, m_k)
93/37: len(winning_moves)
93/38:
n_k=7
m_k=4
mdp={}
winning_moves={}
game('.'*(n_k*m_k), n_k, m_k)
93/39: len(winning_moves)
93/40:
import json
with open('4x7_winning_moves.json', 'w') as f:
    json.dump(winning_moves, f, indent=4)
85/242: print(rewards[-100:])
85/243:
for i in range(1):
    n_not_practice_game()
85/244: len(mdp2_5)
85/245: len(mdp1_5)
85/246:
with open("mdp1_5.json", "w") as f:
    mdp1_orig=json.load(f)
85/247:
with open("mdp1_5.json", "r") as f:
    mdp1_orig=json.load(f)
94/1:
with open("mdp1_5.json", "w") as f:
    json.dump(mdp1_5, f, indent=4)
95/1:
import pygame
import sys

pygame.init()

width = 800
height = 600
screen = pygame.display.set_mode((width, height))
pygame.display.set_caption('Chomp Game')


rows = 6
cols = 8
square_size = 50
padding = 10

black = (0, 0, 0)
brown = (139, 69, 19)
white = (255, 255, 255)


font = pygame.font.Font(None, 36)


game_over = False
turn = 0  


chocolate = [[True for _ in range(cols)] for _ in range(rows)]

def draw_chocolate():
    for row in range(rows):
        for col in range(cols):
            if chocolate[row][col]:
                pygame.draw.rect(screen, brown,
                                 (col * (square_size + padding) + padding,
                                  row * (square_size + padding) + padding,
                                  square_size, square_size))
            else:
                pygame.draw.rect(screen, white,
                                 (col * (square_size + padding) + padding,
                                  row * (square_size + padding) + padding,
                                  square_size, square_size))

def chomp(row, col):
    global game_over
    for r in range(row, rows):
        for c in range(col, cols):
            chocolate[r][c] = False
    if row == 0 and col == 0:
        game_over = True

def display_winner():
    loser = "Player 1" if turn == 1 else "Player 2"
    text = font.render(f'{loser} is a fucking loser !', True, white, brown)
    text_rect = text.get_rect(center=(width // 2, height - 50))
    screen.blit(text, text_rect)

def display_turn():
    player_turn = "Player 1's Turn" if turn == 0 else "Player 2's Turn"
    text = font.render(player_turn, True, white)
    text_rect = text.get_rect(center=(width // 2, 500))
    screen.blit(text, text_rect)

def main_game_loop():
    running = True
    global turn
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            elif event.type == pygame.MOUSEBUTTONDOWN and not game_over:
                x, y = event.pos
                col = x // (square_size + padding)
                row = y // (square_size + padding)
                if chocolate[row][col]: 
                    chomp(row, col)
                    turn = 1 - turn

        screen.fill(black) 
        draw_chocolate() 
        display_turn()   
        if game_over:
            display_winner()

        pygame.display.flip()  

if __name__ == "__main__":
    main_game_loop()
96/1:
import pygame
import sys
import random

pygame.init()
screen = pygame.display.set_mode((1280, 720))
pygame.display.set_caption('Nim Game')
font = pygame.font.Font(None, 40)
piles = [5, 4, 3, 2]
turn = 0  

def draw_piles():
    screen.fill((0, 0, 0))
    matchstick_width = 10
    matchstick_height = 50
    for index, stones in enumerate(piles):
        for stone in range(stones):
            pygame.draw.rect(screen, (255, 149, 0), pygame.Rect(100 + 40 * stone, 100 + 100 * index, matchstick_width, matchstick_height))
    text = font.render(f'Player {turn + 1}\'s turn', True, (255, 255, 255))
    screen.blit(text, (10, 50))

def nim_sum():
    n_sum = 0
    for pile in piles:
        n_sum ^= pile
    return n_sum

def optimal_move():
    n_sum = nim_sum()
    if n_sum == 0:
        return random_move()  
    for i in range(len(piles)):
        target = n_sum ^ piles[i]
        if target < piles[i]:
            return (i, target)
    return random_move()  

def random_move():
    non_empty_piles = [(i, pile) for i, pile in enumerate(piles) if pile > 0]
    if not non_empty_piles:
        return None
    pile_index, stones = random.choice(non_empty_piles)
    stones_to_remove = random.randint(1, stones)
    return (pile_index, stones - stones_to_remove)

def execute_move(move):
    global turn
    pile_index, stones_to_remove = move
    piles[pile_index] = stones_to_remove
    turn = 0  

def handle_click(x, y):
    global turn
    matchstick_width = 10
    matchstick_height = 50
    if turn == 0: 
        for index in range(len(piles)):
            if 100 + 100 * index < y < 100 + 100 * index + matchstick_height:
                stone_count = piles[index]
                for stone in range(stone_count):
                    if 100 + 40 * stone < x < 100 + 40 * stone + matchstick_width:
                        piles[index] = stone
                        turn = 1 - turn
                        break
        if turn == 1:
            move = optimal_move()
            if move:
                execute_move(move)

def check_win():
    if all(pile == 0 for pile in piles):
        winner = "Bot" if turn == 0 else "Player"
        if(turn == 0):
            text = font.render(f'You are a fucking loser, cry about it', True, (255,0,0))
        else:
            text = font.render(f'You win, nerd', True, (255, 0, 0))
        screen.blit(text, (200, 360))
        pygame.display.flip()
        pygame.time.wait(2000)
        pygame.quit()
        sys.exit()

def main_game_loop():
    running = True
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            elif event.type == pygame.MOUSEBUTTONDOWN and turn == 0:
                x, y = event.pos
                handle_click(x, y)

        draw_piles()
        check_win()
        pygame.display.flip()

if __name__ == "__main__":
    main_game_loop()
97/1:
import pygame
import sys

pygame.init()
screen = pygame.display.set_mode((1280, 720))
pygame.display.set_caption('Nim Game')
font = pygame.font.Font(None, 40)
piles = [5, 4, 3, 2]
turn = 0

def draw_piles():
    screen.fill((0, 0, 0))
    matchstick_width = 10
    matchstick_height = 50
    for index, stones in enumerate(piles):
        for stone in range(stones):
            pygame.draw.rect(screen,(255,149,0), pygame.Rect(100 + 40 * stone, 100 + 100 * index, matchstick_width, matchstick_height))
    text = font.render(f'Player {turn + 1}\'s turn', True, (255, 255, 255))
    screen.blit(text, (10, 50))

def handle_click(x, y):
    global turn
    matchstick_width = 10
    matchstick_height = 50
    for index in range(len(piles)):
        if 100 + 100 * index < y < 100 + 100 * index + matchstick_height:
            stone_count = piles[index]
            for stone in range(stone_count):
                if 100 + 40 * stone < x < 100 + 40 * stone + matchstick_width:
                    piles[index] = stone
                    turn = 1 - turn
                    break

def check_win():
    if all(pile == 0 for pile in piles):
        text = font.render(f'Player {2 - turn} wins!', True, (255, 0, 0))
        screen.blit(text, (600, 360))
        pygame.display.flip()
        pygame.time.wait(2000)
        pygame.quit()
        sys.exit()

def main_game_loop():
    running = True
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            elif event.type == pygame.MOUSEBUTTONDOWN:
                x, y = event.pos
                handle_click(x, y)

        draw_piles()
        check_win()
        pygame.display.flip()

if __name__ == "__main__":
    main_game_loop()
98/1: import pandas
98/2: import pandas
98/3: import pandas as pd
98/4: data=pd.read_csv('Untitled\ spreadsheet\ -\ Sheet1.csv')
98/5: data=pd.read_csv('Untitled spreadsheet - Sheet1.csv')
98/6: data.columns
98/7: data['RollNumber Name']
98/8: data=data[data['RollNumber Name'].startswith("23B09") | data['RollNumber Name'].startswith("23B10")]
98/9: data=data[data['RollNumber Name'].to_string().startswith("23B09") | data['RollNumber Name'].to_string().startswith("23B10")]
98/10: data=data[data['RollNumber Name'] != "" & (data['RollNumber Name'].to_string().startswith("23B09") | data['RollNumber Name'].to_string().startswith("23B10"))]
98/11: data=data[data['RollNumber Name'].to_string() != "" & (data['RollNumber Name'].to_string().startswith("23B09") | data['RollNumber Name'].to_string().startswith("23B10"))]
98/12: data=data[(data['RollNumber Name'].to_string().startswith("23B09") | data['RollNumber Name'].to_string().startswith("23B10"))]
98/13: series=data[1:10]
98/14: series
98/15: series=series[series['RollNumber Name'].str.contains('23B09')]
98/16: series
98/17: series=data.dropna()
98/18: series
98/19: series=series[series['RollNumber Name'].str.contains('23B09') | series['RollNumber Name'].str.contains('23B10')]
98/20: series.head()
98/21: series.sort_values('Previous Dues',ascending=False)
98/22: len(series)
98/23: series=data.dropna()
98/24: series.head()
98/25: len(series)
98/26: series['RollNumber Name'=='23B0940']['Room No']
98/27: series[series['RollNumber Name'=='23B0940']]['Room No']
98/28: series[series['RollNumber Name']=='23B0940']['Room No']
98/29: series[series['RollNumber Name']=='23B1056']['Room No']
98/30: series[series['RollNumber Name']=='23b0940']['Room No']
98/31: data[data['RollNumber Name']=='23b0940']['Room No']
98/32: len(data)
98/33: import pandas as pd
98/34: data=pd.read_csv('data.csv')
98/35: data.sort('RollNumber Name')
98/36: data.columns
98/37: data=pd.read_csv('data.csv', columns=['S.No','Room','Roll Num','Name', 'Whatever', 'Total', 'Jan', 'Feb', 'mar', 'Apr'])
98/38: data=pd.read_csv('data.csv', index_col==['S.No','Room','Roll Num','Name', 'Whatever', 'Total', 'Jan', 'Feb', 'mar', 'Apr'])
98/39: data=pd.read_csv('data.csv', names=['S.No','Room','Roll Num','Name', 'Whatever', 'Total', 'Jan', 'Feb', 'mar', 'Apr'])
98/40: data.head
98/41: data.head()
98/42: data=data[data['Roll Num'].str.startswith('23B09') | data['Roll Num'].str.startswith('23B10')]
98/43: data.head()
98/44: data=data[data['S.No'].str.startswith('23B09') | data['S.No'].str.startswith('23B10')]
98/45: data.sort_values('Total')
98/46: data.sort_values('Total', inplace=True, ascending=False)
98/47: data.sort_values('Total')
99/1: import chess
99/2: import chess
99/3: import chess
99/4: board=chess.Board()
99/5: board.generate_legal_moves()
99/6: board.is_checkmate()
99/7: print(board.generate_legal_moves())
99/8:
for move in board.legal_moves:
    print(move)
100/1: board
100/2: import chess
100/3: board=chess.Board()
100/4: board.generate_legal_moves()
100/5:
for move in board.legal_moves:
    print(move)
100/6: board
100/7: Nf3 = chess.Move.from_uci("g1f3")
100/8: board.push(Nf3)
100/9: board
100/10: board.pop()
100/11: board
100/12: Nf3 = chess.Move.from_uci("e2e4")
100/13: board.push(Nf3)
100/14: board
100/15: Nf3 = chess.Move.from_uci("e7e5")
100/16: board.push(Nf3)
100/17: board
100/18: Nf3 = chess.Move.from_uci("f1b5")
100/19: board.push(Nf3)
100/20: board
100/21: Nf3 = chess.Move.from_uci("d7d6")
100/22: board.push(Nf3)
100/23: board
100/24: Nf3 = chess.Move.from_uci("b5e8")
100/25: board.push(Nf3)
100/26: board
100/27: Nf3 = chess.Move.from_uci("f8e7")
100/28: board.push(Nf3)
100/29: board
100/30: Nf3 = chess.Move.from_uci("e7e6")
100/31: board.push(Nf3)
100/32: board
100/33: Nf3 = chess.Move.from_uci("e7e6")
100/34: board.push(Nf3)
103/1:
import pandas as pd
import pandas
103/2: data=pandas.read_csv('courses.csv')
103/3: data.head()
103/4: data=data[data['Sr no.'].isint()]
103/5: data=data[data['Sr no.'].str.isnumeric]
103/6: data=data[data['Sr no.'].str.isnumeric()]
103/7: data.head()
103/8: data['Sr no.'].str.isnumeric()
103/9: data=data[data['Sr no.'].str.isnumeric()]
103/10: data.dropna(subset=['Sr no.'], inplace=True)
103/11: data=data[data['Sr no.'].str.isnumeric()]
103/12: data.head()
103/13: data.to_csv('final_course_list.csv',index=False)
103/14: data.dropna(subset=['Slot'], inplace=True)
103/15: data.to_csv('final_course_list.csv',index=False)
103/16:
for i in [str(i) for i in range(1, 16)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    new_data=data[data['Slot'].str.begins_with(i)]
    new_data.to_csv('slot_'+i+'.csv',index=False)
103/17:
for i in [str(i) for i in range(1, 16)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    new_data=data[data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_'+i+'.csv',index=False)
103/18:
for i in [str(i) for i in range(1, 10)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'\b')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/19:
import pandas as pd
import pandas
import re
103/20:
for i in [str(i) for i in range(1, 10)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'\b')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/21:
for i in [str(i) for i in range(1, 16)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/22:
for i in [str(i) for i in range(1, 17)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/23:
for i in [str(i) for i in range(1, 17)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[not data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[not data['Slot'].str.startswith(i)]
    # new_data.to_csv('slot_' + i + '.csv', index=False)
103/24:
for i in [str(i) for i in range(1, 17)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    # new_data.to_csv('slot_' + i + '.csv', index=False)
103/25:
for i in [str(i) for i in range(1, 17)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    # new_data.t?o_csv('slot_' + i + '.csv', index=False)
103/26:
for i in [str(i) for i in range(1, 17)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    # new_data.to_csv('slot_' + i + '.csv', index=False)
103/27:
for i in [str(i) for i in range(1, 17)] + ['LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/28: data
103/29:
for i in [str(i) for i in range(1, 17)] + ['Lmon','LFri','LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/30:
for i in [str(i) for i in range(1, 17)] + ['LMon','LFri','LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/31: data=pandas.read_csv('courses.csv')
103/32: data.dropna(subset=['Sr no.'], inplace=True)
103/33: data=data[data['Sr no.'].str.isnumeric()]
103/34: data.to_csv('final_course_list.csv',index=False)
103/35: data.dropna(subset=['Slot'], inplace=True)
103/36: data.to_csv('final_course_list.csv',index=False)
103/37:
for i in [str(i) for i in range(1, 17)] + ['LMon','LFri','LThu', 'LTue', 'XWed', 'WFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/38: data
103/39:
for i in ['LWed']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/40: data
103/41:
for i in ['XFri']:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
103/42: data
104/1:
slot_1_data=pd.read_csv('slot_1.csv')
slot_1_data
104/2:
import pandas as pd
import pandas
import re
104/3:
slot_1_data=pd.read_csv('slot_1.csv')
slot_1_data
104/4: slot_1_data=pd.read_csv('slot_1.csv')
104/5: slot_1_data=slot_1_data[slot_1_data['Course content categoryOnly for 2022 Entrants']=='Interdisciplinary STEM Course.']
104/6: slot_1_data
104/7: slot_1_data=slot_1_data[slot_1_data['Course content categoryOnly for 2022 Entrants']=='Interdisciplinary STEM Course.' | slot_1_data['Course content categoryOnly for 2022 Entrants'] == 'Interdisciplinary HASMED Course.']
104/8: slot_1_data=pd.read_csv('slot_1.csv')
104/9: slot_1_data=slot_1_data[(slot_1_data['Course content categoryOnly for 2022 Entrants']=='Interdisciplinary STEM Course.') | (slot_1_data['Course content categoryOnly for 2022 Entrants'] == 'Interdisciplinary HASMED Course.')]
104/10: slot_1_data
104/11:
for i in [str(i) for i in range(1, 17)] + ['LMon','LFri','LThu', 'LTue', 'LWed', 'XWed', 'XFri']:
    slot_i_data=pd.read_csv('slot_' + i + '.csv')
    slot_1_data_without_advanced=slot_1_data[(slot_1_data['Course content categoryOnly for 2022 Entrants']=='Interdisciplinary STEM Course.') | (slot_1_data['Course content categoryOnly for 2022 Entrants'] == 'Interdisciplinary HASMED Course.')]
    slot_1_data_without_advanced.to_csv('non_advanced_slot_'+i+'.csv',index=False)
104/12:
for i in [str(i) for i in range(1, 17)] + ['LMon','LFri','LThu', 'LTue', 'LWed', 'XWed', 'XFri']:
    slot_i_data=pd.read_csv('slot_' + i + '.csv')
    slot_i_data_without_advanced=slot_i_data[(slot_i_data['Course content categoryOnly for 2022 Entrants']=='Interdisciplinary STEM Course.') | (slot_i_data['Course content categoryOnly for 2022 Entrants'] == 'Interdisciplinary HASMED Course.')]
    slot_i_data_without_advanced.to_csv('non_advanced_slot_'+i+'.csv',index=False)
105/1: data=pandas.read_csv('courses.csv')
105/2: data.dropna(subset=['Sr no.'], inplace=True)
105/3:
import pandas as pd
import pandas
import re
105/4: data=pandas.read_csv('courses.csv')
105/5: data.dropna(subset=['Sr no.'], inplace=True)
105/6: data=data[data['Sr no.'].str.isnumeric()]
105/7: data.to_csv('final_course_list.csv',index=False)
105/8: data.dropna(subset=['Slot'], inplace=True)
105/9: data.to_csv('final_course_list.csv',index=False)
105/10:
for i in [str(i) for i in range(8, 9)]:
    if i.isdigit():
        new_data = data[data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
        data=data[~data['Slot'].str.match(r'^' + re.escape(i) + r'(?!\d)')]
    else:
        new_data = data[data['Slot'].str.startswith(i)]
        data=data[~data['Slot'].str.startswith(i)]
    new_data.to_csv('slot_' + i + '.csv', index=False)
105/11:
for i in [str(i) for i in range(8,9)]:
    slot_i_data=pd.read_csv('slot_' + i + '.csv')
    slot_i_data_without_advanced=slot_i_data[(slot_i_data['Course content categoryOnly for 2022 Entrants']=='Interdisciplinary STEM Course.') | (slot_i_data['Course content categoryOnly for 2022 Entrants'] == 'Interdisciplinary HASMED Course.')]
    slot_i_data_without_advanced.to_csv('non_advanced_slot_'+i+'.csv',index=False)
106/1: import pandas
106/2: import pandas as pd
106/3: data=pandas.read_csv('students.csv')
106/4: data=pd.read_csv('students.csv')
106/5: students_list=data['Name'].tolist()
106/6: print(students_list)
106/7: new_data=pd.read_csv('here.csv')
106/8:
with open('here.csv') as f:
    for line in f:
        name=line.split("'")[1:2]
        print(name)
106/9:
result=[]
with open('here.csv') as f:
    for line in f:
        name=line.split("'")[1:2]
        result.append(name[0])
106/10: print(result)
106/11: print(len([name for name in result if name in   students_list]))
106/12: print([name for name in result if name in students_list])
106/13:
result=[]
with open('here.csv') as f:
    for line in f:
        name=line.split("'")[1:2]
        result.append(name[0])
106/14: print(result)
106/15: print([name for name in result if name in students_list])
106/16: print(len([name for name in result if name in students_list]))
109/1:
#!/usr/bin/python3
#

import matplotlib.pyplot as plt
import numpy as np


y = []

with open('results.txt', 'r') as f:

    lines = f.readlines()

    for line in lines[2:]:
        y.append(int(line))

x = [ind for ind, _ in enumerate(lines[2:])]

plt.bar(x, y, width=0.8, align='center')
plt.ylabel('Elements')
plt.title("Hash Function")
plt.savefig("Hash.png")
109/2:
#!/usr/bin/python3
#

import matplotlib.pyplot as plt
import numpy as np


y = []

with open('results.txt', 'r') as f:

    lines = f.readlines()

    for line in lines[2:]:
        y.append(int(line))

x = [ind for ind, _ in enumerate(lines[2:])]

plt.bar(x, y, width=0.8, align='center')
plt.ylabel('Elements')
plt.title("Hash Function")
plt.savefig("Hash.png")
110/1:
def UpdateMean(OldMean, NewDataValue, n, A):
    return (OldMean*n+NewDataValue)/(n+1)
110/2: def UpdateMedian(OldMedian, NewDataValue, n, A):
111/1: from matplotlib import pyplot as plt
111/2: plt.show()
111/3: plt.plot([1, 2, 3, 4], [1, 4, 9, 16])
111/4:
from matplotlib import pyplot as plt
import numpy as np
111/5: x=np.linspace(0,10,100)
111/6:
x=np.linspace(0,10,100)
plt.plot(x, 1/x, label='1/x')
111/7:
x=np.linspace(0,100,1000)
plt.plot(x, 1/x, label='1/x')
111/8:
x=np.linspace(0,10,100)
plt.plot(x, 1/x, label='1/x')
112/1: from matplotlib import pyplot as plt
112/2: from matplotlib import pyplot as plt
112/3: plt.plot([1, 2, 3, 4], [1, 4, 9, 16])
112/4:
df = sns.load_dataset("titanic")
sns.violinplot(x=df["age"])
112/5:
from matplotlib import pyplot as plt
import seaborn as sns
112/6:
df = sns.load_dataset("titanic")
sns.violinplot(x=df["age"])
112/7: sns.violinplot(data=df, x="age", y="class")
112/8:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

df = pd.DataFrame({'country': [177.0, 7.0, 4.0, 2.0, 2.0, 1.0, 1.0, 1.0]})
df.index = ['USA', 'Canada', 'Russia', 'UK', 'Belgium', 'Mexico', 'Germany', 'Denmark']
df = df.sort_values(by='country',ascending=False)
df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


fig, ax = plt.subplots()
ax.bar(df.index, df["country"], color="C0")
ax2 = ax.twinx()
ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/9:
import plotly.express as px
df = px.data.wind()
fig = px.bar_polar(df, r="frequency", theta="direction",
                   color="strength", template="plotly_dark",
                   color_discrete_sequence= px.colors.sequential.Plasma_r)
fig.show()
112/10:
import plotly.express as px
df = px.data.wind()
fig = px.bar_polar(df, r="frequency", theta="direction",
                   color="strength", template="plotly_dark",
                   color_discrete_sequence= px.colors.sequential.Electric)
fig.show()
112/11:
import plotly.express as px
df = px.data.wind()
fig = px.bar_polar(df, r="frequency", theta="direction",
                   color="strength", template="plotly_dark",
                   color_discrete_sequence= px.colors.sequential.Nitro)
fig.show()
112/12:
import plotly.express as px
df = px.data.wind()
fig = px.bar_polar(df, r="frequency", theta="direction",
                   color="strength", template="plotly_dark",
                   color_discrete_sequence= px.colors.sequential.Spectral)
fig.show()
112/13:
import plotly.express as px
df = px.data.wind()
fig = px.bar_polar(df, r="frequency", theta="direction",
                   color="strength", template="plotly_dark",
                   color_discrete_sequence= px.colors.sequential.Magenta)
fig.show()
112/14:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, columns=['country'], index=data.keys())
df = df.sort_values(by='country',ascending=False)
df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


fig, ax = plt.subplots()
ax.bar(df.index, df["country"], color="C0")
ax2 = ax.twinx()
ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/15:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, columns=['country'], index=data.keys())
# df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/16:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, columns=['country'], index=data.keys())
df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/17:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, columns=['country'], index=data.keys())
df = df.sort_values(by='country',ascending=False)
print(df)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/18:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, index=data.keys())
df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/19:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, index=data.keys())
# df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/20:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame(data, index=data.keys())
print(df)
# df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/21:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame.from_dict(data, index=data.keys())
print(df)
# df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/22:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
data={key: int(value) for key, value in data.items()}
df = pd.DataFrame.from_dict(data)
print(df)
# df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/23:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
print(df)
# df = df.sort_values(by='country',ascending=False)
# df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


# fig, ax = plt.subplots()
# ax.bar(df.index, df["country"], color="C0")
# ax2 = ax.twinx()
# ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
# ax2.yaxis.set_major_formatter(PercentFormatter())

# ax.tick_params(axis="y", colors="C0")
# ax2.tick_params(axis="y", colors="C1")
# plt.show()
112/24:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='country',ascending=False)
df["cumpercentage"] = df["country"].cumsum()/df["country"].sum()*100


fig, ax = plt.subplots()
ax.bar(df.index, df["country"], color="C0")
ax2 = ax.twinx()
ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/25:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='country',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df.index, df["country"], color="C0")
ax2 = ax.twinx()
ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/26:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df.index, df["country"], color="C0")
ax2 = ax.twinx()
ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/27:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["country"], df.index, color="C0")
ax2 = ax.twinx()
ax2.plot(df.index, df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/28:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["country"], df.index, color="C0")
ax2 = ax.twinx()
ax2.plot(df["count"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/29:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["count"], df.index, color="C0")
ax2 = ax.twinx()
ax2.plot(df["count"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/30:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["country"], df['count'], color="C0")
ax2 = ax.twinx()
ax2.plot(df["country"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/31:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


plt.figure(figsize=(20, 10))
fig, ax = plt.subplots()
ax.bar(df["country"], df['count'], color="C0")
ax2 = ax.twinx()
ax2.plot(df["country"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/32:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["country"], df['count'], color="C0")
ax2 = ax.twinx()
ax2.plot(df["country"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax.tick_params(axis="x", rotation=45)
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/33:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["country"], df['count'], color="C0")
ax2 = ax.twinx()
ax2.plot(df["country"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax.tick_params(axis="x", rotation=90)
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/34:
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data={'United States': '40', 'China': '40', 'Japan': '20', 'Australia': '18', 'France': '16', 'Netherlands': '15', 'Great Britain': '14', 'South Korea': '13', 'Italy': '12', 'Germany': '12', 'Remaining NOCs': '129'}
df = pd.DataFrame.from_dict({'country': list(data.keys()), 'count': [int(value) for value in data.values()]})
# print(df)
df = df.sort_values(by='count',ascending=False)
df["cumpercentage"] = df["count"].cumsum()/df["count"].sum()*100


fig, ax = plt.subplots()
ax.bar(df["country"], df['count'], color="C0")
ax2 = ax.twinx()
ax2.plot(df["country"], df["cumpercentage"], color="C1", marker="D", ms=7)
ax2.yaxis.set_major_formatter(PercentFormatter())

ax.tick_params(axis="y", colors="C0")
ax.tick_params(axis="x", rotation=75)
ax2.tick_params(axis="y", colors="C1")
plt.show()
112/35:
df = sns.load_dataset("titanic")
sns.violinplot(x=df["age"], orient="v")
112/36:
df = sns.load_dataset("titanic")
sns.violinplot(x=df["age"], y=[1], orient="v")
112/37: sns.violinplot(data=df, x="age", y="class", orient="v")
112/38: sns.violinplot(data=df, y="age", x="class", orient="v")
112/39:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Generate sample data
x = np.linspace(0, 10, 100)  # X-axis values
y = np.linspace(0, 10, 10)   # Y-axis values
X, Y = np.meshgrid(x, y)

Z = np.sin(X) * np.cos(Y[:, np.newaxis])  # Z-axis values as a function of X and Y

# Create the figure and 3D axes
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot each line in the waterfall plot
for i in range(len(y)):
    ax.plot(x, y[i]*np.ones_like(x), Z[i], color='b')

# Set labels and title
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
ax.set_zlabel('Z-axis')
ax.set_title('Waterfall Plot')

# Display the plot
plt.show()
113/1:
from pyqtspecgram import pyqtspecgram
import numpy as np

Fs = 1e6
t = 5  # sec
N = int(t * Fs)
sigma_noise_2 = 0.1
NFFT = 4096

a = np.exp(1j * 2 * np.pi * 150e3 / Fs * np.arange(N))
a += np.exp(-1j * 2 * np.pi * 298e3 / Fs * np.arange(N))
a += np.sqrt(sigma_noise_2) / 2 * (np.random.randn(N) + 1j * np.random.randn(N))

Sxx, f_a, t_a, fig = pyqtspecgram(a, NFFT, Fs, Fc=0)
113/2:
from pyqtspecgram import pyqtspecgram
import numpy as np

Fs = 1e6
t = 5  # sec
N = int(t * Fs)
sigma_noise_2 = 0.1
NFFT = 4096

a = np.exp(1j * 2 * np.pi * 150e3 / Fs * np.arange(N))
a += np.exp(-1j * 2 * np.pi * 298e3 / Fs * np.arange(N))
a += np.sqrt(sigma_noise_2) / 2 * (np.random.randn(N) + 1j * np.random.randn(N))

Sxx, f_a, t_a, fig = pyqtspecgram(a, NFFT, Fs, Fc=0)
113/3: df=sns.load_dataset('titanic')
113/4: import seaborn as sns
113/5: df=sns.load_dataset('titanic')
113/6: df.head()
113/7: df=sns.load_dataset('penguins')
113/8: sns.violinplot(x=df["species"], y=[1], orient="v")
113/9: sns.violinplot(x=df["species"], y=df['bill_length_mm'], orient="v")
113/10:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Create the X and Y meshgrid
X, Y = np.meshgrid(np.arange(-5, 5.5, 0.5), np.arange(-5, 5.5, 0.5))

# Calculate the Z values
Z = Y * np.sin(X) - X * np.cos(Y)

# Create the figure and axis for 3D plotting
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plotting the waterfall
for i in range(len(X)):
    ax.plot(X[i], Y[i], Z[i], color='b')  # Plot each row of Z as a line

# Labels for clarity
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/11:
from pyqtspecgram import pyqtspecgram
import numpy as np

Fs = 1e6
t = 5  # sec
N = int(t * Fs)
sigma_noise_2 = 0.1
NFFT = 4096

a = np.exp(1j * 2 * np.pi * 150e3 / Fs * np.arange(N))
a += np.exp(-1j * 2 * np.pi * 298e3 / Fs * np.arange(N))
a += np.sqrt(sigma_noise_2) / 2 * (np.random.randn(N) + 1j * np.random.randn(N))

Sxx, f_a, t_a, fig = pyqtspecgram(a, NFFT, Fs, Fc=0)
113/12:
from pyqtspecgram import pyqtspecgram
import numpy as np

Fs = 1e6  # Sampling frequency
t = 5  # Duration in seconds
N = int(t * Fs)  # Number of samples
sigma_noise_2 = 0.1  # Noise variance
NFFT = 8192  # Increase FFT size for better resolution

# Generate the signal
a = np.exp(1j * 2 * np.pi * 150e3 / Fs * np.arange(N))
a += np.exp(-1j * 2 * np.pi * 298e3 / Fs * np.arange(N))
a += np.sqrt(sigma_noise_2) / 2 * (np.random.randn(N) + 1j * np.random.randn(N))

# Generate the spectrogram
Sxx, f_a, t_a, fig = pyqtspecgram(a, NFFT, Fs, Fc=0)

# Plot the spectrogram (assuming fig is a matplotlib figure object)
fig.show()
113/13:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

X, Y = np.meshgrid(np.arange(-5, 5.5, 0.5), np.arange(-5, 5.5, 0.5))

Z = Y * np.sin(X) - X * np.cos(Y)

cmap = cm.get_cmap('viridis')

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/14:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.arange(-5, 5.5, 0.5), np.arange(-5, 5.5, 0.5))

Z = Y * np.sin(X) - X * np.cos(Y)

cmap = colormaps['viridris']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/15:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.arange(-5, 5.5, 0.5), np.arange(-5, 5.5, 0.5))

Z = Y * np.sin(X) - X * np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/16:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.arange(-5, 5.5, 0.5), np.arange(-5, 5.5, 0.5))

Z = Y * np.sin(X) - X * np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

# Draw vertical lines at the right edge of the map
for i in range(len(X)):
    # We assume the right edge corresponds to the maximum value in X or Y
    # Find index of the rightmost edge of the plot
    right_edge_idx = len(Y) - 1
    ax.plot(
        [X[i, right_edge_idx], X[i, right_edge_idx]],  # X values
        [Y[i, right_edge_idx], Y[i, right_edge_idx]],  # Y values
        [0, Z[i, right_edge_idx]],                    # Z values from 0 to Z at the right edge
        color='k', linestyle='--'                     # Black dashed lines
    )

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/17:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.arange(-5, 5.5, 0.5), np.arange(-5, 5.5, 0.5))

Z = Y * np.sin(X) - X * np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/18:
import numpy as np
from scipy.io import wavfile
from scipy.signal import stft, hamming
from scipy.fftpack import fft, fftfreq, fftshift
from matplotlib import pyplot as plt

#def init_figure(titles, x_labels, grid=False): ...
#def freq_idx(freqs, fs, n, center=0): ...
#def sample_idx(samples, fs): ...

# Parameters
window = 'hamming' # Type of window
nperseg = 180 # Sample per segment
noverlap = int(nperseg * 0.7) # Overlapping samples
nfft = 256 # Padding length
return_onesided = False # Negative + Positive 
scaling = 'spectrum' # Amplitude

freq_low, freq_high = 600, 1780
time_low, time_high = 0.103, 0.1145

# Read data
fs, data = wavfile.read(filepath)
if len(data.shape) > 1: data = data[:,0] # select first channel

# Prepare plot
fig, (ax1, ax2) = init_figure([f'STFT padding={nfft}', 'DFT of selected samples'], ['time (s)', 'amplitude'])

# STFT (f=freqs, t=times, Zxx=STFT of input)
f, t, Zxx = stft(data, fs, window=window, nperseg=nperseg, noverlap=noverlap,
                 nfft=nfft, return_onesided=return_onesided, scaling=scaling)
f_shifted = fftshift(f)
Z_shifted = fftshift(Zxx, axes=0)

# Plot STFT for selected frequencies
freq_slice = slice(*freq_idx([freq_low, freq_high], fs, nfft, center=len(Zxx)//2))
ax1.pcolormesh(t, f_shifted[freq_slice], np.abs(Z_shifted[freq_slice]), shading='gouraud')
ax1.grid()

# FFT on selected samples
sample_slice = slice(*sample_idx([time_low, time_high], fs))
selected_samples = data[sample_slice]
selected_n = len(selected_samples)
X_shifted = fftshift(fft(selected_samples * hamming(selected_n)) / selected_n)
freqs_shifted = fftshift(fftfreq(selected_n, 1/fs))
ax1.axvspan(time_low, time_high, color = 'r', alpha=0.4)

# Plot FFT
freq_slice = slice(*freq_idx([freq_low, freq_high], fs, len(freqs_shifted), center=len(freqs_shifted)//2))
ax2.plot(abs(X_shifted[freq_slice]), freqs_shifted[freq_slice])
ax2.margins(0, tight=True)
ax2.grid()
fig.tight_layout()
113/19:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = Y * np.sin(X) - X * np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/20:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = np.sin(X) - np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/21:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = np.sin(X) - np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Z[i], X[i], Y[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/22:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = np.sin(X) - np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()
113/23:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = np.sin(X) - np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/24:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = np.sin(X) - np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=90)

plt.show()
113/25:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=90)

plt.show()
113/26:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/27:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(X[i], Y[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/28:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/29:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps
import numpy as np
from matplotlib.collections import PolyCollection

# Generate meshgrid for X and Y
X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

# Calculate Z values based on the function
Z = Y * np.sin(X) - X * np.cos(Y)

# Use the 'viridis' colormap
cmap = colormaps['viridis']

# Create figure and 3D axes
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Create a list to hold the polygons
polys = []

# Generate filled polygons for each line
for i in range(len(X)):
    # Create vertices for the polygon for each Y[i]
    verts = [(Y[i][j], X[i][j], Z[i][j]) for j in range(len(Y[i]))]
    verts.append((Y[i][-1], X[i][-1], 0))  # Close the polygon to the base
    verts.insert(0, (Y[i][0], X[i][0], 0))  # Close the polygon to the base
    polys.append(verts)

    # Plot the line (optional for visual clarity)
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color)

# Create a PolyCollection for filled areas
poly = PolyCollection(polys, facecolors=[cmap(i / len(X)) for i in range(len(X))], alpha=1)
ax.add_collection3d(poly, zs='z')

# Set labels for the axes
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# Rotate the view: Set elevation to 30 and azimuth to 45
ax.view_init(elev=30, azim=45)

# Show the plot
plt.show()
113/30:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps
import numpy as np
from matplotlib.collections import PolyCollection

# Generate meshgrid for X and Y
X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

# Calculate Z values based on the function
Z = Y * np.sin(X) - X * np.cos(Y)

# Use the 'viridis' colormap
cmap = colormaps['viridis']

# Create figure and 3D axes
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Create a list to hold the polygons
polys = []

# Generate filled polygons for each line
for i in range(len(X)):
    # Create vertices for the polygon for each Y[i]
    verts = [(Y[i][j], X[i][j], Z[i][j]) for j in range(len(Y[i]))]
    verts.append((Y[i][-1], X[i][-1]))  # Close the polygon to the base
    verts.insert(0, (Y[i][0], X[i][0]))  # Close the polygon to the base
    polys.append(verts)

    # Plot the line (optional for visual clarity)
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color)

# Create a PolyCollection for filled areas
poly = PolyCollection(polys, facecolors=[cmap(i / len(X)) for i in range(len(X))], alpha=1)
ax.add_collection3d(poly, zs='z')

# Set labels for the axes
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# Rotate the view: Set elevation to 30 and azimuth to 45
ax.view_init(elev=30, azim=45)

# Show the plot
plt.show()
113/31:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps
import numpy as np
from matplotlib.collections import PolyCollection

# Generate meshgrid for X and Y
X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

# Calculate Z values based on the function
Z = Y * np.sin(X) - X * np.cos(Y)

# Use the 'viridis' colormap
cmap = colormaps['viridis']

# Create figure and 3D axes
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Create a list to hold the polygons
polys = []

# Generate filled polygons for each line
for i in range(len(X)):
    # Create vertices for the polygon for each Y[i]
    verts = [(Y[i][j], X[i][j], Z[i][j]) for j in range(len(Y[i]))]
    verts.append((Y[i][-1], X[i][-1], 0))  # Close the polygon to the base
    verts.insert(0, (Y[i][0], X[i][0], 0))  # Close the polygon to the base
    polys.append(verts)

    # Plot the line (optional for visual clarity)
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color)

# Create a PolyCollection for filled areas
poly = PolyCollection(polys, facecolors=[cmap(i / len(X)) for i in range(len(X))], alpha=1)
ax.add_collection3d(poly, zs='z')

# Set labels for the axes
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# Rotate the view: Set elevation to 30 and azimuth to 45
ax.view_init(elev=30, azim=45)

# Show the plot
plt.show()
113/32:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/33:
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
import numpy as np
from matplotlib import colormaps

# Generate meshgrid for X and Y
X, Y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))

# Calculate Z values based on the function
Z = Y * np.sin(X) - X * np.cos(Y)

# Use the 'viridis' colormap
cmap = colormaps['viridis']

# Create figure and 3D axes
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Generate filled polygons for each line
for i in range(len(X)):
    # Create vertices for the polygon for each Y[i]
    verts = [(Y[i][j], X[i][j], Z[i][j]) for j in range(len(Y[i]))]
    verts.append((Y[i][-1], X[i][-1], 0))  # Close the polygon to the base
    verts.insert(0, (Y[i][0], X[i][0], 0))  # Close the polygon to the base

    # Create a Poly3DCollection for filled areas
    poly = Poly3DCollection([verts], color=cmap(i / len(X)), alpha=1)
    ax.add_collection3d(poly)

    # Plot the line (optional for visual clarity)
    ax.plot(Y[i], X[i], Z[i], color=cmap(i / len(X)))

# Set labels for the axes
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# Rotate the view: Set elevation to 30 and azimuth to 45
ax.view_init(elev=30, azim=45)

# Show the plot
plt.show()
113/34:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/35:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y) + np.random.random()

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/36:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y) + 100*np.random.random()

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/37:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i] + np.random.random(), color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/38:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i] + 5*np.random.random(), color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/39:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    for j in range(len(Y)):
        Z[i][j]+=5*np.random.random()
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/40:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    for j in range(len(Y)):
        Z[i][j]+=np.random.random()
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/41:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    for j in range(len(Y)):
        Z[i][j]+=5*np.random.random()
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/42:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colormaps
from mpl_toolkits.mplot3d import Axes3D

# Generate meshgrid for X and Y
X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

# Define the deterministic part of Z
Z = Y * np.sin(X) - X * np.cos(Y)

# Introduce randomness
# Define the noise level (adjust as needed for more or less randomness)
noise_level = 0.5
random_noise = noise_level * np.random.randn(*X.shape)

# Combine deterministic Z with random noise
Z += random_noise

# Use the 'viridis' colormap
cmap = colormaps['viridis']

# Create figure and 3D axes
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot each line with a unique color
for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color)

# Set labels for the axes
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# Rotate the view: Set elevation to 30 and azimuth to 45
ax.view_init(elev=30, azim=45)

# Show the plot
plt.show()
113/43:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import colormaps

X, Y = np.meshgrid(np.linspace(-5, 5, 1000), np.linspace(-5, 5, 1000))

Z = Y*np.sin(X) - X*np.cos(Y)

noise_level = 0.5
random_noise = noise_level * np.random.randn(*X.shape)

Z += random_noise

cmap = colormaps['viridis']

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

for i in range(len(X)):
    color = cmap(i / len(X))
    ax.plot(Y[i], X[i], Z[i], color=color) 

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

ax.view_init(elev=30, azim=45)

plt.show()
113/44:
from sklearn.datasets import load_breast_cancer
cancer=load_breast_cancer()
113/45: print(cancer)
113/46: print(cancer.head())
113/47: print(cancer.keys())
113/48:
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
df = pd.DataFrame(np.c_[cancer['data'], cancer['target']],
                  columns= np.append(cancer['feature_names'], ['target']))
113/49: df.head()
113/50:
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer

df = load_breast_cancer(as_frame=True)
df.frame
115/1:
n=1
m=3
mdp={'.'*(n*m): -2}
115/2:
n=1
m=3
mdp={'.'*(n*m): -2}
116/1:
from numpy import random
from scipy.stats import norm
116/2:
def sample(loc, scale):
    y = random()
    return norm.norminv(y, loc, scale)
116/3: y1=np.vectorize(sample)(array([0, 0]), array([1, 1]))
116/4: y1=vectorize(sample)(array([0, 0]), array([1, 1]))
116/5: y1=vectorize(sample)
116/6: y1=np.vectorize(sample)
116/7:
from numpy import random, array, vectorize
from scipy.stats import norm
import numpy as np
116/8: y1=vectorize(sample)
116/9: y1=vectorize(sample)(array(10**5))
116/10: y1=vectorize(lambda x: sample(0, 0.2))(array(10**5))
116/11: y1=vectorize(lambda x: sample(0, 0.2))
116/12: y1=y1(array(1))
116/13: y1=vectorize(lambda x: sample(0, 0.2))
116/14: y2=y1([1])
116/15:
def sample(loc, scale):
    y = random.random()
    return norm.norminv(y, loc, scale)
116/16: y2=y1([1])
116/17:
from numpy import random, array, vectorize
from scipy.stats import norminvgauss
import numpy as np
116/18:
def sample(loc, scale):
    y = random.random()
    return norminvgauss(y, loc, scale)
116/19: y1=vectorize(lambda x: sample(0, 0.2))
116/20: y2=y1([1])
116/21: y2
116/22: y2=np.array(10**5)
116/23: y2.size
116/24: y2
116/25: y2=np.array(size=1000)
116/26: y2=y1(np.zeros(100000))
116/27:
f1=vectorize(lambda x: sample(0, 0.2))
f2=vectorize(lambda x: sample(0, 1.0))
f3=vectorize(lambda x: sample(0, 5.0))
f4=vectorize(lambda x: sample(-2, 0.5))
116/28: y1=f1(np.zeros(100000))
116/29: y2=f2(np.zeros(100000))
116/30: y3=f3(np.zeros(100000))
116/31: y4=f4(np.zeros(100000))
116/32: from matplotlib import pyplot as plt
116/33: import matplotlib
116/34: import matplotlib
116/35: import matplotlib
117/1: import matplotlib
118/1: from matplotlib import pyplot as plt
118/2: fig, ax= plt.plot()
118/3: fig, ax= plt.subplots(1,1)
118/4:
fig, ax= plt.subplots(1,1)
ax.plot(y1, label='loc=0, scale=0.2')
plt.plot()
118/5:
from numpy import random, array, vectorize
from scipy.stats import norminvgauss
import numpy as np
118/6:
def sample(loc, scale):
    y = random.random()
    return norminvgauss(y, loc, scale)
118/7:
f1=vectorize(lambda x: sample(0, 0.2))
f2=vectorize(lambda x: sample(0, 1.0))
f3=vectorize(lambda x: sample(0, 5.0))
f4=vectorize(lambda x: sample(-2, 0.5))
118/8: y1=f1(np.zeros(100000))
118/9: y2=f2(np.zeros(100000))
118/10: y3=f3(np.zeros(100000))
118/11: y4=f4(np.zeros(100000))
118/12:
fig, ax= plt.subplots(1,1)
ax.plot(y1, label='loc=0, scale=0.2')
plt.plot()
118/13:
fig, ax= plt.subplots(1,1)
ax.plot(np.array(y1), label='loc=0, scale=0.2')
plt.plot()
118/14: y1
118/15: print(norminvgauss(0.5, 0, 0.2))
118/16: print(norminvgauss(0.5, 0, 0.2).var())
118/17: print(norminvgauss(0.5, 0, 0.2).var())
118/18: print(ppf(0.5, 0, 0.2))
118/19:
from numpy import random, array, vectorize
from scipy.stats import norm, norminvgauss
import numpy as np
118/20: print(norm.ppf(0.5, 0, 0.2))
118/21: print(norm.ppf(0.1, 0, 0.2))
118/22: print(norm.ppf(0.01, 0, 0.2))
118/23:
def sample(loc, scale):
    y = random.random()
    return norm.ppf(y, loc, scale)
118/24:
f1=vectorize(lambda x: sample(0, 0.2))
f2=vectorize(lambda x: sample(0, 1.0))
f3=vectorize(lambda x: sample(0, 5.0))
f4=vectorize(lambda x: sample(-2, 0.5))
118/25: y1=f1(np.zeros(100000))
118/26: y2=f2(np.zeros(100000))
118/27: y3=f3(np.zeros(100000))
118/28: y4=f4(np.zeros(100000))
118/29:
fig, ax= plt.subplots(1,1)
ax.plot(np.array(y1), label='loc=0, scale=0.2')
plt.plot()
118/30:
fig, ax= plt.subplots(1,1)
ax.hist(np.array(y1), label='loc=0, scale=0.2')
plt.plot()
118/31: sns.kdeplot(y1, label='loc=0, scale=0.2')
118/32:
from matplotlib import pyplot as plt
import seaborn as sns
118/33: sns.kdeplot(y1, label='loc=0, scale=0.2')
118/34:
fig, ax=sns.subplots()
sns.kdeplot(y1, label='loc=0, scale=0.2')
118/35:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax1, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax2, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax3, color='orange', label='N(-2, 0.5)')
118/36:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='orange', label='N(-2, 0.5)')
118/37:
from numpy import random, array, vectorize
from scipy.stats import norm, norminvgauss
import numpy as np
from math import sqrt
118/38:
def sample(loc, scale):
    y = random.random()
    return norm.ppf(y, loc, sqrt(scale))
118/39:
f1=vectorize(lambda x: sample(0, 0.2))
f2=vectorize(lambda x: sample(0, 1.0))
f3=vectorize(lambda x: sample(0, 5.0))
f4=vectorize(lambda x: sample(-2, 0.5))
118/40: y1=f1(np.zeros(100000))
118/41: y2=f2(np.zeros(100000))
118/42: y3=f3(np.zeros(100000))
118/43: y4=f4(np.zeros(100000))
118/44:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='orange', label='N(-2, 0.5)')
119/1:
from numpy import random, array, vectorize
from scipy.stats import norm, norminvgauss
import numpy as np
from math import sqrt
119/2:
def sample(loc, scale):
    y = random.random()
    return norm.ppf(y, loc, sqrt(scale))
119/3:
f1=vectorize(lambda x: sample(0, 0.2))
f2=vectorize(lambda x: sample(0, 1.0))
f3=vectorize(lambda x: sample(0, 5.0))
f4=vectorize(lambda x: sample(-2, 0.5))
119/4: y1=f1(np.zeros(100000))
119/5: y2=f2(np.zeros(100000))
119/6: y3=f3(np.zeros(100000))
119/7: y4=f4(np.zeros(100000))
119/8:
from matplotlib import pyplot as plt
import seaborn as sns
119/9:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='orange', label='N(-2, 0.5)')
119/10: plt.savefig('2c.png')
119/11:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='orange', label='N(-2, 0.5)')
plt.legend()
119/12:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='orange', label='N(-2, 0.5)')
sns.legend()
119/13:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='green', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='orange', label='N(-2, 0.5)')
ax.legend()
119/14:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='\mu=0, \sigma^2=0.2')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='yellow', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.legend()
119/15:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='yellow', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.legend()
119/16: plt.savefig('2c.png')
119/17:
fig, ax=plt.subplots()
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.legend()
119/18: plt.savefig('2c.png')
119/19:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
ax1=ax.twinx()
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
ax2=ax.twinx()
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
ax3=ax.twinx()
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.set_xlim(-5, 5)
ax.legend()
119/20:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, 
                               FormatStrFormatter, 
                               AutoMinorLocator
import seaborn as sns
119/21:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter, 
import seaborn as sns
119/22:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style('whitegrid')
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.set_xlim(-5, 5)
ax.legend()
119/23:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style('whitegrid')
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
sns.xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
119/24:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style("darkgrid")
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
sns.xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
119/25:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style("darkgrid")
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
119/26:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style("darkgrid")
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.set_xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
119/27: plt.savefig('2c.png')
120/1: import numpy as np
120/2:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,1) else -1
120/3:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,1) else -1
    return result
120/4: simulate_galton(10)
120/5:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,1) else 0
    return result
120/6: simulate_galton(10)
120/7: simulate_galton(10)
120/8:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,2) else 0
    return result
120/9: simulate_galton(10)
120/10: simulate_galton(10)
120/11:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,2) else 0
    return result
120/12: simulate_galton(10)
120/13: simulate_galton(10)
120/14: simulate_galton(10)
120/15: simulate_galton(10)
120/16: simulate_galton(10)
120/17: simulate_galton(10)
120/18: simulate_galton(10)
120/19: simulate_galton(10)
120/20:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,2) else 0
    return result
120/21: y1=np.vectorize(lambda x: simulate_galton(10))
120/22:
y1=np.vectorize(lambda x: simulate_galton(10))
y2=np.vectorize(lambda x: simulate_galton(50))
y3=np.vectorize(lambda x: simulate_galton(100))
120/23:
f1=np.vectorize(lambda x: simulate_galton(10))
f2=np.vectorize(lambda x: simulate_galton(50))
f3=np.vectorize(lambda x: simulate_galton(100))
120/24: y1=f1(np.zeros(1000))
120/25: y1
120/26: y2=f2(np.zeros(1000))
120/27: y3=f3(np.zeros(1000))
119/28:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
119/29:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style("darkgrid")
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.set_xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
120/28:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
120/29:
fig, ax=plt.subplots()
ax.hist(y1, bins=range(11), alpha=0.5, label='h=10')
120/30: ax.hist(y2, bins=range(51), alpha=0.5, label='h=50')
120/31:
ax2=ax.twinx()
ax2.hist(y2, bins=range(51), alpha=0.5, label='h=50')
120/32:
fig, ax=plt.subplots()
ax.hist(y2, bins=range(11), alpha=0.5, label='h=10')
120/33: y2
120/34:
fig, ax=plt.subplots()
ax.hist(y2, bins=range(51), alpha=0.5, label='h=10')
120/35:
fig, ax=plt.subplots()
ax.hist(y1, bins=range(11), alpha=0.9, label='h=10')
120/36:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(11), alpha=0.9, label='h=10')
120/37: y1.min(), y1.max()
120/38: plt.savefig('2d1.png')
120/39:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y2, bins=range(51), alpha=0.9, label='h=50')
120/40: y2.min(), y2.max()
120/41: plt.savefig('2d2.png')
120/42:
fig, ax=plt.subplots()
# ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y3, bins=range(101), alpha=0.9, label='h=100')
120/43: y3.min(), y3.max()
120/44: plt.savefig('2d3.png')
119/30:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
119/31:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style("darkgrid")
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.set_xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
119/32: sns.savefig('2c.png')
119/33: fig.savefig('2c.png')
120/45:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(11), alpha=0.9, label='h=10')
120/46: fig.savefig('2d1.png')
120/47:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y2, bins=range(51), alpha=0.9, label='h=50')
120/48: fig.savefig('2d2.png')
120/49:
fig, ax=plt.subplots()
# ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y3, bins=range(101), alpha=0.9, label='h=100')
120/50: fig.savefig('2d3.png')
120/51:
fig, ax=plt.subplots()
# ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y2, bins=range(51), alpha=0.9, label='h=50')
120/52: fig.savefig('2d2.png')
120/53:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,2) else -1
    return result
120/54:
f1=np.vectorize(lambda x: simulate_galton(10))
f2=np.vectorize(lambda x: simulate_galton(50))
f3=np.vectorize(lambda x: simulate_galton(100))
120/55: y1=f1(np.zeros(1000))
120/56: y2=f2(np.zeros(1000))
120/57: y3=f3(np.zeros(1000))
120/58:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
120/59:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(11), alpha=0.9, label='h=10')
120/60: y1.min()
120/61:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 10), alpha=0.9, label='h=10')
120/62:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 11), alpha=0.9, label='h=10')
120/63: y1.max()
120/64: y1.count(9)
120/65: np.count(y1==9)
120/66: np.array.count(y1==9)
120/67: print(len(y1[y1==9]))
120/68:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 12), alpha=0.9, label='h=10')
120/69:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 12, 2), alpha=0.9, label='h=10')
120/70:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(2))
ax.hist(y1, bins=range(-10, 12, 2), alpha=0.9, label='h=10')
120/71: print(len(y1[y1==-10]))
120/72: fig.savefig('2d1.png')
120/73:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(4))
ax.hist(y2, bins=range(-50, 52, 2), alpha=0.9, label='h=50')
120/74:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(8))
ax.hist(y2, bins=range(-50, 52, 2), alpha=0.9, label='h=50')
120/75: fig.savefig('2d2.png')
120/76:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(2))
ax.hist(y1, bins=range(-10, 12), alpha=0.9, label='h=10')
120/77:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 12), alpha=0.9, label='h=10')
120/78: fig.savefig('2d1.png')
120/79:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(8))
ax.hist(y2, bins=range(-100, 102, 2), alpha=0.9, label='h=100')
120/80:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(8))
ax.hist(y3, bins=range(-100, 102, 2), alpha=0.9, label='h=100')
120/81:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 2), alpha=0.9, label='h=100')
120/82:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 1), alpha=0.9, label='h=100')
120/83:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(8))
ax.hist(y2, bins=range(-50, 52, 1), alpha=0.9, label='h=50')
120/84: fig.savefig('2d2.png')
120/85:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 2), alpha=0.9, label='h=100')
120/86: fig.savefig('2d3.png')
120/87:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 1), alpha=0.9, label='h=100')
120/88: fig.savefig('2d3.png')
121/1:
from numpy import random, array, vectorize
from scipy.stats import norm, norminvgauss
import numpy as np
from math import sqrt
121/2:
def sample(loc, scale):
    y = random.random()
    return norm.ppf(y, loc, sqrt(scale))
121/3:
f1=vectorize(lambda x: sample(0, 0.2))
f2=vectorize(lambda x: sample(0, 1.0))
f3=vectorize(lambda x: sample(0, 5.0))
f4=vectorize(lambda x: sample(-2, 0.5))
121/4: y1=f1(np.zeros(100000))
121/5: y2=f2(np.zeros(100000))
121/6: y3=f3(np.zeros(100000))
121/7: y4=f4(np.zeros(100000))
121/8:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
121/9:
fig, ax=plt.subplots()
# x=np.linspace(-5, 5, 1000)
sns.set_style("darkgrid")
ax.xaxis.set_major_locator(plt.MultipleLocator(1))
ax.xaxis.set_minor_locator(plt.MultipleLocator(0.1))
sns.kdeplot(y1, ax=ax, label='N(0, 0.2)')
sns.kdeplot(y2, ax=ax, color='red', label='N(0, 1.0)')
sns.kdeplot(y3, ax=ax, color='orange', label='N(0, 5.0)')
sns.kdeplot(y4, ax=ax, color='green', label='N(-2, 0.5)')
ax.set_xlabel('X')
ax.set_xlim(-5, 5)
ax.legend()
121/10: fig.savefig('../images/2c.png')
122/1: import numpy as np
122/2:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,2) else -1
    return result
122/3:
f1=np.vectorize(lambda x: simulate_galton(10))
f2=np.vectorize(lambda x: simulate_galton(50))
f3=np.vectorize(lambda x: simulate_galton(100))
122/4: y1=f1(np.zeros(1000))
122/5: y2=f2(np.zeros(1000))
122/6: y3=f3(np.zeros(1000))
122/7:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
122/8:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 12), alpha=0.9, label='h=10')
122/9: fig.savefig('../images/2d1.png')
122/10:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(8))
ax.hist(y2, bins=range(-50, 52, 1), alpha=0.9, label='h=50')
122/11: fig.savefig('../images/2d2.png')
122/12:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 2), alpha=0.9, label='h=100')
122/13: fig.savefig('../images/2d3.png')
122/14:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 1), alpha=0.9, label='h=100')
122/15: fig.savefig('../images/2d3.png')
123/1: import numpy as np
123/2:
def simulate_galton(h):
    result=0
    for _ in range(h):
        result+=1 if np.random.randint(0,2) else -1
    return result
123/3:
f1=np.vectorize(lambda x: simulate_galton(10))
f2=np.vectorize(lambda x: simulate_galton(50))
f3=np.vectorize(lambda x: simulate_galton(100))
123/4: y1=f1(np.zeros(1000))
123/5: y2=f2(np.zeros(1000))
123/6: y3=f3(np.zeros(1000))
123/7:
from matplotlib import pyplot as plt
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
import seaborn as sns
123/8:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(1))
ax.hist(y1, bins=range(-10, 12), alpha=0.9, label='h=10')
123/9: fig.savefig('../images/2d1.png')
123/10:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(8))
ax.hist(y2, bins=range(-50, 52, 1), alpha=0.9, label='h=50')
123/11: fig.savefig('../images/2d2.png')
123/12:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 2), alpha=0.9, label='h=100')
123/13: fig.savefig('../images/2d3.png')
123/14:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 1), alpha=0.9, label='h=100')
123/15:
fig, ax=plt.subplots()
ax.xaxis.set_major_locator(MultipleLocator(16))
ax.hist(y3, bins=range(-100, 102, 1), alpha=0.9, label='h=100')
123/16: fig.savefig('../images/2d3.png')
124/1: from selenium import webdriver
124/2:
from selenium import webdriver
from bs4 import BeautifulSoup
124/3: driver=webdriver.Chrome()
124/4: url='https://www.ambitionbox.com/list-of-companies?indianEmployeeCounts=1001-5000,5001-10000,10001-50000&industries=it-services-and-consulting,financial-services,engineering-and-construction,industrial-machinery,software-product,healthcare,auto-components,real-estate,internet,logistics,chemicals,food-and-beverages,automobile,iron-or-steel,fmcg,telecom,electrical-equipment,pharma,retail,consumer-electronics-and-appliances,mining-and-metals,insurance,oil-and-gas,fintech,agro-chemicals,edtech,analytics-and-kpo,electronics-manufacturing,semiconductors,non-profit,media-and-entertainment,banking,aviation,beverage,packaging,management-consulting,agriculture,biotechnology,defence,design,investment-banking-or-venture-capital-or-private-equity,waste-management,architecture-and-interior-design,emerging-technologies,industrial-automation,chemicals-or-agri-inputs&sortBy=popular'
124/5: driver.get(url)
124/6: driver.quit()
124/7:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
124/8: url='https://www.ambitionbox.com/list-of-companies?indianEmployeeCounts=1001-5000,5001-10000,10001-50000&industries=it-services-and-consulting,financial-services,engineering-and-construction,industrial-machinery,software-product,healthcare,auto-components,real-estate,internet,logistics,chemicals,food-and-beverages,automobile,iron-or-steel,fmcg,telecom,electrical-equipment,pharma,retail,consumer-electronics-and-appliances,mining-and-metals,insurance,oil-and-gas,fintech,agro-chemicals,edtech,analytics-and-kpo,electronics-manufacturing,semiconductors,non-profit,media-and-entertainment,banking,aviation,beverage,packaging,management-consulting,agriculture,biotechnology,defence,design,investment-banking-or-venture-capital-or-private-equity,waste-management,architecture-and-interior-design,emerging-technologies,industrial-automation,chemicals-or-agri-inputs&sortBy=popular'
124/9: driver.get(url)
124/10: driver.get(url)
126/1:
import random as rndm
import time
126/2:
def make_gene(initial=None):
    if initial is None:
        initial = [0] * 9
    mapp = {}
    gene = list(range(1, 10))
    rndm.shuffle(gene)
    for i in range(9):
        mapp[gene[i]] = i
    for i in range(9):
        if initial[i] != 0 and gene[i] != initial[i]:
            temp = gene[i], gene[mapp[initial[i]]]
            gene[mapp[initial[i]]], gene[i] = temp
            mapp[initial[i]], mapp[temp[0]] = i, mapp[initial[i]]
    return gene
126/3:
def make_chromosome(initial=None):
    if initial is None:
        initial = [[0] * 9] * 9
    chromosome = []
    for i in range(9):
        chromosome.append(make_gene(initial[i]))
    return chromosome
126/4:
def make_population(count, initial=None):
    if initial is None:
        initial = [[0] * 9] * 9
    population = []
    for _ in range(count):
        population.append(make_chromosome(initial))
    return population
126/5:
def get_fitness(chromosome):
    """Calculate the fitness of a chromosome (puzzle)."""
    fitness = 0
    for i in range(9): # For each column
        seen = {}
        for j in range(9): # Check each cell in the column
            if chromosome[j][i] in seen:
                seen[chromosome[j][i]] += 1
            else:
                seen[chromosome[j][i]] = 1
        for key in seen: # Subtract fitness for repeated numbers
            fitness -= (seen[key] - 1)
    for m in range(3): # For each 3x3 square
        for n in range(3):
            seen = {}
            for i in range(3 * n, 3 * (n + 1)):  # Check cells in 3x3 square
                for j in range(3 * m, 3 * (m + 1)):
                    if chromosome[j][i] in seen:
                        seen[chromosome[j][i]] += 1
                    else:
                        seen[chromosome[j][i]] = 1
            for key in seen: # Subtract fitness for repeated numbers
                fitness -= (seen[key] - 1)
    return fitness
126/6:
ch = make_chromosome()
print(get_fitness(ch))


def pch(ch):
    for i in range(9):
        for j in range(9):
            print(ch[i][j], end=" ")
        print("")
126/7:
def crossover(ch1, ch2):
    new_child_1 = []
    new_child_2 = []
    for i in range(9):
        x = rndm.randint(0, 1)
        if x == 1:
            new_child_1.append(ch1[i])
            new_child_2.append(ch2[i])
        elif x == 0:
            new_child_2.append(ch1[i])
            new_child_1.append(ch2[i])
    return new_child_1, new_child_2
126/8:
def mutation(ch, pm, initial):
    for i in range(9):
        x = rndm.randint(0, 100)
        if x < pm * 100:
            ch[i] = make_gene(initial[i])
    return ch
126/9:
def read_puzzle(address):
    puzzle = []
    f = open(address, 'r')
    for row in f:
        temp = row.split()
        puzzle.append([int(c) for c in temp])
    return puzzle
126/10:
def r_get_mating_pool(population):
    fitness_list = []
    pool = []
    for chromosome in population:
        fitness = get_fitness(chromosome)
        fitness_list.append((fitness, chromosome))
    fitness_list.sort()
    weight = list(range(1, len(fitness_list) + 1))
    for _ in range(len(population)):
        ch = rndm.choices(fitness_list, weight)[0]
        pool.append(ch[1])
    return pool
126/11:
def w_get_mating_pool(population):
    fitness_list = []
    pool = []
    for chromosome in population:
        fitness = get_fitness(chromosome)
        fitness_list.append((fitness, chromosome))
    weight = [fit[0] - fitness_list[0][0] for fit in fitness_list]
    for _ in range(len(population)):
        ch = rndm.choices(fitness_list, weights=weight)[0]
        pool.append(ch[1])
    return pool
126/12:
def get_offsprings(population, initial, pm, pc):
    new_pool = []
    i = 0
    while i < len(population):
        ch1 = population[i]
        ch2 = population[(i + 1) % len(population)]
        x = rndm.randint(0, 100)
        if x < pc * 100:
            ch1, ch2 = crossover(ch1, ch2)
        new_pool.append(mutation(ch1, pm, initial))
        new_pool.append(mutation(ch2, pm, initial))
        i += 2
    return new_pool
126/13:
# Population size
POPULATION = 1000

# Number of generations
REPETITION = 1000

# Probability of mutation
PM = 0.1

# Probability of crossover
PC = 0.95

# Main genetic algorithm function
def genetic_algorithm(initial_file):
    initial = read_puzzle(initial_file)
    population = make_population(POPULATION, initial)
    for _ in range(REPETITION):
        mating_pool = r_get_mating_pool(population)
        rndm.shuffle(mating_pool)
        population = get_offsprings(mating_pool, initial, PM, PC)
        fit = [get_fitness(c) for c in population]
        m = max(fit)
        if m == 0:
            return population
    return population
126/14:
tic = time.time()
r = genetic_algorithm("./sample_sudoku/Test2.txt")
toc = time.time()
print("time_taken: ", toc - tic)
fit = [get_fitness(c) for c in r]
m = max(fit)
print(max(fit))

# Print the chromosome with the highest fitness
for c in r:
    if get_fitness(c) == m:
        pch(c)
        break
126/15:
# Population size
POPULATION = 1000

# Number of generations
REPETITION = 1000

# Probability of mutation
PM = 0.1

# Probability of crossover
PC = 0.95

# Main genetic algorithm function
def genetic_algorithm(initial_file):
    initial = read_puzzle(initial_file)
    population = make_population(POPULATION, initial)
    for _ in range(REPETITION):
        mating_pool = r_get_mating_pool(population)
        rndm.shuffle(mating_pool)
        population = get_offsprings(mating_pool, initial, PM, PC)
        fit = [get_fitness(c) for c in population]
        m = max(fit)
        print(m)
        if m == 0:
            return population
    return population
126/16:
tic = time.time()
r = genetic_algorithm("./sample_sudoku/Test2.txt")
toc = time.time()
print("time_taken: ", toc - tic)
fit = [get_fitness(c) for c in r]
m = max(fit)
print(max(fit))

# Print the chromosome with the highest fitness
for c in r:
    if get_fitness(c) == m:
        pch(c)
        break
126/17:
tic = time.time()
r = genetic_algorithm("./sample_sudoku/Test2.txt")
toc = time.time()
print("time_taken: ", toc - tic)
fit = [get_fitness(c) for c in r]
m = max(fit)
print(max(fit))

# Print the chromosome with the highest fitness
for c in r:
    if get_fitness(c) == m:
        pch(c)
        break
128/1: import numpy as np
128/2:
import numpy as np 
import pandas as pd
128/3:
import numpy as np 
from enum import Enum
import pandas as pd
128/4:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
128/5:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        self.X = X
        self.Y = Y

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)
    
    def display():
        pass
128/6:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
128/7:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        self.X = X
        self.Y = Y

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self):
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
128/8: data=pd.read_csv('data.csv')
128/9: data=pd.read_csv('data.csv')
128/10: data.head()
128/11: X=data['Al']
128/12: Y=data['RI']
128/13: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/14: regressor.fit(X, Y)
128/15: regressor.display()
128/16:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        self.X = np.array(X)
        self.Y = np.array(Y)

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
128/17: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/18: regressor.fit(X, Y)
128/19: regressor.display()
128/20: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/21: regressor.fit(X, Y)
128/22: regressor.display()
128/23:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        # Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, self.Y, color='red')
128/24: regressor.display()
128/25: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/26: regressor.fit(X, Y)
128/27: regressor.display()
128/28:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
128/29: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/30: regressor.fit(X, Y)
128/31: regressor.display()
128/32: regressor.display(1)
128/33: regressor.display(0.1)
128/34: regressor.display(0.01)
128/35: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/36: regressor.plot_cross_validator()
128/37: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/38: regressor.fit(X, Y)
128/39: regressor.plot_cross_validator()
128/40: regressor.plot_cross_validator()
128/41:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        for i in range(n):
            temp_X = np.delete(self.X, i)
            temp_Y = np.delete(self.Y, i)
            self.fit(temp_X, temp_Y)
            
            y_pred = self.predict(self.X[i])
            weight_i = self.evaluate_kernel(0)

            risk += ((self.Y[i] - y_pred) / (1 - weight_i)) ** 2

        return risk / n
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        risks = np.vectorize(self.return_risk)(bandwidths)
        plt.plot(bandwidths, risks)
128/42: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/43: regressor.plot_cross_validator()
128/44: regressor.fit(X, Y)
128/45: regressor.plot_cross_validator()
128/46:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        for i in range(n):
            temp_X = np.delete(self.X, i)
            temp_Y = np.delete(self.Y, i)
            self.fit(temp_X, temp_Y)
            
            y_pred = self.predict(self.X[i])
            weight_i = self.evaluate_kernel(0)

            risk += ((self.Y[i] - y_pred) / (1 - weight_i)) ** 2

        return risk / n
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        plt.plot(bandwidths, risks)
128/47: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/48: regressor.fit(X, Y)
128/49: regressor.plot_cross_validator()
128/50:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        for i in range(n):
            temp_X = np.delete(self.X, i)
            temp_Y = np.delete(self.Y, i)
            self.fit(temp_X, temp_Y)
            
            y_pred = self.predict(self.X[i])
            weight_i = self.evaluate_kernel(0)

            risk += ((self.Y[i] - y_pred) / (1 - weight_i)) ** 2

        self.fit(self.X, self.Y)
        return risk / n
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        plt.plot(bandwidths, risks)
128/51: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/52: regressor.fit(X, Y)
128/53: regressor.plot_cross_validator()
128/54:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        plt.plot(bandwidths, risks)
128/55: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/56: regressor.fit(X, Y)
128/57: regressor.plot_cross_validator()
128/58:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        plt.plot(bandwidths, risks)
128/59: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/60: regressor.fit(X, Y)
128/61: regressor.plot_cross_validator()
128/62:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        plt.plot(bandwidths, risks)
128/63: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/64: regressor.fit(X, Y)
128/65: regressor.plot_cross_validator()
128/66:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0
        
        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        plt.plot(bandwidths, risks)
128/67: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/68: regressor.fit(X, Y)
128/69: regressor.plot_cross_validator()
128/70:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        risks = vectorized_return_risk(bandwidths)
        print(bandwidths[np.argmin(risks)])
        plt.plot(bandwidths, risks)
128/71: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/72: regressor.fit(X, Y)
128/73: regressor.plot_cross_validator()
128/74:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/75: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/76: regressor.fit(X, Y)
128/77: regressor.plot_cross_validator()
128/78: print(regressor.risks)
128/79: np.argmin(regressor.risks)
128/80: np.argmin(regressor.risks[1:])
128/81: regressor.bandwidths[np.argmin(regressor.risks[1:])+1]
128/82: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/83: regressor.fit(X, Y)
128/84: regressor.plot_cross_validator()
128/85: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.EPANECHNIKOV)
128/86: regressor.fit(X, Y)
128/87:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
128/88:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X == None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/89: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.EPANECHNIKOV)
128/90: regressor.fit(X, Y)
128/91: regressor.plot_cross_validator()
128/92: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/93: regressor.fit(X, Y)
128/94: regressor.display(0.13)
128/95: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/96: regressor.fit(X, Y)
128/97: regressor.display(0.13)
128/98:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/99: regressor.display(0.13)
128/100: regressor.display(0.13)
128/101:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = []
        self.Y = []

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if len(self.X)==0:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if len(self.X)==0:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/102: regressor.display(0.13)
128/103:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/104: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/105: regressor.fit(X, Y)
128/106: regressor.display(0.13)
128/107: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.EPANECHNIKOV)
128/108: regressor.display(0.25)
128/109: regressor.fit(X, Y)
128/110: regressor.display(0.25)
128/111:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        X = np.linspace(-10, 10, 1000)
        Y_predict=vectorized_predictor(X)
        plt.plot(X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/112: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/113: regressor.fit(X, Y)
128/114: regressor.display(0.13)
128/115:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        X = np.linspace(-20, 20, 1000)
        Y_predict=vectorized_predictor(X)
        plt.plot(X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/116: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/117: regressor.fit(X, Y)
128/118: regressor.display(0.13)
128/119:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        X = np.linspace(-20, 2000, 10000)
        Y_predict=vectorized_predictor(X)
        plt.plot(X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/120: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/121: regressor.fit(X, Y)
128/122: regressor.display(0.13)
128/123: regressor.display(0.13)
128/124:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        X = np.linspace(-20, 2000, 10000)
        Y_predict=vectorized_predictor(X)
        plt.plot(X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/125: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/126: regressor.fit(X, Y)
128/127: regressor.display(0.13)
128/128: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/129: regressor.fit(X, Y)
128/130:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        X = np.linspace(-20, 2000, 10000)
        Y_predict=vectorized_predictor(X)
        plt.plot(X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/131: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/132: regressor.fit(X, Y)
128/133: regressor.display(0.13)
128/134:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 2, 200)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/135: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/136: regressor.fit(X, Y)
128/137: regressor.display(0.13)
128/138: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/139: regressor.fit(X, Y)
128/140: regressor.display()
128/141: regressor.display(1)
128/142: regressor.display(0.1)
128/143: regressor.display(0.01)
128/144: regressor.plot_cross_validator()
128/145:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.001, 0.04, 400)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/146: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/147: regressor.fit(X, Y)
128/148: regressor.plot_cross_validator()
128/149: print(regressor.risks)
128/150:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self):
        bandwidths = np.linspace(0.01, 0.04, 40)
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/151:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/152: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/153: regressor.fit(X, Y)
128/154: regressor.plot_cross_validator(np.linspace(0.01, 0.04, 4))
128/155: print(regressor.risks)
128/156:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
128/157:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
128/158:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/159: data=pd.read_csv('data.csv')
128/160: data.head()
128/161: X=data['Al']
128/162: Y=data['RI']
128/163: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/164: regressor.fit(X, Y)
128/165: regressor.display()
128/166: regressor.display(1)
128/167: regressor.display(0.1)
128/168: regressor.display(0.01)
128/169: regressor.plot_cross_validator(np.linspace(0.01, 0.04, 4))
128/170: regressor.display(0.13)
128/171: print(regressor.risks)
128/172: print(regressor.risks)
129/1:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
129/2:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
128/173:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/174: regressor.plot_cross_validator(np.linspace(0.01, 0.04, 4))
128/175:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/176: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/177: regressor.fit(X, Y)
128/178: regressor.plot_cross_validator(np.linspace(0.01, 0.04, 4))
128/179: regressor.plot_cross_validator(np.linspace(0.01, 0.4, 40))
128/180:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)
128/181: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/182: regressor.fit(X, Y)
128/183: regressor.plot_cross_validator(np.linspace(0.01, 0.4, 40))
128/184: regressor.plot_cross_validator(np.linspace(0.01, 2, 200))
128/185: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.EPANECHNIKOV)
128/186: regressor.fit(X, Y)
128/187: regressor.plot_cross_validator(np.linspace(0.01, 2, 200))
130/1:
import pandas as pd 
import sklearn
130/2: data=pd.read_csv('data.csv')
130/3: X=data['Al']
130/4: Y=data['RI']
130/5: from sklearn.neighbors import KernelDensity
130/6: kde=KernelDensity(kernel='tophat', bandwidth=0.75).fit((X, Y))
130/7:
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import train_test_split
130/8:
import pandas as pd 
import sklearn
import numpy as np
130/9:
param_grid = {
    'bandwidth': np.linspace(1e-3, 1, 30),
    'kernel': ['tophat']
}
130/10:
from sklearn.neighbors import KernelDensity
from sklearn.grid_search import GridSearchCV
from sklearn.model_selection import train_test_split
130/11:
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV
130/12:
param_grid = {
    'bandwidth': np.linspace(1e-3, 1, 30),
}
130/13: kde=KernelDensity(kernel='gaussian')
130/14: kde=KernelDensity(kernel='tophat')
130/15:
grid_search = GridSearchCV(kde, param_grid, cv=5)
grid_search.fit(X)
130/16:
best_bandwidth = grid_search.best_params_['bandwidth']
print(f"Best bandwidth: {best_bandwidth}")
130/17:
grid_search = GridSearchCV(kde, param_grid, cv=5)
grid_search.fit((X, Y))
130/18: X=data['Al', 'RI']
130/19: X=data['Al']
130/20: Y=data['RI']
130/21:
grid_search = GridSearchCV(kde, param_grid, cv=5)
grid_search.fit(X=X, y=Y)
130/22:
param_grid = {
    'bandwidth': np.linspace(0.01, 1, 100),
}
130/23: kde=KernelDensity(kernel='gaussian')
130/24:
grid_search = GridSearchCV(kde, param_grid, cv=5)
grid_search.fit(X=X, y=Y)
130/25:
grid_search = GridSearchCV(kde, param_grid, cv=5)
grid_search.fit(X=X)
130/26:
grid_search = GridSearchCV(kde, param_grid, cv=5)
X.reshape(-1,1)
grid_search.fit(X=X)
130/27:
grid_search = GridSearchCV(kde, param_grid, cv=5)
X=np.array(X).reshape(-1,1)
grid_search.fit(X=X)
130/28:
best_bandwidth = grid_search.best_params_['bandwidth']
print(f"Best bandwidth: {best_bandwidth}")
130/29: print(grid_search.grid_scores_)
130/30:
means = grid_search.cv_results_['mean_test_score']
params = grid_search.cv_results_['params']
for mean, param in zip(means, params):
    print(f"Mean test score: {mean:.3f}, with parameter: {param}")
130/31:
means = grid_search.cv_results_['mean_test_score']
params = grid_search.cv_results_['params']
result_test_score= np.array(grid_search.cv_results_['mean_test_score'])
parameters = np.array([param['bandwidth'] for param in params])
plt.plot(parameters, result_test_score)
130/32: from matplotlib import pyplot as plt
130/33:
means = grid_search.cv_results_['mean_test_score']
params = grid_search.cv_results_['params']
result_test_score= np.array(grid_search.cv_results_['mean_test_score'])
parameters = np.array([param['bandwidth'] for param in params])
plt.plot(parameters, result_test_score)
130/34:
means = grid_search.cv_results_['mean_test_score']
params = grid_search.cv_results_['params']
result_test_score= np.array(grid_search.cv_results_['mean_test_score'])
parameters = np.array([param['bandwidth'] for param in params])
plt.plot(parameters, np.exp(-result_test_score))
130/35:
means = grid_search.cv_results_['mean_test_score']
params = grid_search.cv_results_['params']
result_test_score= np.array(grid_search.cv_results_['mean_test_score'])
parameters = np.array([param['bandwidth'] for param in params])
plt.plot(parameters, -result_test_score)
130/36:
best_bandwidth = grid_search.best_params_['bandwidth']
print(f"Best bandwidth: {best_bandwidth}")
128/188: tecator_data=pd.read_csv('tecator.csv')
128/189: tecator_data.head()
128/190: tecator_data=pd.read_csv('tecator.csv', header=None)
128/191: tecator_data.head()
128/192: x_tecator=tecator_data[0]
128/193: x_tecator
128/194: y_tecator=tecator_data[-2]
128/195: y_tecator=tecator_data[len(tecator_data)-2]
128/196: y_tecator=tecator_data[len(tecator_data.columns)-2]
128/197: regressor.fit(x_tecator, y_tecator)
128/198: regressor.plot_cross_validator(np.linspace(0.01, 2, 200))
128/199: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.SQUARE)
128/200: regressor.plot_cross_validator(np.linspace(0.01, 2, 200))
128/201: regressor.fit(x_tecator, y_tecator)
128/202: regressor.plot_cross_validator(np.linspace(0.01, 2, 200))
128/203: regressor.display(0.10)
128/204: regressor.display(0.25)
128/205: regressor.display(0.10)
128/206:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax1.plot(self.X, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.scatter(self.X, self.Y)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax2.plot(self.X, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.scatter(self.X, self.Y)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax3.plot(self.X, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/207: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/208: regressor.fit(X, Y)
128/209: regressor.display_4_plots(0.01, 0.1, 0.13)
128/210: regressor.display_4_plots(0.01, 1, 0.13)
128/211:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=0.05)
        ax1.title('Undersmoothed')
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax1.plot(self.X, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.title('Over smoothed')
        ax2.scatter(self.X, self.Y, s=0.05)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax2.plot(self.X, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.title('Correct Smoothing')
        ax3.scatter(self.X, self.Y, s=0.05)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax3.plot(self.X, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/212: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/213: regressor.fit(X, Y)
128/214: regressor.display_4_plots(0.01, 1, 0.13)
128/215:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=0.05)
        ax1.set_title('Undersmoothed')
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax1.plot(self.X, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over smoothed')
        ax2.scatter(self.X, self.Y, s=0.05)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax2.plot(self.X, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correct Smoothing')
        ax3.scatter(self.X, self.Y, s=0.05)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax3.plot(self.X, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/216: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/217: regressor.fit(X, Y)
128/218: regressor.display_4_plots(0.01, 1, 0.13)
128/219:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=0.5)
        ax1.set_title('Undersmoothed')
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax1.plot(self.X, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over smoothed')
        ax2.scatter(self.X, self.Y, s=0.5)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax2.plot(self.X, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correct Smoothing')
        ax3.scatter(self.X, self.Y, s=0.5)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax3.plot(self.X, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/220: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/221: regressor.fit(X, Y)
128/222: regressor.display_4_plots(0.01, 1, 0.13)
128/223:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=0.5)
        ax1.set_title('Under Smoothened')
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax1.plot(self.X, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=0.5)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax2.plot(self.X, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=0.5)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax3.plot(self.X, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/224: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/225: regressor.fit(X, Y)
128/226: regressor.display_4_plots(0.01, 1, 0.13)
128/227:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=0.5)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax1.plot(self.X, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=0.5)
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax2.plot(self.X, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=0.5)
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        ax3.plot(self.X, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/228: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/229: regressor.fit(X, Y)
128/230: regressor.display_4_plots(0.01, 1, 0.13)
128/231:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=5
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/232: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/233: regressor.fit(X, Y)
128/234: regressor.display_4_plots(0.01, 1, 0.13)
128/235:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax1.set_xlim(-1, 5)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/236: regressor.display_4_plots(0.05, 0.5, 0.13)
128/237: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/238: regressor.fit(X, Y)
128/239: regressor.display_4_plots(0.05, 0.5, 0.13)
128/240:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(15, 15)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/241: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/242: regressor.fit(X, Y)
128/243: regressor.display_4_plots(0.05, 0.5, 0.13)
128/244:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/245: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/246: regressor.fit(X, Y)
128/247: regressor.display_4_plots(0.05, 0.5, 0.13)
128/248:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/249: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/250: regressor.fit(X, Y)
128/251: regressor.display_4_plots(0.05, 0.5, 0.13)
128/252:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        plt.grid()
        ax1=plt.subplot(2, 2, 1)
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/253: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/254: regressor.fit(X, Y)
128/255: regressor.display_4_plots(0.05, 0.5, 0.13)
128/256:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=0.75
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/257: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/258: regressor.fit(X, Y)
128/259: regressor.display_4_plots(0.05, 0.5, 0.13)
128/260:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=1
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/261: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/262: regressor.fit(X, Y)
128/263: regressor.display_4_plots(0.05, 0.5, 0.13)
128/264:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/265: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/266: regressor.fit(X, Y)
128/267: regressor.display_4_plots(0.05, 0.5, 0.13)
128/268:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors='black')
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors='black')
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors='black')
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/269: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/270: regressor.fit(X, Y)
128/271: regressor.display_4_plots(0.05, 0.5, 0.13)
128/272:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors='black', linewidths=0.1)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors='black', linewidths=0.1)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors='black', linewidths=0.1)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/273: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/274: regressor.fit(X, Y)
128/275: regressor.display_4_plots(0.05, 0.5, 0.13)
128/276:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors='black', linewidths=1)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors='black', linewidths=1)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors='black', linewidths=1)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/277: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/278: regressor.fit(X, Y)
128/279: regressor.display_4_plots(0.05, 0.5, 0.13)
128/280:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=10
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/281: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/282: regressor.fit(X, Y)
128/283: regressor.display_4_plots(0.05, 0.5, 0.13)
128/284:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/285: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/286: regressor.fit(X, Y)
128/287: regressor.display_4_plots(0.05, 0.5, 0.13)
128/288:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        plt.figure.figsize=(20, 20)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/289: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/290: regressor.fit(X, Y)
128/291: regressor.display_4_plots(0.05, 0.5, 0.13)
128/292:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout()
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/293: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/294: regressor.fit(X, Y)
128/295: regressor.display_4_plots(0.05, 0.5, 0.13)
128/296:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/297:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/298: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/299: regressor.fit(X, Y)
128/300: regressor.display_4_plots(0.05, 0.5, 0.13)
128/301:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=2, h_pad=1.0)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/302: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/303: regressor.fit(X, Y)
128/304: regressor.display_4_plots(0.05, 0.5, 0.13)
128/305:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=10, h_pad=1.0)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/306: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/307: regressor.fit(X, Y)
128/308: regressor.display_4_plots(0.05, 0.5, 0.13)
128/309:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=10)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/310: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/311: regressor.fit(X, Y)
128/312: regressor.display_4_plots(0.05, 0.5, 0.13)
128/313:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/314: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/315: regressor.fit(X, Y)
128/316: regressor.display_4_plots(0.05, 0.5, 0.13)
128/317:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2.5))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/318: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/319: regressor.fit(X, Y)
128/320: regressor.display_4_plots(0.05, 0.5, 0.13)
128/321:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/322: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/323: regressor.fit(X, Y)
128/324: regressor.display_4_plots(0.05, 0.5, 0.13)
128/325:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
128/326: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/327: regressor.fit(X, Y)
128/328: regressor.display_4_plots(0.05, 0.5, 0.13)
128/329:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk
    
    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        ax4.plot(self.bandwidths, self.risks)
128/330: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/331: regressor.fit(X, Y)
128/332: regressor.display_4_plots(0.05, 0.5, 0.13)
128/333:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
128/334: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/335: regressor.fit(X, Y)
128/336: regressor.display_4_plots(0.05, 0.5, 0.13)
128/337:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(np.arange(0, 2.1, 0.1), rotated=45)
128/338: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/339: regressor.fit(X, Y)
128/340: regressor.display_4_plots(0.05, 0.5, 0.13)
128/341:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(np.arange(0, 2.1, 0.1), rotation='horizontal'=45)
128/342:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(np.arange(0, 2.1, 0.1), rotation=45)
128/343: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/344: regressor.fit(X, Y)
128/345: regressor.display_4_plots(0.05, 0.5, 0.13)
128/346:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1), labels=np.arange(0, 2.1, 0.1), rotation=45)
128/347: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/348: regressor.fit(X, Y)
128/349: regressor.display_4_plots(0.05, 0.5, 0.13)
128/350:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=np.arange(0, 2.1, 0.1), rotation=45)
128/351: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/352: regressor.fit(X, Y)
128/353: regressor.display_4_plots(0.05, 0.5, 0.13)
128/354:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45)
128/355: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/356: regressor.fit(X, Y)
128/357: regressor.display_4_plots(0.05, 0.5, 0.13)
128/358:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        plt.xticks(fontsize=12)
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45)
128/359: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/360: regressor.fit(X, Y)
128/361: regressor.display_4_plots(0.05, 0.5, 0.13)
128/362:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=12)
128/363: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/364: regressor.fit(X, Y)
128/365: regressor.display_4_plots(0.05, 0.5, 0.13)
128/366:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(self.X)
        plt.plot(self.X, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/367: regressor=NadarayaWatsonRegressor(bandwidth=0.5, kernel=KernelType.GAUSSIAN)
128/368: regressor.fit(X, Y)
128/369: regressor.display_4_plots(0.05, 0.5, 0.13)
128/370:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
128/371: regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
128/372: regressor2.fit(X, Y)
128/373: regressor2.plot_cross_validator(np.linspace(0.01, 2, 200))
128/374: print(regressor2.bandwidths[np.argmin(regressor2.risks)])
128/375: regressor2.display_4_plots(0.05, 0.75, 0.31)
128/376:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/377: regressor3.display(0.31)
128/378:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/379:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        # Y_predict=vectorized_predictor(self.X)
        plt.xlim(-1, 4)
        X_vals=np.linspace(-1, 4, 500)
        Y_predict=vectorized_predictor(X_vals)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/380:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/381: regressor3.display(0.31)
128/382:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        vectorized_predictor=np.vectorize(self.predict)
        # Y_predict=vectorized_predictor(self.X)
        plt.xlim(-1, 4)
        X_vals=np.linspace(-1, 4, 500)
        Y_predict=vectorized_predictor(X_vals)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/383:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/384:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*100)
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/385:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/386: regressor3.display(0.31)
128/387:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*1000)
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/388:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/389: regressor3.display(0.31)
128/390:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_vals=np.linspace(x_lim1, x_lim2, (x_lim2-x_lim1)*1000)
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return self.X, Y_pred

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/391:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/392:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-np.max(self.X))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (np.min(self.X)-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return self.X, Y_pred

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/393:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/394:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return self.X, Y_pred

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/395:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/396:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/397:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/398:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        return self.X, Y_pred

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/399:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/400:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
128/401:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/402:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
128/403: regressor1.display(0.05)
128/404: regressor1.display(0.01)
128/405:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.01, 0.5, 0.13)
128/406:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.001, 0.5, 0.13)
128/407:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        vectorized_predictor=np.vectorize(self.predict)
        Y_predict=vectorized_predictor(X_vals)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/408:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 0.5, 0.32)
128/409:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.001, 0.5, 0.13)
128/410:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 5, 0.32)
128/411:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.01, 0.5, 0.13)
128/412:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 1.75, 0.32)
128/413: regressor1.display(0.01)
128/414: regressor1.display(0.13)
128/415: regressor3.display(0.30)
128/416:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
128/417:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.05, 1.75, 0.32)
128/418:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
128/419:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
128/420: print(regressor3.bandwidths[np.argmin(regressor3.risks)])
128/421:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            plt.savefig('gaussian kernel regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            plt.savefig('epanechnikov kernel regression.png')
        elif self.kernel==KernelType.SQUARE:
            plt.savefig('tophat_kernel_regression.png')
128/422:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
128/423:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
128/424:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.01, 0.5, 0.13)
128/425:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
128/426:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize(22, 22)
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            plt.savefig('gaussian kernel regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            plt.savefig('epanechnikov kernel regression.png')
        elif self.kernel==KernelType.SQUARE:
            plt.savefig('tophat_kernel_regression.png')
128/427:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
128/428:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure.figsize((22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            plt.savefig('gaussian kernel regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            plt.savefig('epanechnikov kernel regression.png')
        elif self.kernel==KernelType.SQUARE:
            plt.savefig('tophat_kernel_regression.png')
128/429:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
128/430:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            plt.savefig('gaussian kernel regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            plt.savefig('epanechnikov kernel regression.png')
        elif self.kernel==KernelType.SQUARE:
            plt.savefig('tophat_kernel_regression.png')
128/431:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/1:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/2:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
131/3:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
131/4:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
131/5:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            plt.savefig('tophat_kernel_regression.png')
131/6: data=pd.read_csv('data.csv')
131/7: X=data['Al']
131/8: Y=data['RI']
131/9:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/10:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
131/11:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
131/12:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
131/13:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
131/14:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            plt.title('KDE using Gaussian Kernel')
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            plt.title('KDE using Epanechnikov Kernel')
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            plt.title('KDE using Tophat Kernel')
            plt.savefig('tophat_kernel_regression.png')
131/15: data=pd.read_csv('data.csv')
131/16: X=data['Al']
131/17: Y=data['RI']
131/18:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/19:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
131/20:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
131/21:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
131/22:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
131/23:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            fig.title('KDE using Gaussian Kernel')
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            fig.title('KDE using Epanechnikov Kernel')
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            fig.title('KDE using Tophat Kernel')
            plt.savefig('tophat_kernel_regression.png')
131/24: data=pd.read_csv('data.csv')
131/25: X=data['Al']
131/26: Y=data['RI']
131/27:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/28:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
131/29:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
131/30:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            fig.suptitle('KDE using Gaussian Kernel')
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            fig.suptitle('KDE using Epanechnikov Kernel')
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            fig.suptitle('KDE using Tophat Kernel')
            plt.savefig('tophat_kernel_regression.png')
131/31: data=pd.read_csv('data.csv')
131/32: X=data['Al']
131/33: Y=data['RI']
131/34:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/35:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
131/36:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
131/37:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
131/38:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
131/39:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.5)
        fig.subplots_adjust(top=0.88)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            fig.suptitle('KDE using Gaussian Kernel')
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            fig.suptitle('KDE using Epanechnikov Kernel')
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            fig.suptitle('KDE using Tophat Kernel')
            plt.savefig('tophat_kernel_regression.png')
131/40: data=pd.read_csv('data.csv')
131/41: X=data['Al']
131/42: Y=data['RI']
131/43:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
131/44:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
132/1:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
132/2:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
132/3:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=3)
        fig.subplots_adjust(top=0.88)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            fig.suptitle('KDE using Gaussian Kernel')
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            fig.suptitle('KDE using Epanechnikov Kernel')
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            fig.suptitle('KDE using Tophat Kernel')
            plt.savefig('tophat_kernel_regression.png')
132/4: data=pd.read_csv('data.csv')
132/5: X=data['Al']
132/6: Y=data['RI']
132/7:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
132/8:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
132/9:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
133/1:
import numpy as np 
from enum import Enum
import pandas as pd
from matplotlib import pyplot as plt
133/2:
class KernelType(Enum):
    GAUSSIAN = 1
    SQUARE = 2
    EPANECHNIKOV = 3
133/3:
class NadarayaWatsonRegressor:
    def __init__(self, bandwidth=1, kernel=KernelType.GAUSSIAN):
        if bandwidth <= 0:
            raise ValueError("Bandwidth must be greater than 0")
        
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.mean = 0
        self.x = None
        self.X = None
        self.Y = None

        self.vectorized_func=np.vectorize(self.evaluate_kernel)
    
        if not isinstance(self.kernel, KernelType):
            raise ValueError("Kernel type must be either GAUSSIAN or SQUARE or EPANECHNIKOV")

    def evaluate_kernel(self, value_being_evaluated):
        x=self.x-value_being_evaluated
        x/=self.bandwidth
        if self.kernel == KernelType.GAUSSIAN:
            return np.exp(-0.5 * (x ** 2))/np.sqrt(2 * np.pi)
        elif self.kernel == KernelType.SQUARE:
            return 0.5 * (np.abs(x) <= 1)
        elif self.kernel == KernelType.EPANECHNIKOV:
            return 0.75 * (1 - x ** 2) * (np.abs(x) <= 1)

    def fit(self, X, Y):
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        self.X = np.array(X)
        self.Y = np.array(Y)
        sorted_indices = np.argsort(self.X)
        self.X = self.X[sorted_indices]
        self.Y = self.Y[sorted_indices]

    def predict(self, x):
        self.x = x
        kernel_results = self.vectorized_func(self.X)
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum

    def fit_and_predict(self, X, Y, x):
        self.fit(X, Y)
        self.predict(x)

    def display(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before displaying it")
        if bandwidth is not None:
            self.bandwidth = bandwidth
        plt.scatter(self.X, self.Y)
        plt.xlim(-1, 4)
        X_vals, Y_predict = self.create_data_to_display(-1, 4)
        plt.plot(X_vals, Y_predict, color='red')
    
    def predict_without_changes(self, i):
        self.x = self.X[i]
        kernel_results = self.vectorized_func(self.X)
        kernel_results[i] = 0
        kernel_sum = np.sum(kernel_results)
        if kernel_sum==0:
            return 0
        return np.sum(kernel_results * self.Y) / kernel_sum
    
    def return_risk(self, bandwidth=None):
        if self.X is None:
            raise NotImplementedError("You must fit the model before calculating the risk")
        if bandwidth is not None:
            self.bandwidth = bandwidth

        n = len(self.X)
        risk = 0

        self.x=0
        weight_i = self.evaluate_kernel(0)
        # print(weight_i)
        y_preds=np.vectorize(self.predict_without_changes)(np.arange(n))
        risk = np.sum((self.Y - y_preds) ** 2)
        risk/=(1-weight_i)**2
        risk/=n
        return risk

    def generate_cross_validator(self, bandwidths=None):
        if bandwidths is None:
            bandwidths=np.linspace(0.01, 2, 200)
        self.bandwidths=bandwidths
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)

    def plot_cross_validator(self, bandwidths):
        vectorized_return_risk=np.vectorize(self.return_risk)
        self.risks = vectorized_return_risk(bandwidths)
        self.bandwidths=bandwidths
        # print(bandwidths[np.argmin(self.risks)])
        plt.plot(bandwidths, self.risks)

    def create_data_to_display(self, x_lim1, x_lim2):
        X_after_last=np.linspace(np.max(self.X), x_lim2, (x_lim2-int(np.max(self.X)))*1000)
        X_before_first=np.linspace(x_lim1, np.min(self.X), (int(np.min(self.X))-x_lim1)*1000)
        X_vals=np.concatenate((X_before_first, X_after_last))
        Y_vals=np.vectorize(self.predict)(X_vals)
        Y_pred=np.vectorize(self.predict)(self.X)
        X_vals=np.concatenate((X_vals, self.X))
        Y_vals=np.concatenate((Y_vals, Y_pred))
        indices=np.argsort(X_vals)
        X_vals=X_vals[indices]
        Y_vals=Y_vals[indices]
        # print(X_vals, Y_vals)
        return X_vals, Y_vals

    def display_4_plots(self, bandwidth1, bandwidth2, bandwidth3):
        # plt.figure(figsize=(22, 22))
        point_size=15
        facecolor='orange'
        edgecolor='black'
        linewidth=0.5
        x_lim1=-1
        x_lim2=4
        fig, axs = plt.subplots(2, 2)
        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=3.5)
        fig.subplots_adjust(top=0.88)
        ax1=plt.subplot(2, 2, 1)
        ax1.grid()
        ax1.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax1.set_title('Under Smoothened')
        ax1.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth1
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax1.plot(X_vals, Y_predict, color='red')
        ax1.set_xlabel(f'Bandwidth = {str(round(bandwidth1, 2))}')
        # ax1.set_title("Bandwidth = "+str(bandwidth1))
        ax2=plt.subplot(2, 2, 2)
        ax2.grid()
        ax2.set_title('Over Smoothened')
        ax2.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor, facecolor=facecolor, linewidths=linewidth)
        ax2.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth2
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax2.plot(X_vals, Y_predict, color='red')
        ax2.set_xlabel(f'Bandwidth = {str(round(bandwidth2, 2))}')
        # ax2.set_title("Bandwidth = "+str(bandwidth2))
        ax3=plt.subplot(2, 2, 3)
        ax3.grid()
        ax3.set_title('Correctly Smoothened')
        ax3.scatter(self.X, self.Y, s=point_size, edgecolors=edgecolor,facecolor=facecolor, linewidths=linewidth)
        ax3.set_xlim(x_lim1, x_lim2)
        self.bandwidth=bandwidth3
        X_vals, Y_predict=self.create_data_to_display(x_lim1, x_lim2)
        ax3.plot(X_vals, Y_predict, color='red')
        ax3.set_xlabel(f'Bandwidth = {str(round(bandwidth3, 2))}')
        # ax3.set_title("Bandwidth = "+str(bandwidth3))
        ax4=plt.subplot(2, 2, 4)
        ax4.grid()
        ax4.set_title('LOOCV CrossValidation')
        self.generate_cross_validator()
        ax4.plot(self.bandwidths, self.risks)
        ax4.set_xticks(ticks=np.arange(0, 2.1, 0.1))
        ax4.set_xticklabels(labels=[str(round(x, 1)) for x in np.arange(0, 2.1, 0.1)], rotation=45, fontsize=6)
        if self.kernel==KernelType.GAUSSIAN:
            fig.suptitle('KDE using Gaussian Kernel')
            plt.savefig('gaussian_kernel_regression.png')
        elif self.kernel==KernelType.EPANECHNIKOV:
            fig.suptitle('KDE using Epanechnikov Kernel')
            plt.savefig('epanechnikov_kernel_regression.png')
        elif self.kernel==KernelType.SQUARE:
            fig.suptitle('KDE using Tophat Kernel')
            plt.savefig('tophat_kernel_regression.png')
133/4: data=pd.read_csv('data.csv')
133/5: X=data['Al']
133/6: Y=data['RI']
133/7:
regressor1=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN)
regressor1.fit(X, Y)
regressor1.display_4_plots(0.05, 0.5, 0.13)
133/8:
regressor2=NadarayaWatsonRegressor(kernel=KernelType.SQUARE)
regressor2.fit(X, Y)
regressor2.display_4_plots(0.01, 1.75, 0.31)
133/9:
regressor3=NadarayaWatsonRegressor(kernel=KernelType.EPANECHNIKOV)
regressor3.fit(X, Y)
regressor3.display_4_plots(0.01, 1.75, 0.32)
134/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
135/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
135/2: data = pd.read_csv('data.csv', header=12)
135/3: filter_1 = data['D (Mpc)'][:1500]
135/4: final_data = filter_1[filter_1 < 4]
135/5:
n = len(final_data)
final_data
135/6:
plt.hist(final_data, bins=10)
title = r'\hat{p}(x)'
plt.title('$%s$'%title)
plt.show()
135/7: counts, bins = np.histogram(final_data, bins=10)
135/8:
plt.hist(final_data, bins=10, range=(0, 4))
title = r'\hat{p}(x)'
plt.title('$%s$'%title)
plt.show()
135/9: counts, bins = np.histogram(final_data, bins=10, range=(0, 4))
135/10: h = bins[1] - bins[0]
135/11: p = counts / (h * n)
135/12: p
135/13:
estimator = []
bin_widths = []
for bin_count in range(1, 1001):
    counts, bins = np.histogram(final_data, bins=bin_count)
    bin_width = bins[1] - bins[0]
    estimate = (1/(n**2 * bin_width)) * np.sum(counts**2)
    estimate -= (2/(n*(n-1)*bin_width)) * np.sum(counts**2 - counts)
    estimator.append(estimate)
    bin_widths.append(bin_width)
135/14:
plt.figure(dpi=1000)
plt.plot(bin_widths, estimator)
title = r'\hat{J}(h)'
plt.title('$%s$'%title + ' vs h')
plt.xlabel('h')
plt.ylabel('$%s$'%title)
plt.show()
np.argmin(estimator), bin_widths[np.argmin(estimator)]
135/15:
estimator = []
bin_widths = []
for bin_count in range(1, 1001):
    counts, bins = np.histogram(final_data, bins=bin_count, range=(0, 4))
    bin_width = bins[1] - bins[0]
    estimate = (1/(n**2 * bin_width)) * np.sum(counts**2)
    estimate -= (2/(n*(n-1)*bin_width)) * np.sum(counts**2 - counts)
    estimator.append(estimate)
    bin_widths.append(bin_width)
135/16:
plt.figure(dpi=1000)
plt.plot(bin_widths, estimator)
title = r'\hat{J}(h)'
plt.title('$%s$'%title + ' vs h')
plt.xlabel('h')
plt.ylabel('$%s$'%title)
plt.show()
np.argmin(estimator), bin_widths[np.argmin(estimator)]
135/17:
plt.figure(dpi=1000)
plt.plot(bin_widths, estimator)
plt.plot(range(1, 1001), estimator)
title = r'\hat{J}(h)'
plt.title('$%s$'%title + ' vs h')
plt.xlabel('h')
plt.ylabel('$%s$'%title)
plt.show()
np.argmin(estimator), bin_widths[np.argmin(estimator)]
135/18:
plt.figure(dpi=1000)
plt.plot(bin_widths, estimator)
# plt.plot(range(1, 1001), estimator)
title = r'\hat{J}(h)'
plt.title('$%s$'%title + ' vs h')
plt.xlabel('h')
plt.ylabel('$%s$'%title)
plt.show()
np.argmin(estimator), bin_widths[np.argmin(estimator)]
135/19:
plt.hist(final_data, bins=50)
title = r'\hat{p}(x)'
plt.title('$%s$'%title)
plt.show()
135/20:
for i in p:
    print(round(i, 3), '&', end=' ')
135/21: bins[1] - bins[0]
134/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
134/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
134/4:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
    
    def fit(self, X, Y):
        self.Y = Y
        vectorized_power_generator=np.vectorize(self.generate_powers)
        self.X = vectorized_power_generator(X)

    def generate_powers(self, x):
        powers = [1]
        for i in range(self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
134/5: train_data=pd.read_csv('train.csv')
134/6: train_data.head()
134/7:
X=train_data['x']
Y=train_data['y']
134/8: X
134/9: regressor=HigherOrderRegressor(3)
134/10: regressor.fit(X,Y)
134/11:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
134/12: regressor=HigherOrderRegressor(3)
134/13: regressor.fit(X,Y)
134/14:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
    
    def fit(self, X, Y):
        self.Y = Y
        vectorized_power_generator=np.vectorize(self.generate_powers)
        self.X = vectorized_power_generator(X)

    def generate_powers(self, x):
        powers = [1]
        for i in range(self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
134/15: train_data=pd.read_csv('train.csv')
134/16:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
134/17: regressor=HigherOrderRegressor(3)
134/18: regressor.fit(X,Y)
134/19:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
    
    def fit(self, X, Y):
        self.Y = Y
        self.X = np.vander(X, self.degree + 1, increasing=True)

    def generate_powers(self, x):
        powers = [1]
        for i in range(self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
134/20: train_data=pd.read_csv('train.csv')
134/21:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
134/22: regressor=HigherOrderRegressor(3)
134/23: regressor.fit(X,Y)
134/24:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        self.Y = Y
        self.X = np.vander(X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict(self, x):
        return np.dot(self.generate_powers(x), self.B)
134/25: train_data=pd.read_csv('train.csv')
134/26:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
134/27: regressor=HigherOrderRegressor(3)
134/28: regressor.fit(X,Y)
134/29:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        self.Y = Y
        self.X = np.vander(X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict(self, x):
        return np.dot(self.generate_powers(x), self.B)
134/30: regressor=HigherOrderRegressor(3)
134/31: regressor.fit(X,Y)
134/32: regressor.predict(1.5)
134/33:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        self.Y = Y
        self.X = np.vander(X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict(self, x):
        return np.dot(self.generate_powers(x), self.B)
134/34: regressor=HigherOrderRegressor(3)
134/35: regressor.fit(X,Y)
134/36: regressor.predict(1.5)
134/37: test_data=pd.read_csv('test.csv')
134/38: X_test=test_data['x'].to_numpy()
134/39: Y_test=test_data['y'].to_numpy()
134/40:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        self.Y = Y
        self.X = np.vander(X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)
134/41: regressor=HigherOrderRegressor(3)
134/42: regressor.fit(X,Y)
134/43: test_data=pd.read_csv('test.csv')
134/44: X_test=test_data['x'].to_numpy()
134/45: from sklearn.model_selection import train_test_split
134/46: X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
134/47: regressor.fit(X_train,Y_train)
134/48: regressor.predict(X_val)
134/49: data=regressor.predict(X_val)
134/50: from sklearn.metrics import mean_squared_error, r2_score
134/51: r2_score(Y_val, data)
134/52: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
134/53: mean_squared_error(Y_val, data)
134/54: mean_absolute_error(Y_val, data)
134/55: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
134/56: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
134/57:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices]
        temp_Y = Y[indices]
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        for i in range(0, self.X.shape[0], k):
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(r2_score(predictions, Y_test))
134/58: regressor=HigherOrderRegressor(3)
134/59: regressor.fit(X,Y)
134/60:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices]
        temp_Y = Y[indices]
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        for i in range(0, self.X.shape[0], k):
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(r2_score(predictions, Y_test))
134/61: regressor=HigherOrderRegressor(3)
134/62: regressor.fit(X,Y)
134/63:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices]
        temp_Y = Y[indices]
        print(temp_X)
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        for i in range(0, self.X.shape[0], k):
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(r2_score(predictions, Y_test))
134/64: regressor=HigherOrderRegressor(3)
134/65: regressor.fit(X,Y)
134/66:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        print(temp_X)
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        for i in range(0, self.X.shape[0], k):
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(r2_score(predictions, Y_test))
134/67: regressor=HigherOrderRegressor(3)
134/68: regressor.fit(X,Y)
134/69:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        for i in range(0, self.X.shape[0], k):
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(r2_score(predictions, Y_test))
134/70: regressor.fit(X,Y)
134/71: regressor.fit(X_train,Y_train)
134/72: regressor=HigherOrderRegressor(3)
134/73: regressor.fit(X,Y)
134/74: regressor.fit(X_train,Y_train)
134/75: data=regressor.predict(X_val)
134/76: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
134/77: r2_score(Y_val, data)
134/78: mean_squared_error(Y_val, data)
134/79: mean_absolute_error(Y_val, data)
134/80: regressor.k_fold_testing(10)
134/81:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        print(average_r2/count)
134/82: regressor=HigherOrderRegressor(3)
134/83: regressor.fit(X,Y)
134/84: regressor.k_fold_testing(10)
134/85:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        print(average_r2/count)
        return average_r2/count
    
    def cross_validation(self, params):
        vectorized_k_fold= np.vectorize(self.k_fold_testing)
        responses=vectorized_k_fold(params)
        print(responses)
134/86: regressor=HigherOrderRegressor(3)
134/87: regressor.fit(X,Y)
134/88: regressor.cross_validation([0,1,2,3,4,5,6,7,8,9,10])
134/89: regressor.cross_validation([1,2,3,4,5,6,7,8,9,10])
134/90: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/91:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        print(average_r2/count)
        return average_r2/count
    
    def cross_validation(self, params):
        responses = []
        for param in params:
            self.degree = param
            self.fit(self.X, self.Y)
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/92: regressor=HigherOrderRegressor(3)
134/93: regressor.fit(X,Y)
134/94: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/95:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        print(average_r2/count)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/96: regressor=HigherOrderRegressor(3)
134/97: regressor.fit(X,Y)
134/98: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/99:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/100: regressor=HigherOrderRegressor(3)
134/101: regressor.fit(X,Y)
134/102: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/103:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/104:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/105:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 + self.r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/106: regressor=HigherOrderRegressor(3)
134/107: regressor.fit(X,Y)
134/108: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/109:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            responses.append(self.k_fold_testing())
        # vectorized_k_fold= np.vectorize(self.k_fold_testing)
        # responses=vectorized_k_fold(params)
        print(responses)
134/110: regressor=HigherOrderRegressor(3)
134/111: regressor.fit(X,Y)
134/112: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/113: test_data=pd.read_csv('test.csv')
134/114: X_test=test_data['x'].to_numpy()
134/115: Y_pred=regressor.predict(X_test)
134/116: regressor2=HigherOrderRegressor(3)
134/117:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            plt.scatter(self.orig_X, self.Y)
            plt.plot(self.orig_X, self.predict(self.orig_X))
            plt.savefig(f"degree_{param}.png")
            responses.append(self.k_fold_testing())
        print(responses)
134/118: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/119: regressor2=HigherOrderRegressor(3)
134/120: regressor=HigherOrderRegressor(3)
134/121: regressor.fit(X,Y)
134/122: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/123:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        return average_r2/count
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            print(f"Degree: {param}")
            self.degree = param
            self.X=self.max_X[:,:param+1]
            plt.scatter(self.orig_X, self.Y)
            plt.plot(self.orig_X, self.predict(self.orig_X))
            plt.savefig(f"degree_{param}.png")
            responses.append(self.k_fold_testing())
        print(responses)
134/124: regressor=HigherOrderRegressor(3)
134/125: regressor.fit(X,Y)
134/126: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/127:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        return average_r2/count, np.matmul(self.orig_X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            print(f"Degree: {param}")
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            plt.scatter(self.orig_X, self.Y)
            plt.plot(self.orig_X, plot_values)
            plt.savefig(f"degree_{param}.png")
        print(responses)
134/128: regressor=HigherOrderRegressor(3)
134/129: regressor.fit(X,Y)
134/130: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/131:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            print(f"Degree: {param}")
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            plt.scatter(self.orig_X, self.Y)
            plt.plot(self.orig_X, plot_values)
            plt.savefig(f"degree_{param}.png")
        print(responses)
134/132: regressor=HigherOrderRegressor(3)
134/133: regressor.fit(X,Y)
134/134: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/135:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            print(f"Degree: {param}")
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            fig.scatter(self.orig_X, self.Y)
            fig.plot(self.orig_X, plot_values)
            fig.savefig(f"degree_{param}.png")
        print(responses)
134/136: regressor=HigherOrderRegressor(3)
134/137: regressor.fit(X,Y)
134/138: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/139:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            print(f"Degree: {param}")
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            plt.plot(self.orig_X, plot_values)
            fig.savefig(f"degree_{param}.png")
        print(responses)
134/140: regressor=HigherOrderRegressor(3)
134/141: regressor.fit(X,Y)
134/142: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/143:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            print(f"Degree: {param}")
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices])
            fig.savefig(f"degree_{param}.png")
        print(responses)
134/144: regressor=HigherOrderRegressor(3)
134/145: regressor.fit(X,Y)
134/146: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/147:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices])
            fig.savefig(f"degree_{param}.png")
        print(responses)
134/148:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
134/149: regressor=HigherOrderRegressor(3)
134/150: regressor.fit(X,Y)
134/151: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/152:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
134/153: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/154: regressor=HigherOrderRegressor(3)
134/155: regressor.fit(X,Y)
134/156: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/157:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def r2_score(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
134/158: regressor=HigherOrderRegressor(3)
134/159: regressor=HigherOrderRegressor(3)
134/160: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/161: regressor=HigherOrderRegressor(3)
134/162: regressor=HigherOrderRegressor(3)
134/163: regressor.fit(X,Y)
134/164: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
134/165:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.sum_of_squares(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
134/166:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
134/167:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.sum_of_squares(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/168: regressor3=HigherOrderRegressor(5)
134/169: regressor3.fit(X,Y)
134/170: regressor3.write_params()
134/171:
test_data=pd.read_csv('test.csv')
X_test=test_data['x'].to_numpy()
test_data['y']=regressor3.predict(X_test)
134/172: test_data.to_csv('3_predictions.csv', index=False)
134/173:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.sum_of_squares(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/174:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.sum_of_squares(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/175:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([20])
134/176:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([11])
134/177:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([15])
134/178:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([18])
134/179:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([25])
134/180:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([20])
134/181:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([25])
134/182:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([22])
134/183:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([24])
134/184:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([25])
134/185:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([2, 20])
134/186:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation([2, 5, 20])
134/187:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.sum_of_squares(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/188:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation(np.arange(1, 25))
134/189: plt.plot(np.arange(1, 25), [64653.555302618384, 56427.175480097954, 39251.2107718163, 24080.14499184062, 23135.01264391381, 23147.39205431092, 23316.53520429746, 23507.28378862272, 23391.932968519566, 23591.837720512074, 23799.29808194501, 24306.126217608657, 24567.738597261134, 24511.039529749152, 25040.059015164665, 24245.658838890413, 24135.381150518013, 23388.984818572288, 24449.065706832644, 24631.227983779838, 24937.833575556066, 28290.30555397866, 2673741.993952508, 12928066.182318613])
134/190:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/191:
regressor4=HigherOrderRegressor(20)
regressor4.fit(X, Y)
regressor4.cross_validation(np.arange(1, 25))
133/10: from sklearn.tree import DecisionTreeRegressor
133/11: regressorx=DecisionTreeRegressor()
133/12: from sklearn.model_selection import train_test_split
133/13: X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2)
133/14: sklearn_regressor=regressorx.fit(X_train.values.reshape(-1, 1), Y_train)
133/15: sklearn_regressor.predict(X_test.values.reshape(-1, 1))
133/16: prediction=sklearn_regressor.predict(X_test.values.reshape(-1, 1))
133/17: from sklearn.metrics import r2_score
133/18: print(r2_score(Y_test, prediction))
133/19: print(r2_score(Y_test, regressor1.predict(X_test)))
133/20: regressor1.predict(X_test)
133/21: preds=np.vectorize(regressor1.predict)(X_test)
133/22: print(r2_score(Y_test, preds))
133/23: regressor4=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN, bandwidth=0.13)
133/24: from sklearn.model_selection import LeaveOneOut
133/25: leave_one_out = LeaveOneOut()
133/26: leave_one_out.get_n_splits(X)
133/27: regressor4=NadarayaWatsonRegressor(kernel=KernelType.GAUSSIAN, bandwidth=0.13)
133/28: from sklearn.model_selection import train_test_split
133/29: X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2, random_state=42)
133/30: regressor4.fit_and_predict(X_train, Y_train, X_test)
133/31: regressor4.fit(X_trrain, Y_train)
133/32: regressor4.fit(X_train, Y_train)
133/33: predictions=np.vectorize(regressor4.predict)(X_test)
133/34: from sklearn.metrics import r2_score
133/35: r2_score(Y_test, predictions)
133/36: plt.plot(X_test, Y_test, 'bo')
133/37:
plt.scatter(X_test, Y_test, 'bo')
plt.scatter(X_test, predictions, 'ro')
133/38:
plt.scatter(X_test, Y_test, 'bo')
plt.scatter(X_test, predictions, 'o')
133/39:
plt.scatter(X_test, Y_test, 'bo')
# plt.scatter(X_test, predictions)
133/40:
plt.scatter(X_test, Y_test, 'bo')
# plt.scatter(X_test, predictions)
133/41:
plt.scatter(X_test, Y_test, 'b')
# plt.scatter(X_test, predictions)
133/42:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, predictions)
133/43: r2_score(Y_test, predictions)
133/44: from sklearn.tree import DecisionTreeRegressor
133/45: regressorx=DecisionTreeRegressor()
133/46: regressorx.fit(X_train.values.reshape(-1, 1), Y_train)
133/47: regressorx.predict(X_test.values.reshape(-1, 1))
133/48: preds=regressorx.predict(X_test.values.reshape(-1, 1))
133/49:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, preds)
133/50: print(r2_score(Y_test, preds))
133/51: preds=regressorx.predict(X_test.values.reshape(-1, 1))
133/52:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, preds)
133/53: print(r2_score(Y_test, preds))
133/54: regressorx.fit(X_train.values.reshape(-1, 1), Y_train)
133/55: X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2, random_state=42)
133/56: regressor4.fit(X_train, Y_train)
133/57: predictions=np.vectorize(regressor4.predict)(X_test)
133/58: from sklearn.metrics import r2_score
133/59: r2_score(Y_test, predictions)
133/60:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, predictions)
133/61: r2_score(Y_test, predictions)
133/62: from sklearn.tree import DecisionTreeRegressor
133/63: regressorx=DecisionTreeRegressor()
133/64: regressorx.fit(X_train.values.reshape(-1, 1), Y_train)
133/65: preds=regressorx.predict(X_test.values.reshape(-1, 1))
133/66:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, preds)
133/67: regressorx=DecisionTreeRegressor(max_depth=5)
133/68: regressorx.fit(X_train.values.reshape(-1, 1), Y_train)
133/69: preds=regressorx.predict(X_test.values.reshape(-1, 1))
133/70:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, preds)
133/71: print(r2_score(Y_test, preds))
133/72: regressorx=DecisionTreeRegressor(max_depth=3)
133/73: regressorx.fit(X_train.values.reshape(-1, 1), Y_train)
133/74: preds=regressorx.predict(X_test.values.reshape(-1, 1))
133/75:
plt.plot(X_test, Y_test, 'bo')
plt.scatter(X_test, preds)
133/76: print(r2_score(Y_test, preds))
134/192: X_new=train_data.to_numpy()
134/193: regressor=HigherOrderRegressor(3)
134/194: regressor.generate_sorted_vander(X_new, 3)
134/195:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X[:, indices], axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/196: regressor=HigherOrderRegressor(3)
134/197: regressor.generate_sorted_vander(X_new, 3)
134/198: regressor.generate_sorted_vander(X_new)
134/199:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X[:, indices], axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/200: regressor=HigherOrderRegressor(3)
134/201: regressor.generate_sorted_vander(X_new)
134/202: regressor.generate_sorted_vander(X_new, 3)
134/203:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
134/204: regressor=HigherOrderRegressor(3)
134/205: regressor.generate_sorted_vander(X_new, 3)
136/1: import pandas as pd
136/2: df=pd.read_csv('train.csv')
136/3: from sklearn.linear_model import LassoCV
136/4: X=df.drop('yield', axis=1)
136/5: y=df['yield']
136/6: regressor=LassoCV(cv=5, random_state=0).fit(X, y)
136/7: regressor.predict(X)
136/8: from sklearn.model_selection import train_test_split
136/9: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/10: regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
136/11: preds=regressor.predict(X_test)
136/12: from sklearn.metrics import r2_score
136/13: r2_score(y_test, preds)
136/14: regressor.coef_
136/15: from sklearn.polyfit import PolynomialFeatures
136/16: from sklearn.preprocessing import PolynomialFeatures
136/17: X_poly=PolynomialFeatures(degree=3).fit_transform(X)
136/18: X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)
136/19: regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
136/20: preds=regressor.predict(X_test)
136/21: from sklearn.metrics import r2_score
136/22: r2_score(y_test, preds)
136/23: regressor.coef_
136/24:
for coef in regressor.coef_:
    print(coef)
136/25:
for coef in regressor.coef_:
    if coef!=0:
        print(coef)
136/26: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/27: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/28: regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
136/29: preds=regressor.predict(X_test)
136/30: from sklearn.metrics import r2_score
136/31: r2_score(y_test, preds)
136/32:
for coef in regressor.coef_:
    if coef!=0:
        print(coef)
136/33:
for coef in regressor.coef_:
    print(coef)
136/34: X=df.drop(['yield', 'id', 'Row#'], axis=1)
136/35: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/36: regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
136/37: preds=regressor.predict(X_test)
136/38: from sklearn.metrics import r2_score
136/39: r2_score(y_test, preds)
136/40:
for coef in regressor.coef_:
    print(coef)
136/41:
regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
136/42:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
136/43:
regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
136/44: preds=regressor.predict(X_test)
136/45: from sklearn.metrics import r2_score
136/46: r2_score(y_test, preds)
136/47:
for coef in regressor.coef_:
    print(coef)
136/48: X_poly=PolynomialFeatures(degree=3).fit_transform(X)
136/49: X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)
136/50:
regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
136/51: preds=regressor.predict(X_test)
136/52: from sklearn.metrics import r2_score
136/53: r2_score(y_test, preds)
136/54:
for coef in regressor.coef_:
    print(coef)
136/55: from sklearn.metrics import mean_absolute_error, r2_score
136/56: r2_score(y_test, preds)
136/57: X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
136/58: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/59:
regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
136/60:
# regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
136/61: preds=regressor.predict(X_test)
136/62: from sklearn.metrics import mean_absolute_error, r2_score
136/63: r2_score(y_test, preds)
136/64:
for coef in regressor.coef_:
    print(coef)
136/65: mean_absolute_error(y_test, preds)
137/1:
import pandas as pd
import numpy as np
136/66:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        # temp_X = X[indices][0]
        # temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/67:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        # temp_X = X[indices][0]
        # temp_Y = Y[indices][0]
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/68: regressor=MultiVariateRegressor(3)
136/69: regressor.fit(X_train.values, y_train.values)
136/70:
import pandas as pd
import numpy as np
136/71: regressor.fit(X_train.values, y_train.values)
136/72: regressor.predict(X_test)
136/73:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        # temp_X = X[indices][0]
        # temp_Y = Y[indices][0]
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/74: regressor=MultiVariateRegressor(3)
136/75: regressor.fit(X_train.values, y_train.values)
136/76: regressor.predict(X_test)
136/77: r2_score(Y_test, regressor.predict(X_test))
136/78: r2_score(y_test, regressor.predict(X_test))
136/79: mean_absolute_error(y_test, regressor.predict(X_test))
136/80: reg=MultiVariateRegressor(3)
136/81: reg.fit(X_train.values, y_train.values)
136/82: r2_score(y_test, reg.predict(X_test))
136/83: mean_absolute_error(y_test, reg.predict(X_test))
136/84:
# X=df.drop(['yield', 'id', 'Row#'], axis=1)
X=df.drop('yield', axis=1)
136/85: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/86: reg.fit(X_train.values, y_train.values)
136/87: r2_score(y_test, reg.predict(X_test))
136/88: r2_score(y_test, reg.predict(X_test))
136/89: mean_absolute_error(y_test, reg.predict(X_test))
136/90:
def write_output():
    test_data=pd.read_csv('test.csv')
    test_data=test_data.drop(['id', 'Row#'], axis=1)
    test_data['yield']=reg.predict(test_data)
136/91:
def write_output():
    test_data=pd.read_csv('test.csv')
    test_data=test_data.drop(['id', 'Row#'], axis=1)
    test_data['yield']=reg.predict(test_data)
    test_data.to_csv('output.csv', index=False)
136/92:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/93:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/94: write_output()
136/95: reg=MultiVariateRegressor(3)
136/96: reg.fit(X_train.values, y_train.values)
136/97: r2_score(y_test, reg.predict(X_test))
136/98: mean_absolute_error(y_test, reg.predict(X_test))
136/99:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/100: write_output()
136/101:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
136/102: reg=MultiVariateRegressor(3)
136/103: reg.fit(X_train.values, y_train.values)
136/104: r2_score(y_test, reg.predict(X_test))
136/105: mean_absolute_error(y_test, reg.predict(X_test))
136/106:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
136/107: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
136/108: reg=MultiVariateRegressor(3)
136/109: reg.fit(X_train.values, y_train.values)
136/110: r2_score(y_test, reg.predict(X_test))
136/111: mean_absolute_error(y_test, reg.predict(X_test))
136/112:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/113: write_output()
136/114: from sklearn.tree import DecisionTreeRegressor
136/115: kde = DecisionTreeRegressor().fit(X_train, y_train)
136/116: predictions = kde.predict(X_test)
136/117: r2_score(y_test, predictions)
136/118: mean_absolute_error(y_test, predictions)
136/119: kde = LinearRegression().fit(X_train, y_train)
136/120: predictions = kde.predict(X_test)
136/121: r2_score(y_test, predictions)
136/122: mean_absolute_error(y_test, predictions)
136/123: kde.coef_
136/124: regressor.B
136/125:
for coef in zip(kde.coef_, regressor.B):
    print(coef)
136/126:
for coef in zip(kde.coef_, regressor.B):
    print(coef)
136/127: kde = LassoCV(cv=5, random_state=0).fit(X_train, y_train)
136/128: mean_absolute_error(y_test, kde.predict(X_test))
136/129:
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test)
136/130: kde = LinearRegression().fit(X_train2, y_train)
136/131: mean_absolute_error(y_test, kde.predict(X_test2))
136/132:
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
136/133: kde = LinearRegression().fit(X_train2, y_train)
136/134: mean_absolute_error(y_test, kde.predict(X_test2))
136/135: kde = LassoCV().fit(X_train2, y_train)
136/136: mean_absolute_error(y_test, kde.predict(X_test2))
136/137: kde = LinearRegression().fit(X_train2, y_train)
136/138:
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
136/139: kde = LinearRegression().fit(X_train2, y_train)
136/140: mean_absolute_error(y_test, kde.predict(X_test2))
136/141: reg=MultiVariateRegressor(3)
136/142: reg.fit(X_train2.values, y_train.values)
136/143: reg.fit(X_train2, y_train.values)
136/144: reg.fit(X_train2, y_train)
136/145: reg.fit(X_train2, y_train.values)
136/146: r2_score(y_test, reg.predict(X_test2))
136/147: mean_absolute_error(y_test, reg.predict(X_test))
136/148: mean_absolute_error(y_test, reg.predict(X_test2))
136/149:
reg=MultiVariateRegressor(3)
print(X_train2.shape)
136/150: from sklearn.preprocessing import StandardScaler
136/151: ss=StandardScaler()
136/152: ss=StandardScaler().fit(X_train)
136/153:
X_train3=ss.transform(X_train)
X_test3=ss.transform(X_test)
136/154: kde = LinearRegression().fit(X_train3, y_train)
136/155: mean_absolute_error(y_test, kde.predict(X_test3))
136/156: reg=MultiVariateRegressor(3)
136/157: reg.fit(X_train3, y_train.values)
136/158: r2_score(y_test, reg.predict(X_test3))
136/159: mean_absolute_error(y_test, reg.predict(X_test2))
136/160: mean_absolute_error(y_test, reg.predict(X_test3))
136/161: r2_score(y_test, kde.predict(X_test3))
136/162: reg.fit(X_train, y_train.values)
136/163: reg.fit(X_train.values, y_train.values)
136/164: r2_score(y_test, reg.predict(X_train))
136/165: r2_score(y_test, reg.predict(X_train.values))
136/166: r2_score(y_test, reg.predict(X_test.values))
136/167: mean_absolute_error(y_test, reg.predict(X_test.values))
136/168: reg.fit(X_train3.values, y_train.values)
136/169: reg.fit(X_train3, y_train.values)
136/170: r2_score(y_test, reg.predict(X_test))
136/171: r2_score(y_test, reg.predict(X_test3))
136/172:
for coef in zip(kde.coef_, regressor.B):
    print(coef)
136/173:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        print(self.X)
        self.X = X[indices][0]
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/174: reg.fit(X_train, y_train.values)
136/175: reg=MultiVariateRegressor(3)
136/176: reg.fit(X_train.values, y_train.values)
136/177: r2_score(y_test, reg.predict(X_test.values))
136/178:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        print(self.X)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/179: reg=MultiVariateRegressor(3)
136/180: reg.fit(X_train.values, y_train.values)
136/181:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        # print(self.X)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        regres=sklearn.linear_model.LinearRegression().fit(self.X, self.Y)
        for coef in zip(regres.coef_, self.B):
            print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/182: reg=MultiVariateRegressor(3)
136/183: reg.fit(X_train.values, y_train.values)
136/184:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        # print(self.X)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        regres=LinearRegression().fit(self.X, self.Y)
        for coef in zip(regres.coef_, self.B):
            print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/185: reg=MultiVariateRegressor(3)
136/186: reg.fit(X_train.values, y_train.values)
136/187: reg.fit(X_train.values[:5], y_train.values[:5])
136/188: reg.fit(X_train.values[:1], y_train.values[:1])
136/189: reg.fit(X_train.values[:2], y_train.values[:2])
136/190: reg.fit(X_train.values[:5], y_train.values[:5])
136/191:
from numpy.linalg import pinv
reg=MultiVariateRegressor(3)
136/192:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        # print(self.X)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        self.B = np.dot(pinv(np.dot(self.X.T, self.X)), np.dot(self.X.T, self.Y))

        regres=LinearRegression().fit(self.X, self.Y)
        for coef in zip(regres.coef_, self.B):
            print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/193:
from numpy.linalg import pinv
reg=MultiVariateRegressor(3)
136/194: reg.fit(X_train.values[:5], y_train.values[:5])
136/195: r2_score(y_test, reg.predict(X_test.values))
136/196: reg.fit(X_train.values, y_train.values)
136/197: r2_score(y_test, reg.predict(X_test.values))
136/198: mean_absolute_error(y_test, reg.predict(X_test.values))
136/199:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
136/200:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        # print(self.X)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        regres, _, _, _ = np.linalg.lstsq(self.X, self.Y, rcond=None)
        for coef in zip(regres, self.B):
            print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/201:
from numpy.linalg import pinv
reg=MultiVariateRegressor(3)
136/202: reg.fit(X_train.values, y_train.values)
136/203:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        # print(self.X)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        for coef in zip(regres.coef_, self.B):
            print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/204:
from numpy.linalg import pinv
reg=MultiVariateRegressor(3)
136/205: reg.fit(X_train.values, y_train.values)
136/206:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        # print(self.X)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        for coef in zip(regres.coef_, self.B):
            print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/207:
from numpy.linalg import pinv
reg=MultiVariateRegressor(3)
136/208: reg.fit(X_train.values, y_train.values)
136/209:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(X, self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/210: reg=MultiVariateRegressor(3)
136/211: reg.fit(X_train.values, y_train.values)
136/212: r2_score(y_test, reg.predict(X_test.values))
136/213:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/214: reg=MultiVariateRegressor(3)
136/215: reg.fit(X_train.values, y_train.values)
136/216: r2_score(y_test, reg.predict(X_test.values))
136/217: mean_absolute_error(y_test, reg.predict(X_test.values))
136/218:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/219: write_output()
134/206:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
134/207:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train.values, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
134/208:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train.reshape(-1,1), y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
134/209:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
134/210:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['custom']=mean_absolute_error(y_test, predictions)
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/220:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['custom']=mean_absolute_error(y_test, predictions)
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/221:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['custom']=mean_absolute_error(y_test, predictions)
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/222:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['custom']=mean_absolute_error(y_test, predictions)
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/223:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/224:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['custom']=mean_absolute_error(y_test, predictions)
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/225:
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Support Vector Regression': SVR()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['custom']={'MAE': mean_absolute_error(y_test, predictions)}
# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/226:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/227:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
X_train=ss.transform(X_train)
X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/228:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
X_train=ss.transform(X_train)
X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/229:
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/230:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
136/231:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
136/232:
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}
136/233: reg=MultiVariateRegressor(3)
136/234:
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}
136/235:
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/236: lasso_regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
136/237:
for coef_ in zip(lasso_regressor.coef_, reg.B):
    print(coef_)
136/238: X_train2=X_train[lasso_regressor.coef_ != 0]
136/239: X_train2=X_train[:,lasso_regressor.coef_ != 0]
136/240:
X_train2=X_train[:,lasso_regressor.coef_ != 0]
X_test2=X_test[:,lasso_regressor.coef_ != 0]
136/241: reg.fit(X_train2, y_train)
136/242: print(X_train2)
136/243: reg.fit(X_train2, y_train)
136/244: print(X_train2.shape, y_train.shape)
136/245: reg.fit(X_train2, y_train)
136/246: reg.fit(X_train2, y_train.values)
136/247: mean_absolute_error(y_test, reg.predict(X_test2))
136/248:
for coef_ in zip(lasso_regressor.coef_[np.where(lasso_regressor.coef_ != 0)], reg.B):
    print(coef_)
136/249:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
X_train=ss.transform(X_train)
X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/250:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/251: write_output()
136/252:
def write_output():
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/253: write_output()
136/254:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
136/255:
def write_output():
    reg.fit(np.vstack((X_train, X_test)), np.vstack((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/256:
def write_output():
    reg.fit(np.vstack((X_train, X_test)), np.vstack((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/257: write_output()
136/258:
def write_output():
    print(X_train.shape, X_test.shape)
    reg.fit(np.vstack((X_train, X_test)), np.vstack((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/259: write_output()
136/260:
def write_output():
    print(X_train.shape, X_test.shape)
    reg.fit(np.hstack((X_train, X_test)), np.hstack((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/261: write_output()
136/262:
def write_output():
    print(X_train.shape, X_test.shape)
    reg.fit(np.vstack((X_train, X_test)), np.vstack((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/263: write_output()
136/264:
def write_output():
    X_all = np.vstack((X_train, X_test))
    y_all = np.concatenate((y_train, y_test))
    reg.fit(np.vstack((X_train, X_test)), np.vstack((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/265: write_output()
136/266:
def write_output():
    # X_all = np.vstack((X_train, X_test))
    # y_all = np.concatenate((y_train, y_test))
    reg.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
136/267: write_output()
136/268:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train = X_train.values
# X_test = X_test.values
# from sklearn.preprocessing import PolynomialFeatures
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# # X_train=ss.transform(X_train)
# # X_test=ss.transform(X_test)
X_train=X_train[:, lasso_regressor.coef_ != 0]
X_test=X_test[:, lasso_regressor.coef_ != 0]
# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
138/1:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*i))
        result=np.hstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/2:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
138/3:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*i))
        result=np.hstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/4: regressor=HigherOrderRegressor(3)
138/5: train_data=pd.read_csv('train.csv')
138/6:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
138/7: regressor.fit(X, Y)
138/8:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*i))
        result=np.hstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/9: regressor.fit(X, Y)
138/10: regressor=HigherOrderRegressor(3)
138/11: regressor.fit(X, Y)
138/12:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*i))
        result=np.hstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/13: regressor.fit(X, Y)
138/14: regressor=HigherOrderRegressor(3)
138/15: regressor.fit(X, Y)
138/16: regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/17:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.X[i]))
        result=np.hstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/18: regressor=HigherOrderRegressor(3)
138/19: regressor.fit(X, Y)
138/20: regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/21:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[i]))
        result=np.hstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/22: regressor=HigherOrderRegressor(3)
138/23: regressor.fit(X, Y)
138/24: regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/25: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/26: print(res.shape())
138/27: print(res.shape
138/28: print(res.shape)
138/29:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/30: regressor=HigherOrderRegressor(3)
138/31: regressor.fit(X, Y)
138/32: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/33: print(res.shape)
138/34: res
138/35: print(res)
138/36: print(res[0])
138/37: print(X.shape)
138/38:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = temp_X
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/39: regressor=HigherOrderRegressor(3)
138/40: regressor.fit(X, Y)
138/41: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/42:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = np.vstack(np.ones(X.shape[0]), temp_X)
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/43: regressor=HigherOrderRegressor(3)
138/44: regressor.fit(X, Y)
138/45: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/46:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = np.vstack((np.ones(X.shape[0]), temp_X))
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/47: regressor=HigherOrderRegressor(3)
138/48: regressor.fit(X, Y)
138/49: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/50:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        self.orig_X = np.hstack((np.ones(X.shape[0]), temp_X))
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/51: regressor=HigherOrderRegressor(3)
138/52: regressor.fit(X, Y)
138/53:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones(X.shape[0], 1), temp_X))
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/54: regressor=HigherOrderRegressor(3)
138/55: regressor.fit(X, Y)
138/56:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1, current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/57: regressor=HigherOrderRegressor(3)
138/58: regressor.fit(X, Y)
138/59: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/60: print(res)
138/61: res
138/62: regressor.fit(np.hstack(X, Y), Y)
138/63: regressor.fit(np.hstack((X, Y)), Y)
138/64: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/65:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        # self.X = np.vander(temp_X, self.degree + 1, increasing=True)
        # self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1,
                                                          current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/66: regressor=HigherOrderRegressor(3)
138/67: regressor.fit(np.hstack((X, Y)), Y)
138/68: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/69: print(res.orig_X)
138/70: print(regressor.orig_X)
138/71: print(regressor.orig_X[:1])
138/72: print(regressor.orig_X[:,1])
138/73: print(regressor.orig_X[:,1].shape)
138/74: print(regressor.orig_X[:,0].shape)
138/75: print(X.shape)
138/76: print(regressor.orig_X)
138/77: regressor.fit(np.vstack((X, Y)), Y)
138/78: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/79: print(regressor.orig_X)
138/80: print(regressor.orig_X)
138/81: print(X.shape)
138/82: print(np.vstack((X, Y)).shape)
138/83:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/84: regressor.fit(np.vstack((X, Y)), Y)
138/85: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/86: print(np.vstack((X, Y)).shape)
138/87: print(np.hstack((X, Y)).shape)
138/88: regressor.fit(np.hstack((X, Y)), Y)
138/89: res=regressor.recursive_generate_powers(0, 3, np.ones(X.shape[0]))
138/90: print(res)
138/91: print(res.shape)
138/92:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(self.X.shape[0]))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1,
                                                          current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/93:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/94: regressor.fit(np.hstack((X, Y)), Y)
138/95:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1,
                                                          current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/96:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/97: regressor.fit(np.hstack((X, Y)), Y)
138/98: print(np.hstack((X, Y)).shape)
138/99: print(res)
138/100: print(res.shape)
138/101:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1,
                                                          current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/102:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/103: regressor.fit(np.hstack((X, Y)), Y)
138/104:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1,
                                                          current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result

    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/105:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/106: regressor.fit(np.hstack((X, Y)), Y)
138/107: res=regressor.predict(np.hstack((X, Y)))
138/108:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            result.append(self.recursive_generate_powers(i, current_degree-1,
                                                          current_product*self.orig_X[:,i]))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=self.recursive_generate_powers(0, self.degree, X.T).T
        return np.matmul(self.test_X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/109:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/110: regressor.fit(np.hstack((X, Y)), Y)
138/111: res=regressor.predict(np.hstack((X, Y)))
138/112:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i]))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=self.recursive_generate_powers(0, self.degree, X.T).T
        return np.matmul(self.test_X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/113:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i]), False)
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=self.recursive_generate_powers(0, self.degree, X.T).T
        return np.matmul(self.test_X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/114:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i]), False)
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, X.T, False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/115:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/116: regressor.fit(np.hstack((X, Y)), Y)
138/117: res=regressor.predict(np.hstack((X, Y)))
138/118:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, X.T, False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/119:
regressor=HigherOrderRegressor(3)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/120: regressor.fit(np.hstack((X, Y)), Y)
138/121: res=regressor.predict(np.hstack((X, Y)))
138/122: regressor.fit(X, Y)
138/123: res=regressor.predict(X)
138/124: print(res)
138/125: res=regressor.predict(np.vstack((X, Y)))
138/126: print(res)
138/127: regressor.fit(X, Y)
138/128: res=regressor.predict(X)
138/129: print(res)
138/130: r2_score(Y, res)
138/131: from sklearn.model_selection import train_test_split
138/132: from sklearn.metrics import r2_score
138/133: r2_score(Y, res)
138/134: print(res.X)
138/135: print(regressor.X)
138/136: r2_score(Y, res)
138/137:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/138: regressor.fit(X, Y)
138/139: res=regressor.predict(X)
138/140: print(regressor.X)
138/141: r2_score(Y, res)
138/142: plt.plot(Y, res)
138/143: plt.scatter(X, Y)
138/144:
plt.scatter(X, Y)
plt.scatter(X, res, color='red')
138/145:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(np.vander(self.orig_X[:,0], self.degree+1, increasing=True))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, X.T, False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/146:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/147: regressor.fit(X, Y)
138/148:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(np.vander(self.orig_X[:,1], self.degree+1, increasing=True))
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, X.T, False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/149:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/150: regressor.fit(X, Y)
138/151:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(np.vander(self.orig_X[:,1], self.degree+1, increasing=True))
        self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, X.T, False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/152: print(regressor.final_testX)
138/153: res=regressor.predict(X)
138/154: print(regressor.final_testX)
138/155:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(np.vander(self.orig_X[:,1], self.degree+1, increasing=True))
        self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/156:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/157: regressor.fit(X, Y)
138/158: res=regressor.predict(X)
138/159: print(regressor.final_testX)
138/160: r2_score(Y, res)
138/161:
plt.scatter(X, Y)
plt.scatter(X, res, color='red')
138/162:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(np.vander(self.orig_X[:,1], self.degree+1, increasing=True))
        self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/163: regressor.fit(np.hstack((X,Y)), Y)
138/164: res=regressor.predict(np.hstack((X,Y)))
138/165: r2_score(Y, res)
138/166:
plt.scatter(X, Y)
plt.scatter(X, res, color='red')
138/167: regressor.B
138/168: regressor.X
138/169: Y
138/170: X
138/171: regressor.X
138/172: print(regressor.final_testX)
138/173: r2_score(Y, res)
138/174:
np.matmul(np.linalg.pinv(np.matmul(Y.T, X)),
                            np.matmul(Y.T, Y))
138/175:
np.matmul(np.linalg.pinv(np.matmul(Y.T, Y)),
                            np.matmul(Y.T, Y))
138/176:
np.matmul(np.linalg.pinv(np.matmul(np.vstack(X,Y).T, np.vstack(X,Y))),
                            np.matmul(np.vstack(X,Y).T, Y))
138/177:
np.matmul(np.linalg.pinv(np.matmul(np.vstack((X,Y)).T, np.vstack((X,Y)))),
                            np.matmul(np.vstack((X,Y)).T, Y))
138/178:
np.matmul(np.linalg.pinv(np.matmul(np.hstack((X,Y)).T, np.hstack((X,Y)))),
                            np.matmul(np.hstack((X,Y)).T, Y))
138/179: np.hstack((X,Y))
138/180:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(self.X)
        self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))
        print(self.B)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/181:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/182: regressor.fit(np.hstack((X,Y)), Y)
138/183:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(self.X-self.Y)
        self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))
        print(self.B)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/184:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/185: regressor.fit(np.hstack((X,Y)), Y)
138/186:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        print(self.X-self.Y)
        self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                            np.matmul(self.X.T, self.Y))
        print(self.B)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/187:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/188: regressor.fit(np.hstack((X,Y)), Y)
138/189: regressor.X
138/190: print(regressor.final_testX)
138/191: print(regressor.final_testX)
138/192: regressor.predict(np.hstack((X,Y)))
138/193: print(regressor.final_testX)
138/194: r2_score(Y, res)
138/195:
plt.scatter(X, Y)
plt.scatter(X, res, color='red')
138/196: regressor.predict(np.hstack((X,Y)))
138/197: print(regressor.final_testX)
138/198:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None
        self.vander = []

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/199:
regressor=HigherOrderRegressor(5)
X = X.reshape(-1, 1)  # Reshape X to have a single column if it's 1D
Y = Y.reshape(-1, 1)
138/200: regressor.fit(np.hstack((X,Y)), Y)
138/201: regressor.predict(np.hstack((X,Y)))
138/202: res=regressor.predict(np.hstack((X,Y)))
138/203: print(regressor.final_testX)
138/204: r2_score(Y, res)
138/205: from sklearn.metrics import r2_score
138/206:
plt.scatter(X, Y)
plt.scatter(X, res, color='red')
139/1:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
139/2:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
    
    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
138/207: regressor=HigherOrderRegressor(5)
138/208: regressor.cross_validation()
138/209: regressor.cross_validation([2,3,4])
138/210: regressor.fit(X,Y)
138/211: regressor.cross_validation([2,3,4])
138/212:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials.back()*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/213: regressor=HigherOrderRegressor(5)
138/214: regressor.fit(X,Y)
138/215: regressor.cross_validation([2,3,4])
138/216:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/217: regressor=HigherOrderRegressor(5)
138/218: regressor.fit(X,Y)
138/219: regressor.cross_validation([2,3,4])
138/220:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/221: regressor=HigherOrderRegressor(5)
138/222: regressor.fit(X,Y)
138/223: regressor.cross_validation([2,3,4])
138/224:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/225: regressor=HigherOrderRegressor(5)
138/226: regressor.fit(X,Y)
138/227: regressor.cross_validation([2,3,4])
138/228:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices][:,0], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/229:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X, self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices][:,1], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/230: regressor=HigherOrderRegressor(5)
138/231: regressor.fit(X,Y)
138/232: regressor.cross_validation([2,3,4])
138/233:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices][:,1], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/234: regressor=HigherOrderRegressor(5)
138/235: regressor.fit(X,Y)
138/236: regressor.cross_validation([2,3,4])
138/237:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices][0][:,1], plot_values[indices][0], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/238: regressor=HigherOrderRegressor(5)
138/239: regressor.fit(X,Y)
138/240: regressor.cross_validation([2,3,4])
138/241:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/242: regressor=HigherOrderRegressor(5)
138/243: regressor.fit(X,Y)
138/244: regressor.cross_validation([2,3,4])
138/245:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(indices)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/246: regressor=HigherOrderRegressor(5)
138/247: regressor.fit(X,Y)
138/248: regressor.cross_validation([2,3,4])
138/249:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/250:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[indices])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/251:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/252:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[indices][0])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/253:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/254:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[indices][:,0])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/255:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/256:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[indices][:,0][:1])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/257:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[indices][:,0][:,1])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/258:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/259:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X)
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/260:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/261:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[:,1][indices])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/262:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/263:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X)
            print(self.orig_X[:,1])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/264:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/265:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            print(self.orig_X[:,1])
            plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/266:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/267:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            print(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/268:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/269:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            print(plot_values)
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/270:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/271:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(self.r2_score(Y_test, predictions))
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            print(plot_values)
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/272:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/273:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            print(self.r2_score(Y_test, predictions))
            average_r2 += self.r2_score(Y_test, predictions)
        try:
            B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/274:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/275:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        print(self.X.shape)
        try:
            B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/276:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/277:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        print(self.X.shape)
        try:
            B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            print((factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/278:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/279:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        print(self.X.shape)
        try:
            B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
            print(factorials[-1])
        for param in params:
            self.degree = param
            print((factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/280:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/281:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        print(self.X.shape)
        try:
            B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features-1]//factorials[self.num_features-1])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/282:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/283:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/284:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
138/285:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
139/3:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
139/4:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
139/5:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
139/6: df=pd.read_csv('train.csv')
139/7:
import pandas as pd
import numpy as np
139/8: df=pd.read_csv('train.csv')
139/9:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
139/10:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
139/11: y=df['yield']
139/12: from sklearn.model_selection import train_test_split
139/13: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
139/14:
# regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
139/15: preds=regressor.predict(X_test)
139/16: from sklearn.metrics import mean_absolute_error, r2_score
139/17: r2_score(y_test, preds)
139/18: mean_absolute_error(y_test, preds)
139/19:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
139/20: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
139/21:
# regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
139/22: preds=regressor.predict(X_test)
139/23: from sklearn.metrics import mean_absolute_error, r2_score
139/24: mean_absolute_error(y_test, preds)
139/25:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
139/26:
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
139/27: from sklearn.preprocessing import PolynomialFeatures
139/28:
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
139/29:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/30: reg=MultiVariateRegressor(3)
139/31:
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/32: LinearRegression().fit(X_train, y_train).coef_
139/33: reg.B
139/34: reg.fit(X_train, y_train)
139/35: reg.fit(X_train, y_train.to_numpy())
139/36: reg.B
139/37:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
139/38:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/39:
import pandas as pd
import numpy as np
139/40: df=pd.read_csv('train.csv')
139/41:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
139/42:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
139/43: y=df['yield']
139/44: from sklearn.model_selection import train_test_split
139/45: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
139/46: from sklearn.metrics import mean_absolute_error, r2_score
139/47:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
    
    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/48: reg=MultiVariateRegressor(3)
139/49: reg.fit(X_train.values, y_train.values)
139/50: mean_absolute_error(y_test, reg.predict(X_test.values))
139/51:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/52:
import pandas as pd
import numpy as np
from tqdm import tqdm
139/53:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            print("Singular matrix")
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
    
    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/54: reg=MultiVariateRegressor(3)
139/55: reg.fit(X_train.values, y_train.values)
139/56: reg=MultiVariateRegressor(3)
139/57: reg.fit(X_train, y_train.values)
139/58: mean_absolute_error(y_test, reg.predict(X_test.values))
139/59: mean_absolute_error(y_test, reg.predict(X_test))
139/60: r2_score(y_test, reg.predict(X_test.values))
139/61: r2_score(y_test, reg.predict(X_test))
139/62:
import pandas as pd
import numpy as np
139/63: df=pd.read_csv('train.csv')
139/64:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
139/65:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
139/66: y=df['yield']
139/67: from sklearn.model_selection import train_test_split
139/68: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
139/69:
# regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
139/70: preds=regressor.predict(X_test)
139/71: from sklearn.metrics import mean_absolute_error, r2_score
139/72: r2_score(y_test, preds)
139/73:
for coef in regressor.coef_:
    print(coef)
139/74: from sklearn.preprocessing import PolynomialFeatures
139/75:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def generate_powers(self, x):
        powers = [1]
        for i in range(1, self.degree+1):
            powers.append(powers[-1]*x)
        return np.array(powers)
    
    def generate_sorted_vander(self, X, degree):
        num_samples, num_features = X.shape 
        vander_matrix = [np.ones(num_samples)]
        
        for d in range(1, degree+1):
            for indices in np.ndindex(*(num_features,)*d):
                if sum(indices) == d:                
                    vander_matrix.append(np.prod(X ** indices, axis=1))
        return np.column_stack(vander_matrix)
    
    def predict_one(self, x):
        return np.dot(self.generate_powers(x), self.B)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        vectorized_predict_one = np.vectorize(self.predict_one)
        return vectorized_predict_one(X)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            B = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            # average_r2 += r2_score(Y_test, predictions)
            average_r2 += self.r2_score(Y_test, predictions)
        B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=np.vander(self.orig_X, maximum_param+1, increasing=True)
        responses = []
        for param in params:
            self.degree = param
            self.X=self.max_X[:,:param+1]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            # fig=plt.figure()
            # plt.scatter(self.orig_X, self.Y)
            # indices=np.argsort(self.orig_X)
            # plt.plot(self.orig_X[indices], plot_values[indices], 'red')
            # fig.savefig(f"degree_{param}.png")
        print(responses)
139/76: reg=MultiVariateRegressor(3)
139/77: reg.fit(X_train.values, y_train.values)
139/78: r2_score(y_test, reg.predict(X_test.values))
139/79: mean_absolute_error(y_test, reg.predict(X_test.values))
139/80:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/81:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/82:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/83: reg=MultiVariateRegressor(3)
139/84: reg.fit(X_train.values, y_train.values)
139/85: reg.fit(X_train, y_train.values)
139/86: r2_score(y_test, reg.predict(X_test))
139/87: mean_absolute_error(y_test, reg.predict(X_test))
139/88:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
139/89:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/90:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/91:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
from sklearn.preprocessing import StandardScaler
ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/92:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            print("Singular Matrix")
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        # regres=LinearRegression(fit_intercept=False).fit(self.X, self.Y)
        # for coef in zip(regres.coef_, self.B):
        #     print(coef)

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/93: print(X_train[:,0])
139/94:
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}
139/95: reg=MultiVariateRegressor(3)
139/96: reg.fit(X_train, y_train.values)
139/97:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            print("Singular Matrix")
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/98: reg=MultiVariateRegressor(3)
139/99: reg.fit(X_train, y_train.values)
139/100:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            print("Singular Matrix")
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def predict(self, X):
        return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/101: reg=MultiVariateRegressor(3)
139/102: reg.fit(X_train, y_train.values)
139/103: r2_score(y_test, reg.predict(X_test))
139/104:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def predict(self, X):
        if X.shape[0]
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)


    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/105:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/106: reg=MultiVariateRegressor(3)
139/107: reg.fit(X_train, y_train.values)
139/108: r2_score(y_test, reg.predict(X_test))
139/109: mean_absolute_error(y_test, reg.predict(X_test))
139/110:
# regressor=LassoCV(cv=5, random_state=0).fit(X_train, y_train)
regressor=LinearRegression().fit(X_train, y_train)
139/111: preds=regressor.predict(X_test)
139/112: from sklearn.metrics import mean_absolute_error, r2_score
139/113: r2_score(y_test, preds)
139/114: reg=MultiVariateRegressor(3)
139/115: reg.fit(X_train, y_train.values)
139/116: r2_score(y_test, reg.predict(X_test))
139/117: mean_absolute_error(y_test, reg.predict(X_test))
139/118:
for coef in zip(reg.B, regressor.coef_):
    print(coef)
139/119:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        try:
            self.B = np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            print("Singular")
            self.B = np.matmul(pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/120: reg=MultiVariateRegressor(3)
139/121: reg.fit(X_train, y_train.values)
139/122:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/123:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/124: print(reg.B)
139/125: print(reg.X)
139/126: print(reg.X.shape)
139/127:
print(reg.X.shape)
print(LinearRegression().fit(X_train, y_train).coef_.shape)
139/128:
print(reg.X.shape)
lstsq(reg.X, reg.Y)
139/129: print(reg.B)
139/130:
print(reg.X.shape)
lstsq(reg.X, reg.Y).shape
139/131:
print(reg.X.shape)
lstsq(reg.X, reg.Y)
139/132: np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, y_train))
139/133: np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, y_train))
139/134:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/135:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/136: reg=MultiVariateRegressor(3)
139/137:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/138:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
138/286:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
138/287:
regressor=HigherOrderRegressor(5)
regressor.fit(X, Y)
regressor.cross_validation([2,3,4])
139/139:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
139/140:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/141:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.linalg.lstsq(self.X, self.Y, rcond=None)[0]

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
139/142: reg=MultiVariateRegressor(3)
139/143: reg.fit(X_train, y_train.values)
139/144: r2_score(y_test, reg.predict(X_test))
139/145: mean_absolute_error(y_test, reg.predict(X_test))
139/146:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/147:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/148:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
139/149:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
139/150:
import statsmodels.api as sm
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Load your dataset
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(included[worst_feature])
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
139/151:

import statsmodels.api as sm
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Load your dataset
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(included[worst_feature])
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
139/152:
import statsmodels.api as sm
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(included[worst_feature])
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
139/153:
import statsmodels.api as sm
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(included[worst_feature])
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
139/154:
import statsmodels.api as sm
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature_index = pvalues.argmax()
            included.pop(worst_feature_index)
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
139/155:
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.predict(sm.add_constant(X_test[:, selected_features])))
139/156: r2_score(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
139/157: mean_absolute_error(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
139/158: reg.fit(sm.add_constant(X_train[:, selected_features]), y_train)
139/159: reg.fit(sm.add_constant(X_train[:, selected_features]), y_train.values)
139/160: r2_score(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
139/161: r2_score(y_test, reg.predict(sm.add_constant(X_test[:, selected_features])))
139/162: mean_absolute_error(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
139/163:
def write_output():
    # X_all = np.vstack((X_train, X_test))
    # y_all = np.concatenate((y_train, y_test))
    reg.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
139/164:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
# X=test_data.drop(['id', 'Row#'], axis=1)
# X=PolynomialFeatures(degree=2).fit_transform(X)
# preds=reg.predict(X)
# test_data['yield']=preds
# test_data=test_data[['id', 'yield']]
# test_data.to_csv('output.csv', index=False)
139/165:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
# X=PolynomialFeatures(degree=2).fit_transform(X)
# preds=reg.predict(X)
# test_data['yield']=preds
# test_data=test_data[['id', 'yield']]
# test_data.to_csv('output.csv', index=False)
139/166:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('output.csv', index=False)
139/167:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
139/168:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
print(preds)
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
139/169:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
139/170:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
# test_data['yield']=preds
# test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
139/171:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
# test_data['yield']=preds
# test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
139/172:
data=pd.read_csv('train.csv')
data.to_csv('hello.csv')
140/1:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
# test_data['yield']=preds
# test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
140/2:
import pandas as pd
import numpy as np
140/3:
import pandas as pd
import numpy as np
141/1:
import pandas as pd
import numpy as np
141/2:
data=pd.read_csv('train.csv')
data.to_csv('hello.csv')
141/3:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/4:
import pandas as pd
import numpy as np
141/5: df=pd.read_csv('train.csv')
141/6:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
141/7:
X=df.drop(['yield', 'id', 'Row#', 'honeybee'], axis=1)
# X=df.drop('yield', axis=1)
141/8:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
141/9: y=df['yield']
141/10: from sklearn.model_selection import train_test_split
141/11: from sklearn.preprocessing import PolynomialFeatures
141/12:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.linalg.lstsq(self.X, self.Y, rcond=None)[0]

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
141/13: reg=MultiVariateRegressor(3)
141/14:
def write_output():
    # X_all = np.vstack((X_train, X_test))
    # y_all = np.concatenate((y_train, y_test))
    reg.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))
    test_data=pd.read_csv('test.csv')
    X=test_data.drop(['id', 'Row#'], axis=1)
    X=PolynomialFeatures(degree=2).fit_transform(X)
    preds=reg.predict(X)
    test_data['yield']=preds
    test_data=test_data[['id', 'yield']]
    test_data.to_csv('output.csv', index=False)
141/15: write_output()
141/16:
import statsmodels.api as sm
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature_index = pvalues.argmax()
            included.pop(worst_feature_index)
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
141/17:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/18:
import statsmodels.api as sm
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature_index = pvalues.argmax()
            included.pop(worst_feature_index)
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
141/19: reg.fit(sm.add_constant(X_train[:, selected_features]), y_train.values)
141/20: r2_score(y_test, reg.predict(sm.add_constant(X_test[:, selected_features])))
141/21:
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
141/22: r2_score(y_test, reg.predict(sm.add_constant(X_test[:, selected_features])))
141/23: mean_absolute_error(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
141/24:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#', 'honeybee'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/25:
reg.fit(sm.add_constant(np.vstack((X_train, X_test))[:,selected_features]), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(sm.add_constant(X[:, selected_features]))
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/26:
orig_observations=pd.read_csv('output.csv')['yield']
new_observations=pd.read_csv('hello.csv')['yield']
141/27: r2_score(orig_observations, new_observations)
141/28: mean_absolute_error(orig_observations, new_observations)
141/29: print(selected_features)
141/30: selected_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 75, 103, 127, 93, 84, 141, 148, 107, 151, 129, 105, 140, 147, 149, 152, 95]
141/31: reg.fit(sm.add_constant(X_train[:, selected_features]), y_train.values)
141/32: r2_score(y_test, reg.predict(sm.add_constant(X_test[:, selected_features])))
141/33: mean_absolute_error(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
141/34: mean_absolute_error(y_test, final_model.predict(sm.add_constant(X_test[:, selected_features])))
141/35: reg.fit(X_train[:, selected_features], y_train.values)
141/36: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/37: mean_absolute_error(y_test, final_model.predict(X_test[:, selected_features]))
141/38: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/39: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/40: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/41:
reg.fit(np.vstack((X_train, X_test))[:,selected_features], np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(X[:, selected_features])
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/42:
orig_observations=pd.read_csv('output.csv')['yield']
new_observations=pd.read_csv('hello.csv')['yield']
141/43: r2_score(orig_observations, new_observations)
141/44: mean_absolute_error(orig_observations, new_observations)
141/45:
import statsmodels.api as sm
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.001, threshold_out=0.1):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature_index = pvalues.argmax()
            included.pop(worst_feature_index)
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
141/46: reg.fit(X_train[:, selected_features], y_train.values)
141/47: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/48: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/49: print(selected_features)
141/50:
import statsmodels.api as sm
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Function to perform stepwise regression
def stepwise_selection(X, y, initial_features=[], threshold_in=0.05, threshold_out=0.1):
    included = list(initial_features)
    while True:
        changed = False

        # Forward step
        excluded = list(set(range(X.shape[1])) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included + [new_column]]]))).fit()
            new_pval[new_column] = model.pvalues[-1]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True

        # Backward step
        model = sm.OLS(y, sm.add_constant(np.column_stack([X[:, included]]))).fit()
        pvalues = model.pvalues[1:]  # Exclude the intercept
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature_index = pvalues.argmax()
            included.pop(worst_feature_index)
            changed = True

        if not changed:
            break

    return included

# Perform stepwise regression
selected_features = stepwise_selection(X_train, y_train)
print("Selected features:", selected_features)

# Fit a final model using the selected features
final_model = sm.OLS(y_train, sm.add_constant(X_train[:, selected_features])).fit()
print(final_model.summary())
141/51: reg.fit(X_train[:, selected_features], y_train.values)
141/52: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/53: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/54: print(selected_features)
141/55: selected_features=sorted(selected_features)
141/56: selected_features
141/57: print(selected_features)
141/58: selected_features=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20, 21, 36, 75, 84, 93, 95, 103, 105, 112, 116, 127, 129, 140, 141, 147, 148, 149, 151, 152]
141/59: reg.fit(X_train[:, selected_features], y_train.values)
141/60: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/61: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/62: selected_features=[0, 1,2,3,4,5,6,7,8,9,10,11,12,13, 14, 15, 16, 21, 36, 75, 84, 93, 95, 103, 105, 112, 116, 127, 129, 140, 141, 147, 148, 149, 151, 152]
141/63: reg.fit(X_train[:, selected_features], y_train.values)
141/64: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/65: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/66: selected_features=[0, 12, 14, 16, 21, 36, 75, 84, 93, 95, 103, 105, 112, 116, 127, 129, 140, 141, 147, 148, 149, 151, 152]
141/67: reg.fit(X_train[:, selected_features], y_train.values)
141/68: r2_score(y_test, reg.predict(X_test[:, selected_features]))
141/69: mean_absolute_error(y_test, reg.predict(X_test[:, selected_features]))
141/70: X_train[:,14]
141/71: X_train[:,16]
141/72:
import numpy as np

class GradientBoostingRegressor:
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.models = []

    def fit(self, X, y):
        # Initialize the model with the mean of y
        y_pred = np.full(y.shape, np.mean(y))
        self.models.append(np.mean(y))

        for _ in range(self.n_estimators):
            # Compute the residuals (negative gradient)
            residuals = y - y_pred
            
            # Fit a new decision tree to the residuals
            tree = self._fit_tree(X, residuals)
            self.models.append(tree)

            # Update predictions
            y_pred += self.learning_rate * self._predict_tree(tree, X)

    def _fit_tree(self, X, residuals):
        n_samples, n_features = X.shape
        best_split = None
        best_score = float('inf')

        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                left_mask = X[:, feature] <= threshold
                right_mask = X[:, feature] > threshold

                left_residuals = residuals[left_mask]
                right_residuals = residuals[right_mask]

                score = (np.var(left_residuals) * len(left_residuals) + 
                         np.var(right_residuals) * len(right_residuals))

                if score < best_score:
                    best_score = score
                    best_split = (feature, threshold)

        return best_split

    def _predict_tree(self, tree, X):
        feature, threshold = tree
        predictions = np.where(X[:, feature] <= threshold, 1, -1)
        return predictions

    def predict(self, X):
        y_pred = np.full(X.shape[0], self.models[0])
        for tree in self.models[1:]:
            y_pred += self.learning_rate * self._predict_tree(tree, X)
        return y_pred

# Use your dataset
gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)

# Train the model using your training data
gb_regressor.fit(X_train, y_train)

# Make predictions on the test data
predictions = gb_regressor.predict(X_test)

# Evaluate the model using mean absolute error
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, predictions)
print(f"Mean Absolute Error: {mae}")
142/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
142/2:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
142/3:
input_ads = pd.read_csv('train.csv')

#-----------------------------------------------------------------
print(input_ads.shape)
input_ads.head()
142/4:
X = input_ads.drop(['yield', 'id', 'Row#'], axis=1)
y = input_ads['yield']

X, X_test, y, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

#--------------------------------------------------------------------------------
#Scaling the datasets
scaler = StandardScaler()

X_arr = scaler.fit_transform(X)
X_test_arr = scaler.fit_transform(X_test)

y_arr = np.array(y).reshape(X_arr.shape[0],1)
y_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)

#--------------------------------------------------------------------------------
print(X_arr.shape)
print(X_test_arr.shape)
print(y_arr.shape)
142/5:
from sklearn.tree import DecisionTreeRegressor

#---------------------------------------------------------------------------------------------------------
#Loss func
def loss_calc(y_true,y_pred):
    
    loss = (1/len(y_true)) * 0.5*np.sum(np.square(y_true-y_pred))
        
    return loss

#---------------------------------------------------------------------------------------------------------
#Gradient Calc
def gradient_calc(y_true,y_pred):
    
    grad = -(y_true-y_pred)
    
    return grad

#---------------------------------------------------------------------------------------------------------
#The base estimator
def tree_creator(r_state,X,y):
    
    d_tree = DecisionTreeRegressor(random_state=r_state,criterion='mse',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
    d_tree.fit(X,y)
    
    return d_tree

#---------------------------------------------------------------------------------------------------------
#Predicting through gradient boosting regression
def predict_grad_boost(models_tray,alpha,test_x=X_test_arr,train_y=y_arr):
    
    initial_pred = np.array([np.mean(train_y)] * len(test_x))
        
    final_pred = initial_pred.reshape(len(initial_pred),1)
    #print(final_pred.shape)
    
    for i in range(len(models_tray)):
        
        model = models_tray[i]
        temp_pred = (model.predict(test_x)).reshape(len(test_x),1)
        #print(temp_pred.shape)
        final_pred -= alpha * temp_pred
    
    return final_pred
142/6:
def grad_boost_train(train_x,train_y,alpha=0.01,r_state=100,n_iters=101):

    model_tray = [] #Tray to collect the trained boosted stage estimators
    loss_counter = [] #Tray for loss capture

    
    initial_pred = np.array([np.mean(train_y)] * len(train_y))

    print('Initial val :',initial_pred.shape)
    model_pred = initial_pred.reshape(len(initial_pred),1)

    for epoch in range(n_iters): #Unit iteration

        if epoch%100==0:
            print('#---------- Epoch number :',epoch,' -----------#')
        
        #Calculating loss
        loss = loss_calc(y_true=train_y,
                         y_pred=model_pred)

        loss_counter.append(loss)
        
        #Calculating the gradient (residuals)
        grads = gradient_calc(y_true=train_y,
                              y_pred=model_pred)
        #print(grads.shape)
        #Building the regression tree on the gradient (residuals)
        tree_grad = tree_creator(r_state=r_state,
                                 X=train_x,
                                 y=grads)
        #print(train_x.shape)
        #print(tree_grad.predict(train_x).shape)
        
        #Predicting the residuals according to the tree fit above
        pred_m = (tree_grad.predict(train_x)).reshape(len(train_x),1)
        
        #Updating model through learning rate
        model_pred -= alpha * pred_m
        
        #Appending the model into tray
        model_tray.append(tree_grad)
        
    return model_tray,loss_counter,initial_pred
142/7:
n_estimators = 1001 #No of boosting steps
alpha =0.01 #Learning rate

#Training gradient boosting regression
models_list,loss_counter,initial_pred = grad_boost_train(train_x=X_arr,
                                                         train_y=y_arr,
                                                         alpha=alpha,
                                                         r_state=100,
                                                         n_iters=n_estimators)
142/8:
n_estimators = 1001 #No of boosting steps
alpha =0.01 #Learning rate

#Training gradient boosting regression
models_list,loss_counter,initial_pred = grad_boost_train(train_x=X_arr,
                                                         train_y=y_arr,
                                                         alpha=alpha,
                                                         r_state=100,
                                                         n_iters=n_estimators)
142/9:
from sklearn.tree import DecisionTreeRegressor

#---------------------------------------------------------------------------------------------------------
#Loss func
def loss_calc(y_true,y_pred):
    
    loss = (1/len(y_true)) * 0.5*np.sum(np.square(y_true-y_pred))
        
    return loss

#---------------------------------------------------------------------------------------------------------
#Gradient Calc
def gradient_calc(y_true,y_pred):
    
    grad = -(y_true-y_pred)
    
    return grad

#---------------------------------------------------------------------------------------------------------
#The base estimator
def tree_creator(r_state,X,y):
    
    d_tree = DecisionTreeRegressor(random_state=r_state,criterion='absolute_error',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
    d_tree.fit(X,y)
    
    return d_tree

#---------------------------------------------------------------------------------------------------------
#Predicting through gradient boosting regression
def predict_grad_boost(models_tray,alpha,test_x=X_test_arr,train_y=y_arr):
    
    initial_pred = np.array([np.mean(train_y)] * len(test_x))
        
    final_pred = initial_pred.reshape(len(initial_pred),1)
    #print(final_pred.shape)
    
    for i in range(len(models_tray)):
        
        model = models_tray[i]
        temp_pred = (model.predict(test_x)).reshape(len(test_x),1)
        #print(temp_pred.shape)
        final_pred -= alpha * temp_pred
    
    return final_pred
142/10:
n_estimators = 1001 #No of boosting steps
alpha =0.01 #Learning rate

#Training gradient boosting regression
models_list,loss_counter,initial_pred = grad_boost_train(train_x=X_arr,
                                                         train_y=y_arr,
                                                         alpha=alpha,
                                                         r_state=100,
                                                         n_iters=n_estimators)
142/11:
manual_gbm_pred = predict_grad_boost(models_tray=models_list, #Passing the fitted estimators into the predict function
                                     alpha=alpha, #The alpha val used during training
                                     test_x=X_test_arr) #Test dataset
142/12: from sklearn.metrics import mean_absolute_error
142/13: mean_absolute_error(y_test_arr,manual_gbm_pred)
142/14: from sklearn.ensemble import GradientBoostingRegressor
142/15:

skl_gbm = GradientBoostingRegressor(random_state=100,n_estimators=1001,criterion='mse',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
142/16: skl_gbm.fit(X_arr,y_arr)
142/17:

skl_gbm = GradientBoostingRegressor(random_state=100,n_estimators=1001,criterion='absolute_error',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
142/18: skl_gbm.fit(X_arr,y_arr)
142/19:

skl_gbm = GradientBoostingRegressor(random_state=100,n_estimators=1001,criterion='squared_error',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
142/20: skl_gbm.fit(X_arr,y_arr)
142/21: skl_pred = skl_gbm.predict(X_test_arr)
142/22: np.sqrt(mean_squared_error(y_test_arr,skl_pred))
142/23: mean_absolute_error(y_test_arr,skl_pred)
141/73: GradientBoostingRegressor().fit(X_train, y_train)
141/74: grad_Reg=GradientBoostingRegressor().fit(X_train, y_train)
141/75: grad_Reg=GradientBoostingRegressor().fit(X_train, y_train)
141/76: from sklearn.ensemble import GradientBoostingRegressor
141/77: grad_Reg=GradientBoostingRegressor().fit(X_train, y_train)
141/78: grad_Reg=GradientBoostingRegressor().fit(X_train, y_train)
142/24: X['fruitset'][X['fruitset'] < 0.4]
142/25: X['yield'][X['fruitset'] < 0.4]
141/79: df['yield'][df['fruitset'] < 0.4]
141/80: df['fruitset'][df['fruitset'] < 0.4]
141/81: df['fruitset'][df['fruitset'] < 0.4].shape
141/82: df['fruitset'][df['fruitset'] < 0.4].mean
141/83: df['fruitset'][df['fruitset'] < 0.4].mean()
141/84: df['yield'][df['fruitset'] < 0.4].mean()
141/85: df['yield'][df['fruitset'] < 0.4].std()
141/86: df['yield'][df['fruitset'] > 0.6].std()
141/87: df['fruitset'].value_counts()
141/88: df['fruitset'].value_counts().shape
141/89:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
141/90:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=3).fit_transform(X_train)
X_test = PolynomialFeatures(degree=3).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/91:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/92:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=4).fit_transform(X_train)
X_test = PolynomialFeatures(degree=4).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/93:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/94:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/95:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/96:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/97:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/98:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/99: reg.B
141/100: B=reg.B[np.abs(reg.B) > 0.0001]
141/101: mean_absolute_error(y_test, np.dot(X_test, B))
141/102: B=reg.B[np.abs(reg.B) > 0.001]
141/103: mean_absolute_error(y_test, np.dot(X_test, B))
141/104: B=reg.B[np.abs(reg.B) > 0.01]
141/105: mean_absolute_error(y_test, np.dot(X_test, B))
141/106: B=reg.B[np.abs(reg.B) > 0.1]
141/107: mean_absolute_error(y_test, np.dot(X_test, B))
141/108: B=reg.B*(np.abs(reg.B) > 0.1)
141/109: mean_absolute_error(y_test, np.dot(X_test, B))
141/110: B=reg.B*(np.abs(reg.B) > 0.05)
141/111: mean_absolute_error(y_test, np.dot(X_test, B))
141/112: B=reg.B*(np.abs(reg.B) > 0.01)
141/113: mean_absolute_error(y_test, np.dot(X_test, B))
141/114: cols=np.where(np.abs(reg.B) > 0.01)[0]
141/115: cols
141/116: reg.fit(X_train[:, cols], y_train)
141/117: reg.fit(X_train[:, cols], y_train.values)
141/118: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/119: cols=np.where(np.abs(reg.B) > 0.1)[0]
141/120: reg.fit(X_train[:, cols], y_train.values)
141/121: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/122: reg.fit(X_train, y_train.values)
141/123: cols=np.where(np.abs(reg.B) > 0.1)[0]
141/124: reg.fit(X_train[:,cols], y_train.values)
141/125: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/126:
reg.fit(np.vstack((X_train, X_test))[:,selected_features], np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(X[:, cols])
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/127: X.shape
141/128: X_train.shape
141/129: reg.B.shape
141/130: X[:,cols].shape
141/131: reg.fit(X_train[:,cols], y_train.values)
141/132: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/133:
reg.fit(np.vstack((X_train, X_test))[:,selected_features], np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(X[:, cols])
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/134: X[:,cols].shape
141/135: cols.size
141/136: X_train[:,cols]
141/137: X_train[:,cols].shap
141/138: X_train[:,cols].shape
141/139: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/140: reg.fit(X_train[:,cols], y_train.values)
141/141: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/142:
reg.fit(np.vstack((X_train, X_test))[:,cols], np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(X[:, cols])
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/143:
orig_observations=pd.read_csv('output.csv')['yield']
new_observations=pd.read_csv('hello.csv')['yield']
141/144: mean_absolute_error(orig_observations, new_observations)
141/145: cols=np.where(np.abs(reg.B) > 0.2)[0]
141/146: reg.fit(X_train[:,cols], y_train.values)
141/147: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/148: cols=np.where(np.abs(reg.B) > 0.3)[0]
141/149: reg.fit(X_train[:,cols], y_train.values)
141/150: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/151: cols=np.where(np.abs(reg.B) > 1)[0]
141/152: reg.fit(X_train[:,cols], y_train.values)
141/153: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/154: cols=np.where(np.abs(reg.B) > 10)[0]
141/155: reg.fit(X_train[:,cols], y_train.values)
141/156: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/157: cols=np.where(np.abs(reg.B) > 20)[0]
141/158: reg.fit(X_train[:,cols], y_train.values)
141/159: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/160: cols=np.where(np.abs(reg.B) > 25)[0]
141/161: reg.fit(X_train[:,cols], y_train.values)
141/162: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/163: cols=np.where(np.abs(reg.B) > 100)[0]
141/164: reg.fit(X_train[:,cols], y_train.values)
141/165: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/166: cols=np.where(np.abs(reg.B) > 0.1)[0]
141/167: reg.fit(X_train[:,cols], y_train.values)
141/168: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/169: cols=np.where(np.abs(reg.B) > 0.01)[0]
141/170: reg.fit(X_train[:,cols], y_train.values)
141/171: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/172:
reg.fit(X_train, y_train)
cols=np.where(np.abs(reg.B) > 0.01)[0]
141/173:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 0.01)[0]
141/174: reg.fit(X_train[:,cols], y_train.values)
141/175: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/176:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 0.1)[0]
141/177: reg.fit(X_train[:,cols], y_train.values)
141/178: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/179:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 1)[0]
141/180: reg.fit(X_train[:,cols], y_train.values)
141/181: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/182:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 10)[0]
141/183: reg.fit(X_train[:,cols], y_train.values)
141/184: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/185:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 100)[0]
141/186: reg.fit(X_train[:,cols], y_train.values)
141/187: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/188:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
141/189: reg.fit(X_train[:,cols], y_train.values)
141/190: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/191:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 500)[0]
141/192: reg.fit(X_train[:,cols], y_train.values)
141/193: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/194:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 1000)[0]
141/195: reg.fit(X_train[:,cols], y_train.values)
141/196: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/197:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
141/198: reg.fit(X_train[:,cols], y_train.values)
141/199: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/200:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 0.01)[0]
141/201: reg.fit(X_train[:,cols], y_train.values)
141/202: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/203:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
141/204: reg.fit(X_train[:,cols], y_train.values)
141/205: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/206: cols
141/207:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
if 0 not in cols:
    cols=np.append(cols, 0)
141/208: reg.fit(X_train[:,cols], y_train.values)
141/209: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/210: cols
141/211:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
# if 0 not in cols:
#     cols=np.append(cols, 0)
141/212: reg.fit(X_train[:,cols], y_train.values)
141/213: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/214:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
if 0 not in cols:
    cols=np.append(cols, 0)
141/215: reg.fit(X_train[:,cols], y_train.values)
141/216: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/217:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
if 0 not in cols:
    cols=np.append(cols, 0)
141/218: reg.fit(X_train[:,cols], y_train.values)
141/219: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/220:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
# if 0 not in cols:
#     cols=np.append(cols, 0)
141/221: reg.fit(X_train[:,cols], y_train.values)
141/222: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/223:
reg.fit(X_train, y_train.values)
cols=np.where(np.abs(reg.B) > 200)[0]
if 0 not in cols:
    cols=np.append(cols, 0)
141/224: reg.fit(X_train[:,cols], y_train.values)
141/225: mean_absolute_error(y_test, reg.predict(X_test[:, cols]))
141/226:
without_zero=268.6223732665417
with_zero=268.62237327093663
141/227:
reg.fit(np.vstack((X_train, X_test))[:,cols], np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=reg.predict(X[:, cols])
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
141/228:
orig_observations=pd.read_csv('output.csv')['yield']
new_observations=pd.read_csv('hello.csv')['yield']
141/229: mean_absolute_error(orig_observations, new_observations)
142/26:
import numpy as np

class GradientBoostingRegressor:
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []

    def _mse_split(self, X, y, feature_index, threshold):
        left_mask = X[:, feature_index] <= threshold
        right_mask = ~left_mask
        return left_mask, right_mask

    def _fit_tree(self, X, y):
        best_split = None
        best_mse = float('inf')
        best_left, best_right = None, None

        for feature_index in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_index])
            for threshold in thresholds:
                left_mask, right_mask = self._mse_split(X, y, feature_index, threshold)
                if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:
                    continue

                left_error = np.var(y[left_mask]) * len(y[left_mask])
                right_error = np.var(y[right_mask]) * len(y[right_mask])
                mse = left_error + right_error

                if mse < best_mse:
                    best_mse = mse
                    best_split = (feature_index, threshold)
                    best_left, best_right = left_mask, right_mask

        return best_split, best_left, best_right

    def _build_tree(self, X, y, depth):
        if depth == self.max_depth or len(np.unique(y)) == 1:
            return np.mean(y)

        split, left_mask, right_mask = self._fit_tree(X, y)
        if split is None:
            return np.mean(y)

        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)

        return (split, left_subtree, right_subtree)

    def fit(self, X, y):
        self.initial_prediction = np.mean(y)
        residuals = y - self.initial_prediction

        for _ in range(self.n_estimators):
            tree = self._build_tree(X, residuals, depth=0)
            self.trees.append(tree)

            predictions = self._predict_tree(tree, X)
            residuals -= self.learning_rate * predictions

    def _predict_tree(self, tree, X):
        if not isinstance(tree, tuple):
            return np.full(X.shape[0], tree)

        feature_index, threshold = tree[0]
        left_mask = X[:, feature_index] <= threshold
        right_mask = ~left_mask

        predictions = np.zeros(X.shape[0])
        predictions[left_mask] = self._predict_tree(tree[1], X[left_mask])
        predictions[right_mask] = self._predict_tree(tree[2], X[right_mask])

        return predictions

    def predict(self, X):
        predictions = np.full(X.shape[0], self.initial_prediction)

        for tree in self.trees:
            predictions += self.learning_rate * self._predict_tree(tree, X)

        return predictions
142/27:
# Instantiate the model
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)

# Fit the model on the training data
gbr.fit(X_train, y_train)

# Predict on the test data
predictions = gbr.predict(X_test)

# Calculate the Mean Absolute Error (MAE)
mae = np.mean(np.abs(y_test - predictions))
print(f"Mean Absolute Error: {mae}")
142/28: df=pd.read_csv('train.csv')
142/29: X=df.drop(['yield', 'id', 'Row#'],axis=1)
141/230:
X=df.drop(['yield'], axis=1)
# X=df.drop('yield', axis=1)
141/231:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/232:
X=df.drop(['yield'], axis=1)
# X=df.drop('yield', axis=1)
141/233:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df.drop('yield', axis=1)
141/234:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X['clonesize'].value_counts()
141/235:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X['bumbles'].value_counts()
141/236:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X['bumbles'].value_counts()
X['bumbles_is_38']=X['bumbles']==0.38
X['bumbles_is_25']=X['bumbles']==0.25
141/237:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/238:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X['bumbles'].value_counts()
X['bumbles_is_38']=X['bumbles']==0.38
X['bumbles_is_25']=X['bumbles']==0.25
141/239:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/240:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/241:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/242:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/243:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X['bumbles'].value_counts()
X['bumbles_is_38']=X['bumbles']==0.38
X['bumbles_is_25']=X['bumbles']==0.25
141/244:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X['bumbles'].value_counts()
X['bumbles_is_38']=X['bumbles']==0.38
X['bumbles_is_25']=X['bumbles']==0.25
X['clonesize_is_12.5']=X['clonesize']==12.5
X['clonesize_is_25']=X['clonesize']==25
141/245:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/246:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train = X_train.values
# X_test = X_test.values
# from sklearn.preprocessing import PolynomialFeatures
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# # X_train=ss.transform(X_train)
# # X_test=ss.transform(X_test)
X_train=X_train[:, lasso_regressor.coef_ != 0]
X_test=X_test[:, lasso_regressor.coef_ != 0]
# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/247:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/248:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X=np.asarray(X, dtype=np.float64)
y=np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/249:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X=np.asarray(X, dtype=np.float64)
y=np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
141/250:
data['CloneYieldRatio'] = data['yield'] / (data['clonesize'] + 1)

plt.figure(figsize=(8, 5))
sns.scatterplot(x='CloneYieldRatio', y='yield', data=data)
plt.title('Clone to Yield Ratio vs Yield')
plt.show()
141/251: from matplotlib import pyplot as plt
141/252:
val = df['yield'] / (df['clonesize'] + 1)

plt.figure(figsize=(8, 5))
plt.scatterplot(x=val, y=df['yield'])
plt.title('Clone to Yield Ratio vs Yield')
plt.show()
141/253:
val = df['yield'] / (df['clonesize'] + 1)

plt.figure(figsize=(8, 5))
plt.scatter(x=val, y=df['yield'])
plt.title('Clone to Yield Ratio vs Yield')
plt.show()
141/254: data_clonesize=df[df['clonesize'] == 12.5]
141/255:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize['yield']
141/256:
reg.fit(X_clone, y_clone)
mean_absolute_error(y_clone, reg.predict(X_clone))
141/257:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1).as_numpy()
y_clone=data_clonesize['yield'].as_numpy()
141/258:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize['yield']
141/259:
reg.fit(X_clone.values, y_clone.values)
mean_absolute_error(y_clone, reg.predict(X_clone.values))
141/260:
X_train, X_test, y_train, y_test = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
141/261:
reg.fit(X_clone.values, y_clone.values)
mean_absolute_error(y_clone, reg.predict(X_clone.values))
141/262:
reg.fit(X_train.values, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test.values))
141/263:
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
141/264: data_clonesize2=df[df['clonesize'] == 25]
141/265:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize['yield']
141/266:
X_clone=data_clonesize2.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize2['yield']
141/267:
X_clone2=data_clonesize2.drop(['yield', 'id', 'Row#'], axis=1)
y_clone2=data_clonesize2['yield']
141/268:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize['yield']
141/269:
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
141/270:
reg.fit(X_train2, y_train2.values)
mean_absolute_error(y_test2, reg.predict(X_test2))
141/271:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
X_train=np.hstack((X_train, X_train*(df['clonesize']==12.5), X_train*(df['clonesize']==25)))
141/272:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
X_train=np.hstack((X_train, 
                   X_train*(df['clonesize']==12.5), 
                   X_train*(df['clonesize']==25)))
141/273: X_train[:,0]
141/274: X_train[:,1]
141/275:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
X_train=np.hstack((X_train, 
                   X_train*(X_train[:,1]==12.5), 
                   X_train*(X_train[:,1]==25)))
141/276:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
X_train=np.hstack((X_train, 
                   X_train*((X_train[:,1]==12.5).reshape(-1, 1)), 
                   X_train*((X_train[:,1]==25).reshape(-1, 1))))
141/277: reg.fit(X_train, y_train.values)
141/278: reg.fit(X_train, y_train)
141/279: mean_absolute_error(y_test, reg.predict(X_test))
141/280:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
X_train=np.hstack((X_train, 
                   X_train*((X_train[:,1]==12.5).reshape(-1, 1)), 
                   X_train*((X_train[:,1]==25).reshape(-1, 1))))
X_test=np.hstack((X_test,
                    X_test*((X_test[:,1]==12.5).reshape(-1, 1)),
                    X_test*((X_test[:,1]==25).reshape(-1, 1))))
141/281: reg.fit(X_train, y_train)
141/282: mean_absolute_error(y_test, reg.predict(X_test))
141/283: X_train
141/284: X_train[0]
141/285:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.linalg.lstsq(self.X, self.Y, rcond=None)[0]
        X1=self.X[self.X[:,1]==12.5]
        X2=self.X[self.X[:,1]==25]
        y1=self.Y[self.X[:,1]==12.5]
        y2=self.Y[self.X[:,1]==25]
        self.B2 = np.linalg.lstsq(X1, y1, rcond=None)[0]
        self.B3 = np.linalg.lstsq(X2, y2, rcond=None)[0]

    def predict_one(self, x):
        if x[1] == 12.5:
            return np.dot(x, self.B2)
        if x[1] == 25:
            return np.dot(x, self.B3)
        return np.dot(x, self.B)
    
    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
            # return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.array([self.predict_one(x) for x in X])
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
141/286: regr=SpecialRegressor(3)
141/287:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
# X_train=np.hstack((X_train, 
#                    X_train*((X_train[:,1]==12.5).reshape(-1, 1)), 
#                    X_train*((X_train[:,1]==25).reshape(-1, 1))))
# X_test=np.hstack((X_test,
#                     X_test*((X_test[:,1]==12.5).reshape(-1, 1)),
#                     X_test*((X_test[:,1]==25).reshape(-1, 1))))
141/288: regr.fit(X_train, y_train)
141/289: mean_absolute_error(y_test, regr.predict(X_test))
141/290: X_clone=X_train[X_train[:,1]==12.5]
141/291: y_clone=y_train[X_train[:,1]==12.5]
141/292: regr.fit(X_clone, y_clone)
141/293: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
141/294:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize['yield']
141/295:
X_train, X_test, y_train, y_test = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
141/296:
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
141/297: X_clone=X_train[X_train[:,1]==12.5]
141/298: y_clone=y_train[X_train[:,1]==12.5]
141/299: reg.fit(X_clone, y_clone)
141/300: X_clone=X_train[X_train[:,1]==12.5]
141/301:
X_clone
y_clone=y_train[X_train[:,1]==12.5]
141/302:
print(X_clone)
y_clone=y_train[X_train[:,1]==12.5]
141/303: y_clone=y_train[X_train[:,1]==12.5]
141/304: reg.fit(X_clone, y_clone)
141/305: reg.fit(X_clone, y_clone.values)
141/306: mean_absolute_error(y_test[X_test[:,1]==12.5], reg.predict(X_test[X_test[:,1]==12.5]))
141/307:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.linalg.lstsq(self.X, self.Y, rcond=None)[0]
        X1=self.X[self.X[:,1]==12.5]
        X2=self.X[self.X[:,1]==25]
        y1=self.Y[self.X[:,1]==12.5]
        y2=self.Y[self.X[:,1]==25]
        self.B2 = np.linalg.lstsq(X1, y1, rcond=None)[0]
        self.B3 = np.linalg.lstsq(X2, y2, rcond=None)[0]

    def predict_one(self, x):
        if x[1] == 12.5:
            return np.dot(x, self.B2)
        if x[1] == 25:
            return np.dot(x, self.B3)
        return np.dot(x, self.B)
    
    def predict(self, X):
        # if self.B.shape[0] != X.shape[1]:
        #     X=np.hstack((np.ones((X.shape[0], 1)), X))
            # return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.array([self.predict_one(x) for x in X])
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
141/308: regr=SpecialRegressor(3)
141/309: regr.fit(X_train, y_train)
141/310:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
# X_train=np.hstack((X_train, 
#                    X_train*((X_train[:,1]==12.5).reshape(-1, 1)), 
#                    X_train*((X_train[:,1]==25).reshape(-1, 1))))
# X_test=np.hstack((X_test,
#                     X_test*((X_test[:,1]==12.5).reshape(-1, 1)),
#                     X_test*((X_test[:,1]==25).reshape(-1, 1))))
141/311: regr.fit(X_train, y_train)
141/312: mean_absolute_error(y_test, regr.predict(X_test))
141/313: mean_absolute_error(y_test[X_train[:,1]==12.5], regr.predict(X_test[X_train[:,1]==12.5]))
141/314: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
141/315:
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
141/316:
X_clone2=data_clonesize2.drop(['yield', 'id', 'Row#'], axis=1)
y_clone2=data_clonesize2['yield']
141/317: data_clonesize2=df[df['clonesize'] == 25]
141/318:
X_clone2=data_clonesize2.drop(['yield', 'id', 'Row#'], axis=1)
y_clone2=data_clonesize2['yield']
141/319:
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
141/320:
reg.fit(X_train2, y_train2.values)
mean_absolute_error(y_test2, reg.predict(X_test2))
141/321:
reg.fit(X_train2, y_train2.values)
# mean_absolute_error(y_test2, reg.predict(X_test2))
143/1:
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
143/2:
import pandas as pd
import numpy as np
143/3: df=pd.read_csv('train.csv')
143/4:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
143/5: y=df['yield']
143/6:
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
143/7: from sklearn.preprocessing import PolynomialFeatures
143/8:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.linalg.lstsq(self.X, self.Y, rcond=None)[0]

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/9: data_clonesize=df[df['clonesize'] == 12.5]
143/10:
X_clone=data_clonesize.drop(['yield', 'id', 'Row#'], axis=1)
y_clone=data_clonesize['yield']
143/11:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/12:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/13:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/14:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/15:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/16: y=df['yield']
143/17:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/18: df=pd.read_csv('train.csv')
143/19:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
143/20:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/21:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/22: df=pd.read_csv('train.csv')
143/23:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
143/24:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/25: y=df['yield']
143/26:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/27: X_clone=X_train[X_train[:,0] == 0.25]
143/28: X_clone.shape
143/29: X_clone=X_train[X_train[:,1] == 0.25]
143/30: X_clone.shape
143/31: X_clone=X_train[X_train[:,1] == 0.25]
143/32: X_clone.shape
143/33: X_train[:,1]
143/34: X_clone=X_train[X_train[:,1] == 12.5]
143/35: X_train[:,1]
143/36: X_clone.shape
143/37:
X_clone=X_train[X_train[:,1] == 12.5]
y_clone=y_train[X_train[:,1] == 12.5]
X_test_clone=X_test[X_test[:,1] == 12.5]
y_test_clone=y_test[X_test[:,1] == 12.5]
143/38:
reg.fit(X_clone, y_clone)
mean_absolute_error(y_test_clone, reg.predict(X_test_clone))
143/39:
reg=MultiVariateRegressor(3)
reg.fit(X_clone, y_clone)
mean_absolute_error(y_test_clone, reg.predict(X_test_clone))
143/40:
reg=MultiVariateRegressor(3)
reg.fit(X_clone, y_clone.values)
mean_absolute_error(y_test_clone, reg.predict(X_test_clone))
143/41: df_clone=data_clonesize[data_clonesize['clonesize'] == 12.5]
143/42: X_clone_train, X_clone_test, y_clone_train, y_clone_test = train_test_split(df_clone.drop(['yield', 'id', 'Row#'], axis=1), df_clone['yield'], test_size=0.2, random_state=42)
143/43: df_clone=df[df['clonesize'] == 12.5]
143/44: df_clone=X[X['clonesize'] == 12.5]
143/45:
df_clone=X[X['clonesize'] == 12.5]
y_clone=df[X['clonesize'] == 12.5]
143/46: X_clone_train, X_clone_test, y_clone_train, y_clone_test = train_test_split(df_clone, y_clone, test_size=0.2, random_state=42)
143/47:
X_clone_train, X_clone_test, y_clone_train, y_clone_test = train_test_split(df_clone, y_clone, test_size=0.2, random_state=42)
X_clone_train=PolynomialFeatures(degree=2).fit_transform(X_clone_train)
X_clone_test=PolynomialFeatures(degree=2).fit_transform(X_clone_test)
143/48:
reg.fit(X_clone_train, y_clone_train.values)
mean_absolute_error(y_clone_test, reg.predict(X_clone_test))
143/49:
df_clone2=X[X['clonesize'] == 25]
y_clone2=df[X['clonesize'] == 25]
143/50:
X_clone_train2, X_clone_test2, y_clone_train2, y_clone_test2 = train_test_split(df_clone2, y_clone2, test_size=0.2, random_state=42)
X_clone_train2=PolynomialFeatures(degree=2).fit_transform(X_clone_train2)
X_clone_test2=PolynomialFeatures(degree=2).fit_transform(X_clone_test2)
143/51: reg.fit(X_clone_train2, y_clone_train2.values)
143/52:
reg.fit(X_clone_train2, y_clone_train2.values)
mean_absolute_error(y_clone_test2, reg.predict(X_clone_test2))
143/53:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X, self.B1)
        elif X[1] == 25:
            return np.dot(X, self.B1)
        return np.dot(X, self.B1)

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/54: regr=SpecialRegressor(3)
143/55: regr.fit(X_train, y_train)
143/56: regr.fit(X_train, y_train.values)
143/57: mean_absolute_error(y_test, regr.predict(X_test))
143/58:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X, self.B1)
        elif X[1] == 25:
            return np.dot(X, self.B1)
        return np.dot(X, self.B1)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/59: regr=SpecialRegressor(3)
143/60: regr.fit(X_train, y_train.values)
143/61: mean_absolute_error(y_test, regr.predict(X_test))
143/62: regr.fit(X_train[X_train[:,1]==12.5], y_train[X_train[:,1]==12.5].values)
143/63: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
143/64: regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
143/65: mean_absolute_error(y_test[X_test[:,1]==25], regr.predict(X_test[X_test[:,1]==25]))
143/66: regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
143/67: mean_absolute_error(y_test[X_test[:,1]==25], regr.predict(X_test[X_test[:,1]==25]))
143/68: X_train[X_train[:,1]==25].shape
143/69: X_test[X_test[:,1]==25].shape
143/70:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X, self.B1)
        elif X[1] == 25:
            return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/71: regr=SpecialRegressor(3)
143/72: regr.fit(X_train, y_train.values)
143/73: mean_absolute_error(y_test, regr.predict(X_test))
143/74: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
143/75: regr.fit(X_train[X_train[:,1]==12.5], y_train[X_train[:,1]==12.5].values)
143/76: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
143/77:
regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
143/78:
regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
mean_absolute_error(y_test[X_test[:,1]==25], regr.predict(X_test[X_test[:,1]==25]))
143/79:
df_clone2=X[X['clonesize'] == 25]
y_clone2=df[X['clonesize'] == 25]
143/80:
X_clone=X_train[X_train[:,1] == 25]
y_clone=y_train[X_train[:,1] == 25]
X_test_clone=X_test[X_test[:,1] == 25]
y_test_clone=y_test[X_test[:,1] == 25]
143/81:
X_clone2=X_train[X_train[:,1] == 25]
y_clone2=y_train[X_train[:,1] == 25]
X_test_clone2=X_test[X_test[:,1] == 25]
y_test_clone2=y_test[X_test[:,1] == 25]
143/82:
X_clone=X_train[X_train[:,1] == 12.5]
y_clone=y_train[X_train[:,1] == 12.5]
X_test_clone=X_test[X_test[:,1] == 12.5]
y_test_clone=y_test[X_test[:,1] == 12.5]
143/83:
reg=MultiVariateRegressor(3)
reg.fit(X_clone2, y_clone2.values)
mean_absolute_error(y_test_clone2, reg.predict(X_test_clone2))
143/84:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X, self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/85: regr=SpecialRegressor(3)
143/86: regr.fit(X_train[X_train[:,1]==12.5], y_train[X_train[:,1]==12.5].values)
143/87:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
143/88:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/89:
X_train, X_test, y_train, y_test = train_test_split(X_clone, y_clone, test_size=0.2, random_state=42)
X_train=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test=PolynomialFeatures(degree=2).fit_transform(X_test)
143/90:
X_clone=X_train[X_train[:,1] == 12.5]
y_clone=y_train[X_train[:,1] == 12.5]
X_test_clone=X_test[X_test[:,1] == 12.5]
y_test_clone=y_test[X_test[:,1] == 12.5]
143/91:
reg=MultiVariateRegressor(3)
reg.fit(X_clone, y_clone.values)
mean_absolute_error(y_test_clone, reg.predict(X_test_clone))
143/92:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/93:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/94:
X_clone=X_train[X_train[:,1] == 12.5]
y_clone=y_train[X_train[:,1] == 12.5]
X_test_clone=X_test[X_test[:,1] == 12.5]
y_test_clone=y_test[X_test[:,1] == 12.5]
143/95:
reg=MultiVariateRegressor(3)
reg.fit(X_clone, y_clone.values)
mean_absolute_error(y_test_clone, reg.predict(X_test_clone))
143/96:
X_clone2=X_train[X_train[:,1] == 25]
y_clone2=y_train[X_train[:,1] == 25]
X_test_clone2=X_test[X_test[:,1] == 25]
y_test_clone2=y_test[X_test[:,1] == 25]
143/97:
reg=MultiVariateRegressor(3)
reg.fit(X_clone2, y_clone2.values)
mean_absolute_error(y_test_clone2, reg.predict(X_test_clone2))
143/98:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/99:
X_clone=X_train[X_train[:,1] == 12.5]
y_clone=y_train[X_train[:,1] == 12.5]
X_test_clone=X_test[X_test[:,1] == 12.5]
y_test_clone=y_test[X_test[:,1] == 12.5]
143/100:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/101:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)
143/102:
X_clone=X_train[X_train[:,1] == 12.5]
y_clone=y_train[X_train[:,1] == 12.5]
X_test_clone=X_test[X_test[:,1] == 12.5]
y_test_clone=y_test[X_test[:,1] == 12.5]
143/103:
reg=MultiVariateRegressor(3)
reg.fit(X_clone2, y_clone2.values)
mean_absolute_error(y_test_clone2, reg.predict(X_test_clone2))
143/104:
X_clone2=X_train[X_train[:,1] == 25]
y_clone2=y_train[X_train[:,1] == 25]
X_test_clone2=X_test[X_test[:,1] == 25]
y_test_clone2=y_test[X_test[:,1] == 25]
143/105:
reg=MultiVariateRegressor(3)
reg.fit(X_clone2, y_clone2.values)
mean_absolute_error(y_test_clone2, reg.predict(X_test_clone2))
143/106:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X, self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/107:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/108: regr.fit(X_train[X_train[:,1]==12.5], y_train[X_train[:,1]==12.5].values)
143/109: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
143/110:
regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
mean_absolute_error(y_test[X_test[:,1]==25], regr.predict(X_test[X_test[:,1]==25]))
143/111: y_clone=y_train[X_train[:,1]==12.5]
143/112: reg.fit(X_clone, y_clone.values)
143/113:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/114: regr.fit(X_train[X_train[:,1]==12.5], y_train[X_train[:,1]==12.5].values)
143/115: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
143/116:
regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
mean_absolute_error(y_test[X_test[:,1]==25], regr.predict(X_test[X_test[:,1]==25]))
143/117:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5][:,1:]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25][:,1:]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X, self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/118:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/119: regr.fit(X_train[X_train[:,1]==12.5], y_train[X_train[:,1]==12.5].values)
143/120: mean_absolute_error(y_test[X_test[:,1]==12.5], regr.predict(X_test[X_test[:,1]==12.5]))
143/121:
regr.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25].values)
mean_absolute_error(y_test[X_test[:,1]==25], regr.predict(X_test[X_test[:,1]==25]))
143/122:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5][:,1:]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25][:,1:]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X[1:], self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/123:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/124:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=X[X[:,1] == 12.5][:,1:]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=X[X[:,1] == 25][:,1:]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X[1:], self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/125:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
regr.B1.shape
mean_absolute_error(y_test, regr.predict(X_test))
143/126:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
print(regr.B1.shape)
mean_absolute_error(y_test, regr.predict(X_test))
143/127:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
print(X_train.shape)
print(regr.B1.shape)
mean_absolute_error(y_test, regr.predict(X_test))
143/128:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=(X[X[:,1] == 12.5])[:,1:]
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=(X[X[:,1] == 25])[:,1:]
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X[1:], self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/129:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
print(X_train.shape)
print(regr.B1.shape)
mean_absolute_error(y_test, regr.predict(X_test))
143/130:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X[1:], self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B1.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/131:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
print(X_train.shape)
print(regr.B1.shape)
mean_absolute_error(y_test, regr.predict(X_test))
143/132:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(X[1:], self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/133:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
print(X_test.shape)
print(regr.B1.shape)
mean_absolute_error(y_test, regr.predict(X_test))
143/134:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/135:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        # if X[1] == 12.5:
        #     return np.dot(X[1:], self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/136:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/137:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(np.delete(X, 1, axis=1), self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/138:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/139:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(np.delete(X, 1), self.B1)
        # elif X[1] == 25:
        #     return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/140:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/141:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/142:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(np.delete(X, 1), self.B1)
        elif X[1] == 25:
            return np.dot(X, self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/143:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/144:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(np.delete(X, 1), self.B1)
        elif X[1] == 25:
            return np.dot(np.delete(X, 1), self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/145:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/146:
class SpecialRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,1] == 12.5], 1, axis=1)
        Y_clone1=Y[X[:,1] == 12.5]
        X_clone2=np.delete(X[X[:,1] == 25], 1, axis=1)
        Y_clone2=Y[X[:,1] == 25]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(np.delete(X, 1), self.B1)
        # elif X[1] == 25:
        #     return np.dot(np.delete(X, 1), self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/147:
regr=SpecialRegressor(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/148:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,2] == 0.25], 1, axis=1)
        Y_clone1=Y[X[:,1] == 0.25]
        X_clone2=np.delete(X[X[:,1] == 0.5], 1, axis=1)
        Y_clone2=Y[X[:,1] == 0.5]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 12.5:
            return np.dot(np.delete(X, 1), self.B1)
        # elif X[1] == 25:
        #     return np.dot(np.delete(X, 1), self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/149:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,2] == 0.25], 1, axis=1)
        Y_clone1=Y[X[:,1] == 0.25]
        X_clone2=np.delete(X[X[:,1] == 0.5], 1, axis=1)
        Y_clone2=Y[X[:,1] == 0.5]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[1] == 0.25:
            return np.dot(np.delete(X, 1), self.B1)
        # elif X[1] == 25:
        #     return np.dot(np.delete(X, 1), self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/150:
regr=SpecialRegressor2(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/151:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        X_clone1=np.delete(X[X[:,2] == 0.25], 2, axis=1)
        Y_clone1=Y[X[:,2] == 0.25]
        X_clone2=np.delete(X[X[:,2] == 0.5], 2, axis=1)
        Y_clone2=Y[X[:,2] == 0.5]
        reg1=MultiVariateRegressor(3)
        reg2=MultiVariateRegressor(3)
        reg3=MultiVariateRegressor(3)
        reg1.fit(X_clone1, Y_clone1)
        reg2.fit(X_clone2, Y_clone2)
        reg3.fit(X, Y)
        self.B1=reg1.B
        self.B2=reg2.B
        self.B3=reg3.B

    def predict_one(self, X):
        if X[2] == 0.25:
            return np.dot(np.delete(X, 2), self.B1)
        # elif X[1] == 25:
        #     return np.dot(np.delete(X, 1), self.B2)
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B3.shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/152:
regr=SpecialRegressor2(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/153:
regr=SpecialRegressor2(3)
regr.fit(X_train, y_train.values)
mean_absolute_error(y_test, regr.predict(X_test))
143/154:
for col in df.columns:
    print(df[col].value_counts().shape)
143/155:
for col in df.columns:
    print(df[col].value_counts())
143/156:
for col in df.columns:
    if df[col].value_counts().shape[0] < 10:
        print(df[col].value_counts())
    # print(df[col].value_counts())
143/157: X_train[:,13]
143/158:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        self.values=df[col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/159: regressor3=SpecialRegressor2(3)
143/160:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/161:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        self.values=df[col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/162:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/163:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        self.values=df[col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/164:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/165:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        col=13
        self.values=df[self.col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/166:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/167:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        col=13
        self.values=df[self.col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/168:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/169:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/170:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        col=13
        self.values=df.iloc[:,self.col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B3)

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/171:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/172:
class SpecialRegressor2():
    def __init__(self, degree):
        self.degree = degree
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        self.col=13
        col=13
        self.values=df.iloc[:,self.col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/173:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/174:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values=df.iloc[:,self.col].value_counts().index
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/175:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/176: df.iloc[:,13].value_counts().index
143/177: df.iloc[:,13].value_counts()
143/178: df.iloc[:,13].value_counts().values
143/179:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values=[val.index for val in df.iloc[:,self.col].value_counts() if val > 10]
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/180:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/181:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col].value_counts().items() if count > 10]
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/182:
regressor3=SpecialRegressor2(3)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/183:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/184:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/185:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/186:
for i in range(len(df.columns)):
    if df[:,i].value_counts().shape[0] < 10:
        print(i)
    # print(df[col].value_counts())
143/187:
for i in range(len(df.columns)):
    if df.iloc[:,i].value_counts().shape[0] < 10:
        print(i)
    # print(df[col].value_counts())
143/188: values=[2,3, 4, 7, 8, 9, 10, 11, 12, 13, 14]
143/189:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train.values)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/190: df['andrena'].value_counts()
143/191:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/192:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train.values)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/193:
reg=MultiVariateRegressor(3)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/194:
regr=SpecialRegressor2(2)
regr.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=regr.predict(X)
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
143/195:
orig_observations=pd.read_csv('output.csv')['yield']
new_observations=pd.read_csv('hello.csv')['yield']
143/196: mean_absolute_error(orig_observations, new_observations)
143/197:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)
143/198:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)
143/199: df=pd.read_csv('train.csv')
143/200:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/201: y=df['yield']
143/202:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)
143/203:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/204:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
143/205:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/206:
regressor3=SpecialRegressor2(2)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/207:
for i in range(len(df.columns)):
    if df.iloc[:,i].value_counts().shape[0] < 10:
        print(i)
    # print(df[col].value_counts())
143/208:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train.values)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/209:
reg=MultiVariateRegressor(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/210:
reg=MultiVariateRegressor(3)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/211:
reg=MultiVariateRegressor(4)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/212:
reg=MultiVariateRegressor(5)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/213:
reg=MultiVariateRegressor(6)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/214:
reg=MultiVariateRegressor(7)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/215:
reg=MultiVariateRegressor(10)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/216:
reg=MultiVariateRegressor(1)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/217:
reg=MultiVariateRegressor(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/218:
reg=MultiVariateRegressor(3)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/219:
reg=MultiVariateRegressor(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/220:
reg=MultiVariateRegressor(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/221:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train.values)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/222:
reg=MultiVariateRegressor(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/223:
reg=SpecialRegressor2(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/224: df['yield'].value_counts()
143/225: from scipy.optimize import minimize
143/226:
import numpy as np
from scipy.optimize import minimize

# Sample data
X_here = df.drop(['yield', 'id', 'Row#'], axis=1)  # 100 samples with 2 features
y = df['yield']     # 100 target values
X_here = PolynomialFeatures(degree=2).fit_transform(X_here)

# Define the objective function for MAE
def objective_function(coeffs, X, y):
    predictions = np.dot(X, coeffs)
    mae = np.mean(np.abs(predictions - y))
    return mae

# Initial guess for the coefficients
initial_guess = np.zeros(X_here.shape[1])

# Minimize the objective function
result = minimize(objective_function, initial_guess, args=(X_here, y), method='BFGS')

# Optimized coefficients
optimized_coeffs = result.x
print(f"Optimized coefficients: {optimized_coeffs}")
143/227:
import numpy as np
from scipy.optimize import minimize

# Sample data
X_here = df.drop(['yield', 'id', 'Row#'], axis=1)  # 100 samples with 2 features
y = df['yield']     # 100 target values
X_here = PolynomialFeatures(degree=2).fit_transform(X_here)
X_here_train, X_here_test, y_train, y_test = train_test_split(X_here, y, test_size=0.2, random_state=42)

# Define the objective function for MAE
def objective_function(coeffs, X, y):
    predictions = np.dot(X, coeffs)
    mae = np.mean(np.abs(predictions - y))
    return mae

# Initial guess for the coefficients
initial_guess = np.zeros(X_here_train.shape[1])

# Minimize the objective function
result = minimize(objective_function, initial_guess, args=(X_here_train, y_train), method='BFGS')

# Optimized coefficients
optimized_coeffs = result.x
print(f"Optimized coefficients: {optimized_coeffs}")
143/228: mean_absolute_error(y_test, np.dot(X_here_test, optimized_coeffs))
143/229:
X=df.drop(['yield', 'id', 'Row#', 'MinOfUpperTRange', 'MaxOfUpperTRange'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/230:
X=df.drop(['yield', 'id', 'Row#', 'MinOfUpperTRange', 'MaxOfUpperTRange'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/231:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
143/232:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train.values)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
143/233:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
143/234:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = X_train.values
X_test = X_test.values
from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
143/235:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X=np.asarray(X, dtype=np.float64)
y=np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
142/30:
input_ads = pd.read_csv('train.csv')

#-----------------------------------------------------------------
print(input_ads.shape)
input_ads.head()
142/31:
from sklearn.tree import DecisionTreeRegressor

#---------------------------------------------------------------------------------------------------------
#Loss func
def loss_calc(y_true,y_pred):
    
    loss = np.sum(np.abs(y_true-y_pred))
        
    return loss

#---------------------------------------------------------------------------------------------------------
#Gradient Calc
def gradient_calc(y_true,y_pred):
    
    grad = -(y_true-y_pred)
    
    return grad

#---------------------------------------------------------------------------------------------------------
#The base estimator
def tree_creator(r_state,X,y):
    
    d_tree = DecisionTreeRegressor(random_state=r_state,criterion='absolute_error',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
    d_tree.fit(X,y)
    
    return d_tree

#---------------------------------------------------------------------------------------------------------
#Predicting through gradient boosting regression
def predict_grad_boost(models_tray,alpha,test_x=X_test_arr,train_y=y_arr):
    
    initial_pred = np.array([np.mean(train_y)] * len(test_x))
        
    final_pred = initial_pred.reshape(len(initial_pred),1)
    #print(final_pred.shape)
    
    for i in range(len(models_tray)):
        
        model = models_tray[i]
        temp_pred = (model.predict(test_x)).reshape(len(test_x),1)
        #print(temp_pred.shape)
        final_pred -= alpha * temp_pred
    
    return final_pred
142/32:
def grad_boost_train(train_x,train_y,alpha=0.01,r_state=100,n_iters=101):

    model_tray = [] #Tray to collect the trained boosted stage estimators
    loss_counter = [] #Tray for loss capture

    
    initial_pred = np.array([np.mean(train_y)] * len(train_y))

    print('Initial val :',initial_pred.shape)
    model_pred = initial_pred.reshape(len(initial_pred),1)

    for epoch in range(n_iters): #Unit iteration

        if epoch%100==0:
            print('#---------- Epoch number :',epoch,' -----------#')
        
        #Calculating loss
        loss = loss_calc(y_true=train_y,
                         y_pred=model_pred)

        loss_counter.append(loss)
        
        #Calculating the gradient (residuals)
        grads = gradient_calc(y_true=train_y,
                              y_pred=model_pred)
        #print(grads.shape)
        #Building the regression tree on the gradient (residuals)
        tree_grad = tree_creator(r_state=r_state,
                                 X=train_x,
                                 y=grads)
        #print(train_x.shape)
        #print(tree_grad.predict(train_x).shape)
        
        #Predicting the residuals according to the tree fit above
        pred_m = (tree_grad.predict(train_x)).reshape(len(train_x),1)
        
        #Updating model through learning rate
        model_pred -= alpha * pred_m
        
        #Appending the model into tray
        model_tray.append(tree_grad)
        
    return model_tray,loss_counter,initial_pred
142/33:
n_estimators = 1001 #No of boosting steps
alpha =0.01 #Learning rate

#Training gradient boosting regression
models_list,loss_counter,initial_pred = grad_boost_train(train_x=X_arr,
                                                         train_y=y_arr,
                                                         alpha=alpha,
                                                         r_state=100,
                                                         n_iters=n_estimators)
142/34:
manual_gbm_pred = predict_grad_boost(models_tray=models_list, #Passing the fitted estimators into the predict function
                                     alpha=alpha, #The alpha val used during training
                                     test_x=X_test_arr) #Test dataset
142/35: from sklearn.metrics import mean_absolute_error
142/36: mean_absolute_error(y_test_arr,manual_gbm_pred)
142/37: mean_absolute_error(y_test_arr,manual_gbm_pred)
142/38: from sklearn.ensemble import GradientBoostingRegressor
142/39:

skl_gbm = GradientBoostingRegressor(random_state=100,n_estimators=1001,criterion='squared_error',
                                    max_depth=2,min_samples_split=5,
                                    min_samples_leaf=5,max_features=3)
142/40: skl_gbm.fit(X_arr,y_arr)
142/41: skl_pred = skl_gbm.predict(X_test_arr)
142/42: mean_absolute_error(y_test_arr,skl_pred)
143/236:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, penalty='l2', max_iter=1000, tol=1e-3, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/237:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, penalty='l2', max_iter=1000, tol=1e-3, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/238:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=1000, tol=1e-3, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/239:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=10000, tol=1e-3, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/240:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=100000, tol=1e-3, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/241:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=100000, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/242:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=100000, tol=0.01, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/243:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=100000, tol=0.001, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/244:
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create the SGDRegressor with epsilon_insensitive loss and L2 regularization
model = make_pipeline(
    StandardScaler(),  # Scale the features for better convergence
    SGDRegressor(loss='epsilon_insensitive', epsilon=0, max_iter=100000, tol=0.00001, random_state=42)
)

# Fit the model to your data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the performance
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
143/245:
import numpy as np

# Generate a synthetic dataset
np.random.seed(42)
X = np.random.rand(100, 3)  # 100 samples, 3 features
true_coefficients = np.array([1.5, -2.0, 3.0])
y = X @ true_coefficients + np.random.randn(100) * 0.1  # adding some noise

# Add a bias term (intercept) to the feature matrix
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Hyperparameters
learning_rate = 0.01
max_iter = 1000
tolerance = 1e-6

# Initialize coefficients randomly
coefficients = np.random.randn(X.shape[1])

# Function to calculate MAE
def compute_mae(X, y, coefficients):
    predictions = X @ coefficients
    mae = np.mean(np.abs(y - predictions))
    return mae

# Subgradient Descent loop
for iteration in range(max_iter):
    predictions = X @ coefficients
    errors = y - predictions

    # Compute the subgradient of MAE
    subgradient = -np.sign(errors) @ X / len(y)

    # Update coefficients
    coefficients -= learning_rate * subgradient

    # Calculate the new MAE
    mae = compute_mae(X, y, coefficients)

    # Check for convergence
    if np.linalg.norm(learning_rate * subgradient) < tolerance:
        print(f"Converged in {iteration} iterations")
        break

print("Optimal coefficients:", coefficients)
print("Minimum MAE:", mae)

# Making predictions using the optimized coefficients
new_predictions = X @ coefficients
143/246:
import numpy as np

# Generate a synthetic dataset
np.random.seed(42)
X = df.drop(['yield', 'id', 'Row#'])  # 100 samples, 3 features
# true_coefficients = np.array([1.5, -2.0, 3.0])
y = df['yield']  # adding some noise

# Add a bias term (intercept) to the feature matrix
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Hyperparameters
learning_rate = 0.01
max_iter = 1000
tolerance = 1e-6

# Initialize coefficients randomly
coefficients = np.random.randn(X.shape[1])

# Function to calculate MAE
def compute_mae(X, y, coefficients):
    predictions = X @ coefficients
    mae = np.mean(np.abs(y - predictions))
    return mae

# Subgradient Descent loop
for iteration in range(max_iter):
    predictions = X @ coefficients
    errors = y - predictions

    # Compute the subgradient of MAE
    subgradient = -np.sign(errors) @ X / len(y)

    # Update coefficients
    coefficients -= learning_rate * subgradient

    # Calculate the new MAE
    mae = compute_mae(X, y, coefficients)

    # Check for convergence
    if np.linalg.norm(learning_rate * subgradient) < tolerance:
        print(f"Converged in {iteration} iterations")
        break

print("Optimal coefficients:", coefficients)
print("Minimum MAE:", mae)

# Making predictions using the optimized coefficients
new_predictions = X @ coefficients
143/247:
import numpy as np

# Generate a synthetic dataset
np.random.seed(42)
X = df.drop(['yield', 'id', 'Row#'], axis=1)  # 100 samples, 3 features
# true_coefficients = np.array([1.5, -2.0, 3.0])
y = df['yield']  # adding some noise

# Add a bias term (intercept) to the feature matrix
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Hyperparameters
learning_rate = 0.01
max_iter = 1000
tolerance = 1e-6

# Initialize coefficients randomly
coefficients = np.random.randn(X.shape[1])

# Function to calculate MAE
def compute_mae(X, y, coefficients):
    predictions = X @ coefficients
    mae = np.mean(np.abs(y - predictions))
    return mae

# Subgradient Descent loop
for iteration in range(max_iter):
    predictions = X @ coefficients
    errors = y - predictions

    # Compute the subgradient of MAE
    subgradient = -np.sign(errors) @ X / len(y)

    # Update coefficients
    coefficients -= learning_rate * subgradient

    # Calculate the new MAE
    mae = compute_mae(X, y, coefficients)

    # Check for convergence
    if np.linalg.norm(learning_rate * subgradient) < tolerance:
        print(f"Converged in {iteration} iterations")
        break

print("Optimal coefficients:", coefficients)
print("Minimum MAE:", mae)

# Making predictions using the optimized coefficients
new_predictions = X @ coefficients
143/248:
reg=SpecialRegressor2(2)
reg.fit(X_train, y_train.values)
mean_absolute_error(y_test, reg.predict(X_test))
143/249:
reg=SpecialRegressor2(2)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/250:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train.values)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/251:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/252: df.iloc[:,2].value_counts()
143/253: df.iloc[:,1].value_counts()
143/254: values=[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]
143/255:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/256:
reg=SpecialRegressor2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/257: df.iloc[:,1].value_counts()
143/258: df.iloc[:,2].value_counts()
143/259:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col] == value], col, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/260:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/261:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/262:
regressor3=SpecialRegressor2(2)
regressor3.fit(X_train, y_train.values)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/263:
regressor3=SpecialRegressor2(2)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/264: values=[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]
143/265:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/266: vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
143/267:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,0] == val]
    y_test_clone=y_test[X_test[:,0] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/268:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    print(val)
    X_test_clone=X_test[X_test[:,0] == val]
    y_test_clone=y_test[X_test[:,0] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/269:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,0] == val]
    y_test_clone=y_test[X_test[:,0] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/270:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/271:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/272:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/273:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/274: values=[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]
143/275:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/276:
reg=SpecialRegressor2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/277:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/278:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values[0]:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/279:
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/280:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if value not in [0.25]:
                continue
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/281:
regressor3=SpecialRegressor_for_2(2)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/282:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/283:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if value not in [0.25]:
                continue
            if X[self.col] == value:
                print("HERE")
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/284:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/285:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/286:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if value not in [25]:
                continue
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/287:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/288:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if value != 25:
                continue
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/289:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if value != 25:
                continue
            if X[self.col] == value:
                print("HERE")
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/290:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/291:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            print(value)
            if value != 25:
                continue
            if X[self.col] == value:
                print("HERE")
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/292:
regressor3=SpecialRegressor2(13)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/293:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/294:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            # print(value)
            if value not in [25]:
                continue
            if X[self.col+1] == value:
                print("HERE")
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/295:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/296:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            # print(value)
            if value not in [25]:
                continue
            if X[self.col+1] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/297:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/298:
regressor3=SpecialRegressor_for_2(0)
regressor3.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25])
mean_absolute_error(y_test[X_test[:,1]==25], regressor3.predict(X_test[X_test[:,1]==25]))
143/299:
regressor3=MultiVariateRegressor(13)
regressor3.fit(X_train, y_train)
mean_absolute_error(y_test, regressor3.predict(X_test))
143/300:
regressor3=MultiVariateRegressor(0)
regressor3.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25])
mean_absolute_error(y_test[X_test[:,1]==25], regressor3.predict(X_test[X_test[:,1]==25]))
143/301:
regressor3=MultiVariateRegressor(0)
regressor3.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25])
mean_absolute_error(y_test[X_test[:,1]==25], regressor3.predict(X_test[X_test[:,1]==25]))
143/302:
reg=SpecialRegressor2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/303:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/304:
reg=SpecialRegressor2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/305:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/306:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in [25]:
            if X[self.col] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/307:
reg=SpecialRegressor2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/308:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in [25]:
            if X[self.col+1] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/309:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in values:
            if X[self.col+1] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/310:
values=[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]
for value in values:
    regressor3=MultiVariateRegressor(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/311:
regressor3=MultiVariateRegressor(0)
regressor3.fit(X_train[X_train[:,1]==25], y_train[X_train[:,1]==25])
mean_absolute_error(y_test[X_test[:,1]==25], regressor3.predict(X_test[X_test[:,1]==25]))
143/312:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/313:
reg=SpecialRegressor_for_2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/314:
reg=SpecialRegressor_for_2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/315:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            # print(value)
            if value not in [25]:
                continue
            if X[self.col+1] == value:
                print("here")
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/316:
reg=SpecialRegressor_for_2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/317:
class SpecialRegressor_for_2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            # print(value)
            if value not in [25]:
                continue
            if X[self.col+1] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/318:
reg=SpecialRegressor_for_2(0)
reg.fit(X_train, y_train)
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/319:
reg=SpecialRegressor_for_2(0)
reg.fit(X_train[X_train[:,1] == 25], y_train[X_train[:,1] == 25])
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/320:
reg=MultiVariateRegressor(0)
reg.fit(X_train[X_train[:,1] == 25], y_train[X_train[:,1] == 25])
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/321:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/322:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg=MultiVariateRegressor(2)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/323:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg=MultiVariateRegressor(2)
reg.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/324:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg=SpecialRegressor(0)
reg.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/325:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg=MultiVariateRegressor(0)
reg.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/326:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg=SpecialRegressor(0)
reg.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg.predict(X_test_clone)))
143/327:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/328:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col+1] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/329:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/330:
reg=MultiVariateRegressor(0)
reg.fit(X_train[X_train[:,1] == 25], y_train[X_train[:,1] == 25])
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/331:
reg=MultiVariateRegressor(0)
reg.fit(X_train[X_train[:,1] == 25], y_train[X_train[:,1] == 25])
reg.fit(X_train, y_train)
mean_absolute_error(y_test[X_test[:,1] == 25], reg.predict(X_test[X_test[:,1] == 25]))
143/332:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col+1] == value:
                print('here')
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/333:
values=[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]
for value in values:
    regressor3=MultiVariateRegressor(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/334:
values=[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]
for value in values:
    regressor3=SpecialRegressor2(value)
    regressor3.fit(X_train, y_train)
    print(mean_absolute_error(y_test, regressor3.predict(X_test)))
143/335:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/336:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            if X[self.col+1] == value:
                print('here')
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/337:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/338:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    print(vals)
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/339:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/340:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            print(value)
            if X[self.col+1] == value:
                print('here')
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/341:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/342:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/343:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            print(value)
            if X[self.col+1] == value:
                # print('here')
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/344:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/345:
class SpecialRegressor2():
    def __init__(self, column=13):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            # print(value)
            if X[self.col+1] == value:
                # print('here')
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/346:
vals = [val for val, count in df.iloc[:, 2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(0)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(0)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,1] == val]
    y_test_clone=y_test[X_test[:,1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/347:
col=0
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/348:
col=1
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/349:
col=2
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/350:
col3
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/351:
col=3
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/352:
col=4
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/353:
col=5
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/354:
col=6
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/355:
col=7
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/356:
col=8
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/357:
col=9
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/358:
col=10
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/359:
col=11
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/360:
col=12
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/361:
col=13
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/362:
col=14
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/363:
col=15
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/364:
col=16
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/365:
col=17
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/366:
col=1
vals = [val for val, count in df.iloc[:, col+2].value_counts().items() if count > 1000]
reg1=MultiVariateRegressor(col)
reg1.fit(X_train, y_train)
reg2=SpecialRegressor2(col)
reg2.fit(X_train, y_train)
for val in vals:
    X_test_clone=X_test[X_test[:,col+1] == val]
    y_test_clone=y_test[X_test[:,col+1] == val]
    if X_test_clone.shape[0] == 0:
        continue
    print(val, mean_absolute_error(y_test_clone, reg2.predict(X_test_clone))-mean_absolute_error(y_test_clone, reg1.predict(X_test_clone)))
143/367:
class SpecialRegressor_for_3():
    def __init__(self, column=1):
        self.col = column
        self.B1 = None
        self.B2 = None
        self.B3 = None

    def fit(self, X, Y):
        col=self.col
        self.values = [val for val, count in df.iloc[:, self.col+2].value_counts().items() if count > 1000]
        X_clones=[np.delete(X[X[:,col+1] == value], col+1, axis=1) for value in self.values]
        regressors=[MultiVariateRegressor(3) for _ in range(len(self.values))]
        for i, reg in enumerate(regressors):
            reg.fit(X_clones[i], Y[X[:,col+1] == self.values[i]])
        self.B=[reg.B for reg in regressors]
        reg=MultiVariateRegressor(3)
        reg.fit(X, Y)
        self.B.append(reg.B)

    def predict_one(self, X):
        for value in self.values:
            # print(value)
            if value not in [0.25]:
                continue
            if X[self.col+1] == value:
                return np.dot(np.delete(X, self.col+1), self.B[self.values.index(value)])
        return np.dot(X, self.B[-1])

    def predict(self, X):
        if self.B[-1].shape[0] != X.shape[1]:
            X=np.hstack((np.ones((X.shape[0], 1)), X))
        return np.array([self.predict_one(x) for x in X])

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
143/368:
reg=SpecialRegressor_for_3(1)
reg.fit(X_train, y_train)
mean_absolute_error(y_test, reg.predict(X_test))
143/369:
regr=SpecialRegressor_for_3(1)
regr.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=regr.predict(X)
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
143/370:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
143/371:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Yes honeybee 

# Split your data
X=np.asarray(X, dtype=np.float64)
y=np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
143/372:
regr=SpecialRegressor_for_3(1)
regr.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))
test_data=pd.read_csv('test.csv')
X=test_data.drop(['id', 'Row#'], axis=1)
X=PolynomialFeatures(degree=2).fit_transform(X)
preds=regr.predict(X)
test_data['yield']=preds
test_data=test_data[['id', 'yield']]
test_data.to_csv('hello.csv', index=False)
143/373:
orig_observations=pd.read_csv('output.csv')['yield']
new_predictions=pd.read_csv('hello.csv')['yield']
mean_absolute_error(orig_observations, new_predictions)
144/1: reg=HigherOrderRegressor(5)
144/2:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
144/3:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
144/4: train_data=pd.read_csv('train.csv')
144/5:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
144/6: reg=HigherOrderRegressor(5)
144/7: reg.fit(X, Y)
144/8: reg.predict(X_test)
144/9: r2_score(Y, reg.predict(X))
144/10: from sklearn.metrics import r2_score
144/11: r2_score(Y, reg.predict(X))
144/12: reg.r2_score(Y, reg.predict(X))
144/13: reg.predict(X)
144/14: regressor2=HigherOrderRegressor(3)
144/15: regressor=HigherOrderRegressor(3)
144/16: regressor.fit(X,Y)
144/17: X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
144/18: np.hstack((np.ones((X.shape[0], 1)), X))
144/19: X.shape
144/20: np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
144/21:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
144/22: reg=HigherOrderRegressor(5)
144/23: reg.fit(X, Y)
144/24: reg.r2_score(Y, reg.predict(X))
144/25: reg.r2_score(Y, reg.predict(X))
144/26:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
145/1:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X=df[['AverageRainingDays','fruitset','fruitmass','seeds'], axis=1]
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
145/2:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X=df['AverageRainingDays','fruitset','fruitmass','seeds']
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
145/3:
import pandas as pd
import numpy as np
145/4: df=pd.read_csv('train.csv')
145/5:
from sklearn.linear_model import LassoCV
from sklearn.linear_model import LinearRegression
from scipy.linalg import lstsq
from numpy.linalg import pinv
145/6:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X=df['AverageRainingDays','fruitset','fruitmass','seeds']
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
145/7:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X=df[('AverageRainingDays','fruitset','fruitmass','seeds')]
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
145/8:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
X=df[['AverageRainingDays','fruitset','fruitmass','seeds']]
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
145/9: y=df['yield']
145/10:
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
145/11: from sklearn.preprocessing import PolynomialFeatures
145/12:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/13:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import PolynomialFeatures
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler().fit(X_train)
# X_train=ss.transform(X_train)
# X_test=ss.transform(X_test)

# List of models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost Regressor': AdaBoostRegressor(),
    'Support Vector Regression': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'K-Neighbors Regressor': KNeighborsRegressor()
}

# Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/14:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/15:
class MultiVariateRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None

    def fit(self, X, Y):
        indices=np.random.shuffle(np.arange(X.shape[0]))
        self.orig_X = X[indices][0]
        self.Y = Y[indices][0]
        self.X = X[indices][0]
        if np.linalg.norm(self.X[:,0] - np.ones((self.X.shape[0], 1))) != 0:
            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.B = np.linalg.lstsq(self.X, self.Y, rcond=None)[0]

    def predict(self, X):
        if self.B.shape[0] != X.shape[1]:
            return np.dot(np.hstack((np.ones((X.shape[0], 1)), X)), self.B)
        return np.dot(X, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        # return np.sum((Y_true - Y_pred)**2)
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)
145/16: reg=MultiVariateRegressor(3)
145/17:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=1).fit_transform(X_train)
X_test = PolynomialFeatures(degree=1).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/18:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
X_test = PolynomialFeatures(degree=2).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/19:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=3).fit_transform(X_train)
X_test = PolynomialFeatures(degree=3).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/20:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=4).fit_transform(X_train)
X_test = PolynomialFeatures(degree=4).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/21:
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Split your data
X = np.asarray(X, dtype=np.float64)
y = np.asarray(y, dtype=np.float64)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = PolynomialFeatures(degree=5).fit_transform(X_train)
X_test = PolynomialFeatures(degree=5).fit_transform(X_test)


# List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# Evaluate each model
results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# Fit your custom regressor
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# Display the results
for model_name, metrics in results.items():
    print(f"{model_name}: MAE = {metrics['MAE']}")
145/22:
df.columns.size()    
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
# from sklearn.svm import SVR
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.metrics import mean_absolute_error
# from sklearn.model_selection import train_test_split

# # Split your data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# from sklearn.preprocessing import PolynomialFeatures
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# # from sklearn.preprocessing import StandardScaler
# # ss=StandardScaler().fit(X_train)
# # X_train=ss.transform(X_train)
# # X_test=ss.transform(X_test)

# # List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# # Evaluate each model
# results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# # Fit your custom regressor
# reg.fit(X_train, y_train)
# predictions = reg.predict(X_test)
# results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# # Display the results
# for model_name, metrics in results.items():
#     print(f"{model_name}: MAE = {metrics['MAE']}")
145/23:
df.columns.size
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
# from sklearn.svm import SVR
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.metrics import mean_absolute_error
# from sklearn.model_selection import train_test_split

# # Split your data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# from sklearn.preprocessing import PolynomialFeatures
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# # from sklearn.preprocessing import StandardScaler
# # ss=StandardScaler().fit(X_train)
# # X_train=ss.transform(X_train)
# # X_test=ss.transform(X_test)

# # List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# # Evaluate each model
# results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# # Fit your custom regressor
# reg.fit(X_train, y_train)
# predictions = reg.predict(X_test)
# results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# # Display the results
# for model_name, metrics in results.items():
#     print(f"{model_name}: MAE = {metrics['MAE']}")
145/24:
X.shape
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
# from sklearn.svm import SVR
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.metrics import mean_absolute_error
# from sklearn.model_selection import train_test_split

# # Split your data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# from sklearn.preprocessing import PolynomialFeatures
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# # from sklearn.preprocessing import StandardScaler
# # ss=StandardScaler().fit(X_train)
# # X_train=ss.transform(X_train)
# # X_test=ss.transform(X_test)

# # List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# # Evaluate each model
# results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# # Fit your custom regressor
# reg.fit(X_train, y_train)
# predictions = reg.predict(X_test)
# results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# # Display the results
# for model_name, metrics in results.items():
#     print(f"{model_name}: MAE = {metrics['MAE']}")
145/25:
X=df.drop(['yield', 'id', 'Row#'], axis=1)
# X=df[['AverageRainingDays','fruitset','fruitmass','seeds']]
# X['bumbles'].value_counts()
# X['bumbles_is_38']=X['bumbles']==0.38
# X['bumbles_is_25']=X['bumbles']==0.25
# X['clonesize_is_12.5']=X['clonesize']==12.5
# X['clonesize_is_25']=X['clonesize']==25
145/26:
X.shape
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
# from sklearn.svm import SVR
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.metrics import mean_absolute_error
# from sklearn.model_selection import train_test_split

# # Split your data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# from sklearn.preprocessing import PolynomialFeatures
# X_train = PolynomialFeatures(degree=2).fit_transform(X_train)
# X_test = PolynomialFeatures(degree=2).fit_transform(X_test)
# # from sklearn.preprocessing import StandardScaler
# # ss=StandardScaler().fit(X_train)
# # X_train=ss.transform(X_train)
# # X_test=ss.transform(X_test)

# # List of models to evaluate
# models = {
#     'Linear Regression': LinearRegression(),
#     'Ridge Regression': Ridge(),
#     'Lasso Regression': Lasso(),
#     'ElasticNet Regression': ElasticNet(),
#     'Random Forest': RandomForestRegressor(),
#     'Gradient Boosting': GradientBoostingRegressor(),
#     'AdaBoost Regressor': AdaBoostRegressor(),
#     'Support Vector Regression': SVR(),
#     'Decision Tree Regressor': DecisionTreeRegressor(),
#     'K-Neighbors Regressor': KNeighborsRegressor()
# }

# # Evaluate each model
# results = {}
# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     mae = mean_absolute_error(y_test, predictions)
#     results[name] = {'MAE': mae}

# # Fit your custom regressor
# reg.fit(X_train, y_train)
# predictions = reg.predict(X_test)
# results['Custom Regressor'] = {'MAE': mean_absolute_error(y_test, predictions)}

# # Display the results
# for model_name, metrics in results.items():
#     print(f"{model_name}: MAE = {metrics['MAE']}")
146/1: df=pd.read_csv('train.csv')
146/2: X=df.drop(['yield', 'id', 'Row#'],axis=1)
146/3:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
146/4: df=pd.read_csv('train.csv')
146/5: X=df.drop(['yield', 'id', 'Row#'],axis=1)
146/6:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
146/7:
from matplotlib import pyplot as plt
import pickle
146/8:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
146/9:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
from matplotlib import pyplot as plt
import pickle
146/10:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
146/11: reg=HigherOrderRegressor(1)
146/12:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
import pickle
146/13: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
146/14: reg.fit(X_train, y_train)
146/15:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
import pickle
146/16:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values()
X_test=X_test.values()
y_train=y_train.values()
y_test=y_test.values()
146/17:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/18: reg.fit(X_train, y_train)
146/19: mean_absolute_error(y_test, reg.predict(X_test))
146/20:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error
import pickle
146/21: mean_absolute_error(y_test, reg.predict(X_test))
146/22:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
146/23: reg=HigherOrderRegressor(1)
146/24:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/25: reg.fit(X_train, y_train)
146/26: mean_absolute_error(y_test, reg.predict(X_test))
146/27:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
146/28: reg=HigherOrderRegressor(1)
146/29:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/30: reg.fit(X_train, y_train)
146/31: mean_absolute_error(y_test, reg.predict(X_test))
146/32:
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train=X_train[:,mask]
    X_test=X_test[:,mask]
146/33: df=pd.read_csv('train.csv')
146/34:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error
import pickle
146/35:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
146/36: reg=HigherOrderRegressor(1)
146/37:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/38: reg.fit(X_train, y_train)
146/39: mean_absolute_error(y_test, reg.predict(X_test))
146/40:
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train=X_train[:,mask]
    X_test=X_test[:,mask]
146/41:
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train.shape
    X_train=X_train[:,mask]
    X_test=X_test[:,mask]
146/42:
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    print(X_train.shape)
    X_train=X_train[:,mask]
    X_test=X_test[:,mask]
146/43:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/44:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
print(X_train.shape)
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/45:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
146/46:
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
146/47:
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    reg.fit(X_train2, y_train)
    print(mean_absolute_error(y_test, reg.predict(X_test2)))
146/48:
errors=[]
for i in range(1, 2**16):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
146/49: from tqdm import tqdm
146/50: from tqdm import tqdm
146/51: errors=np.array(errors)
146/52: print(min(errors))
146/53: print(np.min(errors))
146/54: print(np.argmin(errors))
146/55:
errors=[]
for i in tqdm(range(2**15+1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
147/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
147/2: df=pd.read_csv('train.csv')
147/3:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error
import pickle
147/4:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
147/5: reg=HigherOrderRegressor(1)
147/6:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
147/7: reg.fit(X_train, y_train)
147/8: mean_absolute_error(y_test, reg.predict(X_test))
147/9: from tqdm import tqdm
147/10:
errors=[]
for i in tqdm(range(1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=1).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=1).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
147/11: print(np.argmin(errors))
147/12: print(np.min(errors))
147/13:
errors=[]
for i in tqdm(range(1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
148/1: df=pd.read_csv('train.csv')
148/2:
import pandas as pd
df=pd.read_csv('train.csv')
148/3:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_error
import pickle
148/4:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
148/5: reg=HigherOrderRegressor(1)
148/6:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
148/7:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle
148/8:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
148/9: from tqdm import tqdm
148/10: from tqdm import tqdm
148/11:
errors=[]
for i in tqdm(range(1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
148/12:
from tqdm import tqdm
import numpy as np
148/13:
errors=[]
for i in tqdm(range(1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
147/14: print(np.min(errors))
147/15: print(np.argmin(errors))
147/16:
errors=[]
for i in tqdm(range(46390, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
147/17: print(np.min(errors))
147/18: print(np.argmin(errors))
147/19:
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/20:
mask = np.array(list(np.binary_repr(1, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/21:
mask = np.array(list(np.binary_repr(2**16-1, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/22:
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/23:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/24:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/25: test_data=pd.read_csv('test.csv')
147/26:
X_test_final=test_data.drop(['yield', 'id', 'Row#'],axis=1)
y_test_final=test_data['yield']
147/27:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
y_test_final=test_data['yield']
147/28: X_test_final=test_data.drop(['id', 'Row#'],axis=1)
147/29:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
147/30: test_data['yield']=reg.predict(X_test_final.values)
147/31: test_data['yield']=reg.predict(X_test_final)
147/32: test_data_new=test_data['id', 'yield']
147/33: test_data_new=test_data[['id', 'yield']]
147/34: test_data_new.to_csv('hello.csv',index=False)
147/35:
orig_preds=pd.read_csv('output.csv')['yield']
new_preds=pd.read_csv('hello.csv')['yield']
147/36: mean_absolute_error(orig_preds, new_preds)
147/37:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/38: test_data['yield']=reg.predict(X_test_final)
147/39: test_data_new=test_data[['id', 'yield']]
147/40: test_data_new.to_csv('hello.csv',index=False)
147/41: test_data['yield']=reg.predict(X_test_final[:,mask])
147/42:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final[:,mask])
147/43:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
147/44:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
147/45: X_train.shape
147/46: X_test_final.shape
147/47: X_test_final.shape
147/48: test_data.columns
147/49: test_data=pd.read_csv('test.csv')
147/50:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
147/51: test_data['yield']=reg.predict(X_test_final[:,mask])
147/52: test_data['yield']=reg.predict(X_test_final)
147/53: test_data_new=test_data[['id', 'yield']]
147/54: test_data_new.to_csv('hello.csv',index=False)
147/55: test_data_new.to_csv('hello.csv',index=False)
147/56:
orig_preds=pd.read_csv('output.csv')['yield']
new_preds=pd.read_csv('hello.csv')['yield']
147/57: mean_absolute_error(orig_preds, new_preds)
147/58:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=5).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=5).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/59:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/60:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
147/61:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
148/14: print(np.min(errors))
148/15: print(np.argmin(errors))
148/16:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
148/17:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
148/18:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=5).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=5).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
148/19:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
148/20: test_data=pd.read_csv('test.csv')
148/21:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
148/22:
X_trainfinal=PolynomialFeatures(degree=4).fit_transform(X)
y_trainfinal=y.values
148/23: reg.fit(X_trainfinal, y_trainfinal)
149/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
149/2:
import pandas as pd
df=pd.read_csv('train.csv')
149/3:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle
149/4:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
149/5: reg=HigherOrderRegressor(1)
149/6:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
149/7:
from tqdm import tqdm
import numpy as np
149/8: test_data=pd.read_csv('test.csv')
149/9:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/10:
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
149/11:
X_trainfinal=PolynomialFeatures(degree=4).fit_transform(X[:,mask])
y_trainfinal=y.values
149/12: print(X.shape)
149/13:
X_trainfinal=PolynomialFeatures(degree=4).fit_transform(X.values[:,mask])
y_trainfinal=y.values
149/14: reg.fit(X_trainfinal, y_trainfinal)
149/15: test_data['yield']=reg.predict(X_test_final)
149/16: test_data_new=test_data[['id', 'yield']]
149/17: test_data_new.to_csv('output.csv',index=False)
149/18:
orig_preds=pd.read_csv('hello.csv')['yield']
new_preds=pd.read_csv('output.csv')['yield']
149/19: mean_absolute_error(orig_preds, new_preds)
149/20:
mask = np.array(list(np.binary_repr(247, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/21:
mask = np.array(list(np.binary_repr(247, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=5).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=5).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/22:
mask = np.array(list(np.binary_repr(127, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=5).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=5).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/23:
mask = np.array(list(np.binary_repr(127, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/24:
mask = np.array(list(np.binary_repr(127, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/25:
mask = np.array(list(np.binary_repr(5536, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/26:
mask = np.array(list(np.binary_repr(46941, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/27:
mask = np.array(list(np.binary_repr(118, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/28:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
149/29:
from sklearn.linear_model import LassoCV
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg2=LassoCV()
reg2.fit(X_train2, y_train)
mean_absolute_error(y_test, reg2.predict(X_test2))
151/1: from sns import heatmap
151/2: from seaborn import heatmap
151/3: heatmap(test_data.corr())
151/4: heatmap(X_train.corr())
151/5: heatmap(X.corr())
151/6:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
151/7:
import pandas as pd
df=pd.read_csv('train.csv')
151/8:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle
151/9:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
151/10: reg=HigherOrderRegressor(1)
151/11:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
151/12:
from tqdm import tqdm
import numpy as np
151/13: from seaborn import heatmap
151/14: heatmap(X.corr())
151/15: heatmap(df.corr())
151/16:
errors=[]
for i in tqdm(range(1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    print(mask)
    break
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
151/17:
errors=[]
for i in tqdm(range(1, 2**12)):
    mask = np.array(list(np.binary_repr(16*i+15, width=16)), dtype=int).astype(bool)
    print(mask)
    break
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
151/18:
errors=[]
for i in tqdm(range(0, 2**12)):
    mask = np.array(list(np.binary_repr(16*i+15, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
151/19: print(np.min(errors))
151/20: print(np.argmin(errors))
151/21:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
151/22:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=5).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=5).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
151/23:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
151/24:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=1).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=1).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
151/25:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
151/26:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
151/27: heatmap(PolynomialFeatures(degree=2).fit(X).corr())
151/28: heatmap(PolynomialFeatures(degree=2).fit(X), yield)
151/29: heatmap(PolynomialFeatures(degree=2).fit(X))
151/30: from seaborn import heatmap, pairplot
151/31: from seaborn import heatmap, PairGrid
151/32: PairGrid(df).map(heatmap)
151/33: from sklearn.feature_selection import VarianceThreshold, SelectKBest, RFECV
151/34:
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Load the dataset (replace this with your own dataset)
data = load_boston()
X = data.data
y = data.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a base model (can be any estimator, here we use RandomForest)
estimator = LinearRegression(random_state=42)

# Create the RFECV object with cross-validation
rfecv = RFECV(
    estimator=estimator, 
    step=1,  # Number of features to remove at each iteration
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_squared_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# Plot the number of features vs. cross-validation score
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross-validation score (neg_mean_squared_error)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train)
X_test_transformed = rfecv.transform(X_test)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = model.score(X_test_transformed, y_test)
print("Test score with selected features:", test_score)
151/35:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# Plot the number of features vs. cross-validation score
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross-validation score (neg_mean_squared_error)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train)
X_test_transformed = rfecv.transform(X_test)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = model.score(X_test_transformed, y_test)
print("Test score with selected features:", test_score)
151/36:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train)
X_test_transformed = rfecv.transform(X_test)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = model.score(X_test_transformed, y_test)
print("Test score with selected features:", test_score)
151/37:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=2).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train)
X_test_transformed = rfecv.transform(X_test)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
151/38:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=4).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train)
X_test_transformed = rfecv.transform(X_test)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
151/39:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=4).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
151/40:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=4).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train2)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
151/41:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X_train2=PolynomialFeatures(degree=4).fit_transform(X_train)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
152/2:
import pandas as pd
df=pd.read_csv('train.csv')
152/3:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle
152/4:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
152/5: reg=HigherOrderRegressor(1)
152/6:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
152/7: from seaborn import heatmap
152/8:
from seaborn import heatmap
heatmap(pd.DataFrame(X_trainfinal).corr())
152/9:
from seaborn import heatmap
heatmap(df.corr())
152/10:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test)
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/11:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/12:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/13:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14407, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/14:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/15:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/16:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=5).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=5).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/17:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/18: X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[rfecv.support_]
152/19: X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
152/20:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/21:
test_data=pd.read_csv('test.csv')   
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)[:,rfecv.support_]
reg.fit(PolynomialFeatures(degree=4).fit_transform(X.values[:,mask])[:,rfecv.support_], y.values)
y_pred=reg.predict(PolynomialFeatures(degree=4).fit_transform(X_test_final))
152/22:
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/23:
orig_preds=pd.read_csv('output.csv')
print(mean_absolute_error(orig_preds['yield'], y_pred))
152/24:
test_data=pd.read_csv('test.csv')   
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
X_test_final=rfecv.transform(X_test_final)
reg.fit(PolynomialFeatures(degree=4).fit_transform(X.values[:,mask])[:,rfecv.support_], y.values)
y_pred=reg.predict(PolynomialFeatures(degree=4).fit_transform(X_test_final))
152/25:
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/26:
orig_preds=pd.read_csv('output.csv')
print(mean_absolute_error(orig_preds['yield'], y_pred))
152/27:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/28:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
# reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/29:
test_data=pd.read_csv('test.csv')   
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
X_test_final=rfecv.transform(X_test_final)
reg.fit(PolynomialFeatures(degree=4).fit_transform(X.values[:,mask])[:,rfecv.support_], y.values)
y_pred=reg.predict(PolynomialFeatures(degree=4).fit_transform(X_test_final))
152/30:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
# reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/31:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/32:
test_data=pd.read_csv('test.csv')   
X_test_final=test_data.drop(['id', 'Row#'],axis=1)
X_test_final=X_test_final.values[:,mask]
X_test_final=PolynomialFeatures(degree=4).fit_transform(X_test_final)
X_test_final=rfecv.transform(X_test_final)
reg.fit(PolynomialFeatures(degree=4).fit_transform(X.values[:,mask])[:,rfecv.support_], y.values)
y_pred=reg.predict(X_test_final)
152/33:
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/34:
orig_preds=pd.read_csv('output.csv')
print(mean_absolute_error(orig_preds['yield'], y_pred))
152/35:
orig_preds=pd.read_csv('output.csv')
print(mean_absolute_error(orig_preds['yield'], y_pred))
152/36:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/37:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/38:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
?rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/39:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
# rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/40:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
# rfecv.fit(X_train2, y_train)

# # Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
# print("Optimal number of features: %d" % rfecv.n_features_)

# # Get the selected features
# selected_features = rfecv.support_
# print("Selected features (True means feature is selected):", selected_features)

# # Transform the training data to only use the selected features
# X_train_transformed = rfecv.transform(X_train2)
# X_test_transformed = rfecv.transform(X_test2)

# # Evaluate the performance on the test set
# model = estimator.fit(X_train_transformed, y_train)
# test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
# print("Test score with selected features:", test_score)
152/41:
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])[:,rfecv.support_]
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])[:,rfecv.support_]
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/42:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross-validation score (neg_mean_squared_error)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/43:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross-validation score (neg_mean_squared_error)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/44:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

# rfecv = RFECV(
#     estimator=estimator, 
#     step=5,  
#     cv=5,    # Number of folds in cross-validation
#     scoring='neg_mean_absolute_error'  # Metric to evaluate the model
# )

# Fit RFECV to the training data
# rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/45:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(9055, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=5,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/46:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(9055, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=1,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/47:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=1,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
152/48:
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train[:,mask])
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test[:,mask])
estimator = LinearRegression()

rfecv = RFECV(
    estimator=estimator, 
    step=1,  
    cv=5,    # Number of folds in cross-validation
    scoring='neg_mean_absolute_error'  # Metric to evaluate the model
)

# Fit RFECV to the training data
rfecv.fit(X_train2, y_train)

# Plot the number of features vs. cross-validation score
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Cross-validation score (neg_mean_squared_error)")
# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
# plt.show()

# Print the optimal number of features
print("Optimal number of features: %d" % rfecv.n_features_)

# Get the selected features
selected_features = rfecv.support_
print("Selected features (True means feature is selected):", selected_features)

# Transform the training data to only use the selected features
X_train_transformed = rfecv.transform(X_train2)
X_test_transformed = rfecv.transform(X_test2)

# Evaluate the performance on the test set
model = estimator.fit(X_train_transformed, y_train)
test_score = mean_absolute_error(y_test, model.predict(X_test_transformed))
print("Test score with selected features:", test_score)
153/1:
import os
from matplotlib import pyplot as plt

values=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    
    values.append(f'0.{data[0].split(".")[1].split()[0]}')

plt.plot([2**(i+2) for i in range(8)], values)
plt.savefig('spice plot')
# os.system('../d4-7/dineroIV -l1-usize 4096 -l1-ubsize 16 -l1-uassoc 1 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din')
154/1:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        

plt.plot([2**(i+2) for i in range(8)], values_spice)
plt.plot([2**(i+2) for i in range(8)], values_cc1)
plt.plot([2**(i+2) for i in range(8)], values_tex)
plt.savefig('spice plot')
# os.system('../d4-7/dineroIV -l1-usize 4096 -l1-ubsize 16 -l1-uassoc 1 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din')
154/2:
print(values_spice)
print(values_cc1)
print(values_tex)
154/3:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        

plt.plot([2**(i+2) for i in range(8)], values_spice)
plt.plot([2**(i+2) for i in range(8)], values_cc1)
plt.plot([2**(i+2) for i in range(8)], values_tex)
plt.legend()
plt.savefig('spice plot')
154/4:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        

plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.savefig('spice plot')
154/5:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.savefig('spice plot')
154/6:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('spice plot')
155/1:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('spice plot')
155/2:
print(values_spice)
print(values_cc1)
print(values_tex)
155/3:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(f'0.{data[0].split(".")[1].split()[0]}')
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(f'0.{data[0].split(".")[1].split()[0]}')
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
155/4:
plt.grid()
plt.plot([i+2 for i in range(8)], values_spice, label='spice')
plt.plot([i+2 for i in range(8)], values_cc1, label='cc1')
plt.plot([i+2 for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
155/5:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
155/6:
plt.grid()
plt.plot([i+2 for i in range(8)], values_spice, label='spice')
plt.plot([i+2 for i in range(8)], values_cc1, label='cc1')
plt.plot([i+2 for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
155/7:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
plt.show()
155/8:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('cache-size-plot.png')
plt.show()
155/9:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()
155/10:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()
155/11:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()
155/12:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
# plt.savefig('cache-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
# plt.savefig('block-size-plot.png')
plt.show()
155/13:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Block size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Associativity')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('assoc-plot.png')
plt.show()
155/14:
import numpy as np
print(2**np.argmin(values_spice))
print(2**np.argmin(values_cc1))
print(2**np.argmin(values_tex))
155/15:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Block size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex')
plt.legend()
plt.title('Plot of miss rate vs Associativity')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('assoc-plot.png')
import numpy as np
print(f"Optimal associativity for spice: {2**np.argmin(values_spice)}")
print(f"Optimal associativity for cc1: {2**np.argmin(values_cc1)}")
print(f"Optimal associativity for tex: {2**np.argmin(values_tex)}")
plt.show()
155/16:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice', marker='o')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1', marker='o')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex', marker='o')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex', marker='o')
plt.legend()
plt.title('Plot of miss rate vs Block size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex', marker='o')
plt.legend()
plt.title('Plot of miss rate vs Associativity')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('assoc-plot.png')
import numpy as np
print(f"Optimal associativity for spice: {2**np.argmin(values_spice)}")
print(f"Optimal associativity for cc1: {2**np.argmin(values_cc1)}")
print(f"Optimal associativity for tex: {2**np.argmin(values_tex)}")
plt.show()
155/17:
import os
from matplotlib import pyplot as plt

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(8):
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize {2**(i+2)}k -l1-ubsize 16 -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(8)], values_spice, label='spice', marker='o')
plt.plot([2**(i+2) for i in range(8)], values_cc1, label='cc1', marker='o')
plt.plot([2**(i+2) for i in range(8)], values_tex, label='tex', marker='o')
plt.legend()
plt.title('Plot of miss rate vs Cache size')
plt.xlabel('Cache size in bytes')
plt.ylabel('Miss rate')
plt.savefig('cache-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize {4*(2**i)} -l1-uassoc 2 -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex', marker='o')
plt.legend()
plt.title('Plot of miss rate vs Block size')
plt.xlabel('Block size in bytes')
plt.ylabel('Demand Miss Rate')
plt.savefig('block-size-plot.png')
plt.show()

values_spice=[]
values_cc1=[]
values_tex=[]
for i in range(9):
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./spice.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_spice.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./cc1.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_cc1.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
    os.system(f'../d4-7/dineroIV -l1-usize 8k -l1-ubsize 16 -l1-uassoc {2**i} -l1-uwalloc a -l1-uwback a -informat d < ./tex.din > output.txt')
    with open("output.txt", "r") as file:
        data=[line for line in file.readlines() if "Demand miss rate" in line]
    values_tex.append(float(f'0.{data[0].split(".")[1].split()[0]}'))
        
plt.grid()
plt.plot([2**(i+2) for i in range(9)], values_spice, label='spice', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_cc1, label='cc1', marker='o')
plt.plot([2**(i+2) for i in range(9)], values_tex, label='tex', marker='o')
plt.legend()
plt.title('Plot of miss rate vs Associativity')
plt.xlabel('Associativity')
plt.ylabel('Miss rate')
plt.savefig('assoc-plot.png')
import numpy as np
print(f"Optimal associativity for spice: {2**np.argmin(values_spice)}")
print(f"Optimal associativity for cc1: {2**np.argmin(values_cc1)}")
print(f"Optimal associativity for tex: {2**np.argmin(values_tex)}")
plt.show()
152/49:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/50:
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/51:
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=2).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=2).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/52:
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/53:
errors=[]
for i in tqdm(range(0, 2**12)):
    mask = np.array(list(np.binary_repr(16*i+15, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
152/54:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
152/55:
import pandas as pd
df=pd.read_csv('train.csv')
152/56:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle
152/57:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
152/58: reg=HigherOrderRegressor(1)
152/59:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
152/60:
from tqdm import tqdm
import numpy as np
152/61:
errors=[]
for i in tqdm(range(0, 2**12)):
    mask = np.array(list(np.binary_repr(16*i+15, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
152/62: print(np.min(errors))
152/63: print(np.argmin(errors))
152/64:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/65:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
156/1:
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
156/2:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
156/3: train_data=pd.read_csv('train.csv')
156/4:
X=train_data['x'].to_numpy()
Y=train_data['y'].to_numpy()
156/5: regressor=HigherOrderRegressor(3)
156/6: regressor.fit(X,Y)
156/7: test_data=pd.read_csv('test.csv')
156/8: X_test=test_data['x'].to_numpy()
156/9: regressor2=HigherOrderRegressor(3)
156/10: X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
156/11:
import numpy as np
import pandas as pd
import pickle
from sklearn.linear_model import train_test_split
import matplotlib.pyplot as plt
156/12: X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
156/13: X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
156/14:
import numpy as np
import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
156/15: X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
156/16: regressor.fit(X_train,Y_train)
156/17: data=regressor.predict(X_val)
156/18: r2_score(Y_val, data)
156/19: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
156/20: r2_score(Y_val, data)
156/21: mean_squared_error(Y_val, data)
156/22: mean_absolute_error(Y_val, data)
156/23: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
156/24: regressor3=HigherOrderRegressor(5)
156/25: regressor3.fit(X,Y)
156/26: regressor3.write_params()
156/27:
test_data=pd.read_csv('test.csv')
X_test=test_data['x'].to_numpy()
test_data['y']=regressor3.predict(X_test)
156/28: reg.r2_score(Y, reg.predict(X))
156/29: reg=HigherOrderRegressor(5)
156/30: reg.fit(X, Y)
156/31: reg.r2_score(Y, reg.predict(X))
156/32:
plt.scatter(X, Y)
plt.plot(X, reg.predict(X), color='red')
156/33:
plt.scatter(X, Y)
plt.scatter(X, reg.predict(X), color='red')
156/34: np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
156/35: X.shape
156/36: reg.B.shape
156/37: regressor3.B.shape
156/38:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1,1)))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B.reshape((self.degree+1,)), f)
156/39: regressor3=HigherOrderRegressor(5)
156/40: regressor3.fit(X,Y)
156/41: regressor3.write_params()
156/42: regressor_3_params=pickle.load(open("3_weights.pkl", "rb"))
156/43: regressor_3_params.shape
156/44: mean_absolute_error(Y, reg.predict(X))
156/45: mean_absolute_error(Y, regressor3.predict(X))
156/46: mean_absolute_error(Y, reg.predict(X))
156/47: mean_absolute_error(Y, regressor2.predict(X))
156/48: mean_absolute_error(Y, regressor3.predict(X))
156/49: r2_score(Y, regressor3.predict(X))
156/50: r2_score(Y, reg.predict(X))
156/51: r2_score(Y, regressor3.predict(X))
156/52: r2_score(Y_val, data)
156/53: regressor2=HigherOrderRegressor(5)
156/54: regressor.fit(X_train,Y_train)
156/55: data=regressor.predict(X_val)
156/56: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
156/57: r2_score(Y_val, data)
156/58: regressor.fit(X,Y)
156/59: data=regressor.predict(X_val)
156/60: from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
156/61: r2_score(Y_val, data)
156/62: mean_squared_error(Y_val, data)
156/63: mean_absolute_error(Y_val, data)
156/64: regressor.cross_validation([2,3,4,5,6,7,8,9,10])
152/66:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/67:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y)
mean_absolute_error(y_test, reg.predict(X_test2))
152/68:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y)
mean_absolute_error(y_test, reg.predict(X_test2))
152/69:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
mean_absolute_error(y_test, reg.predict(X_test2))
152/70:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
mean_absolute_error(y.values, reg.predict(X_train_final_2))
152/71:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
X_test_final['yield']=y_pred
X_test_final.to_csv('hello.csv', index=False)
152/72:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#']).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
X_test_final['yield']=y_pred
X_test_final.to_csv('hello.csv', index=False)
152/73:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
X_test_final['yield']=y_pred
X_test_final.to_csv('hello.csv', index=False)
152/74:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
test_data['yield']=y_pred
test_data.to_csv('hello.csv', index=False)
152/75:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
test_data['yield']=y_pred
test_data['id', 'yield'].to_csv('hello.csv', index=False)
152/76:
mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/77:
orig_preds=pd.read_csv('output.csv')
new_preds=pd.read_csv('hello.csv')
mean_absolute_error(orig_preds['yield'], new_preds['yield'])
152/78: %history
152/79:
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/80:
mask = np.array(list(np.binary_repr(9055, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/81:
mask = np.array(list(np.binary_repr(5535, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/82:
mask = np.array(list(np.binary_repr(5534, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/83:
mask = np.array(list(np.binary_repr(14407, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/84:
mask = np.array(list(np.binary_repr(14343, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/85:
errors=[]
for i in tqdm(range(0, 2**12)):
    mask = np.array(list(np.binary_repr(16*i+7, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
152/86: print(np.min(errors))
152/87: print(np.argmin(errors))
152/88:
errors=[]
for i in tqdm(range(1, 2**16)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
152/89:
errors=[]
for i in tqdm(range(1, 2**12)):
    mask = np.array(list(np.binary_repr(16*i+14, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
157/1:
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as pt
import seaborn as sns
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
157/2:
import pandas as pd
df=pd.read_csv('train.csv')
157/3:
X=df.drop(['yield', 'id', 'Row#'],axis=1)
y=df['yield']
from matplotlib import pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle
157/4:
class HigherOrderRegressor():
    def __init__(self, degree):
        self.degree = degree
        self.X = None
        self.Y = None
        self.B = None
        self.num_features = None

    def fit(self, X, Y):
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        if len(Y.shape) == 1:
            Y = Y.reshape(-1, 1)
        self.num_features=X.shape[1]
        indices=np.random.shuffle(np.arange(X.shape[0]))
        temp_X = X[indices][0]
        temp_Y = Y[indices][0]
        # np.hstack((np.ones((self.X.shape[0], 1)), self.X))
        self.orig_X = np.hstack((np.ones((X.shape[0], 1)), temp_X))
        self.Y = temp_Y
        self.X = self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0])).T
        try:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))
        except:
            self.B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)),
                                np.matmul(self.X.T, self.Y))

    def recursive_generate_powers(self, max_feature_encountered, current_degree, current_product, use_orig=True):
        if current_degree == 0:
            return current_product
        result=[]
        for i in range(max_feature_encountered, self.num_features+1):
            if use_orig:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.orig_X[:,i]))
            else:
                result.append(self.recursive_generate_powers(i, current_degree-1,
                                                            current_product*self.test_X[:,i], False))
        result=np.vstack(result)
        return result
    
    def predict(self, X):
        if X.ndim == 1:
            X = X.reshape(-1,1)
        self.test_X=np.hstack((np.ones((X.shape[0], 1)), X))
        self.final_testX=self.recursive_generate_powers(0, self.degree, np.ones(X.shape[0]), False).T
        return np.matmul(self.final_testX, self.B)

    def sum_of_squares(self, Y_true, Y_pred):
        return np.sum((Y_true - Y_pred)**2)
        
    def r2_score(self, Y_true, Y_pred):
        return 1 - np.sum((Y_true - Y_pred)**2) / np.sum((Y_true - np.mean(Y_true))**2)

    def k_fold_testing(self, k=10):
        width = self.X.shape[0]//k
        average_r2 = 0
        count = 0
        for i in range(0, self.X.shape[0], k):
            count += 1
            X_train = np.concatenate((self.X[:i], self.X[i+width:]))
            Y_train = np.concatenate((self.Y[:i], self.Y[i+width:]))
            X_test = self.X[i:i+width]
            Y_test = self.Y[i:i+width]
            try:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            except:
                B = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, Y_train))
            predictions = np.matmul(X_test, B)
            average_r2 += self.r2_score(Y_test, predictions)
        # print(self.X.shape)
        try:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        except:
            B = np.matmul(np.linalg.pinv(np.matmul(self.X.T, self.X)), np.matmul(self.X.T, self.Y))
        return average_r2/count, np.matmul(self.X, B)
    
    def cross_validation(self, params):
        maximum_param=np.max(params)
        self.max_X=self.recursive_generate_powers(0, maximum_param, np.ones(self.X.shape[0])).T
        # print(self.max_X)
        responses = []
        factorials=[1]
        for i in range(1, maximum_param+self.num_features+1):
            factorials.append(factorials[-1]*i)
        for param in params:
            self.degree = param
            # print((factorials[param+self.num_features]//factorials[self.num_features])//factorials[param])
            self.X=self.max_X[:,:(factorials[param+self.num_features]//factorials[self.num_features])//factorials[param]]
            response, plot_values = self.k_fold_testing()
            responses.append(response)
            fig=plt.figure()
            plt.scatter(self.orig_X[:,1], self.Y)
            indices=np.argsort(self.orig_X[:,1])
            plt.plot(self.orig_X[:,1][indices], plot_values[indices], 'red')
            fig.savefig(f"degree_{param}.png")
        print(responses)
    
    def write_params(self):
        with open("3_weights.pkl", "wb") as f:
            pickle.dump(self.B, f)
157/5: reg=HigherOrderRegressor(1)
157/6:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train=X_train.values
X_test=X_test.values
y_train=y_train.values
y_test=y_test.values
157/7:
from tqdm import tqdm
import numpy as np
157/8:
errors=[]
for i in tqdm(range(1, 15000)):
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    errors.append(mean_absolute_error(y_test, reg.predict(X_test2)))
152/90: print(np.min(errors))
152/91:
mask = np.array(list(np.binary_repr(46940, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/92:
mask = np.array(list(np.binary_repr(46941, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/93:
mask = np.array(list(np.binary_repr(247, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/94:
mask = np.array(list(np.binary_repr(127, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/95:
mask = np.array(list(np.binary_repr(2613, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/96:
mask = np.array(list(np.binary_repr(9055, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/97:
mask = np.array(list(np.binary_repr(9056, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/98:
mask = np.array(list(np.binary_repr(9055, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/99: print(np.argmin(errors))
152/100: print(np.min(errors))
152/101: print(np.argmin(errors))
152/102: print(np.min(errors))
152/103: print(np.argmin(errors))
152/104:
mask = np.array(list(np.binary_repr(88559, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/105:
mask = np.array(list(np.binary_repr(247, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/106:
mask = np.array(list(np.binary_repr(127, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/107:
# mask = np.array(list(np.binary_repr(41823, width=16)), dtype=int).astype(bool)
# X_train_final=X.values[:,mask]
# X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
# reg.fit(X_train_final, y.values)
# test_data=pd.read_csv('test.csv')
# X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
# X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
# y_pred=reg.predict(X_test_final)
# test_data['yield']=y_pred
# test_data[['id', 'yield']].to_csv('hello.csv', index=False)
%history -g -f filename.py
152/108:
mask = np.array(list(np.binary_repr(46941, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
mean_absolute_error(y_test, reg.predict(X_test2))
152/109:
for i in [5535, 119, 247, 127, 5536, 46941, 118, 119, 2613, 9055, 9056, 46940, 14407, 14343, 41823]:
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    mean_absolute_error(y_test, reg.predict(X_test2))
152/110:
for i in [5535, 119, 247, 127, 5536, 46941, 118, 119, 2613, 9055, 9056, 46940, 14407, 14343, 41823]:
    mask = np.array(list(np.binary_repr(i, width=16)), dtype=int).astype(bool)
    X_train2=X_train[:,mask]
    X_test2=X_test[:,mask]
    X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
    X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
    reg.fit(X_train2, y_train)
    print(mean_absolute_error(y_test, reg.predict(X_test2)))
152/111:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=3).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=3).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/112:
orig_preds=pd.read_csv('output.csv')
new_preds=pd.read_csv('hello.csv')
mean_absolute_error(orig_preds['yield'], new_preds['yield'])
152/113:
orig_preds=pd.read_csv('output.csv')
new_preds=pd.read_csv('hello.csv')
mean_absolute_error(orig_preds['yield'], new_preds['yield'])
152/114:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=4).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=4).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/115:
orig_preds=pd.read_csv('output.csv')
new_preds=pd.read_csv('hello.csv')
mean_absolute_error(orig_preds['yield'], new_preds['yield'])
152/116:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=4).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=4).fit_transform(X_test2)
reg.fit(X_train2, y_train)
print(mean_absolute_error(y_test, reg.predict(X_test2)))
152/117:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train2=X_train[:,mask]
X_test2=X_test[:,mask]
X_train2=PolynomialFeatures(degree=3).fit_transform(X_train2)
X_test2=PolynomialFeatures(degree=3).fit_transform(X_test2)
reg.fit(X_train2, y_train)
print(mean_absolute_error(y_test, reg.predict(X_test2)))
152/118:
mask = np.array(list(np.binary_repr(119, width=16)), dtype=int).astype(bool)
X_train_final=X.values[:,mask]
X_train_final_2=PolynomialFeatures(degree=4).fit_transform(X_train_final)
reg.fit(X_train_final, y.values)
test_data=pd.read_csv('test.csv')
X_test_final=test_data.drop(['id', 'Row#'], axis=1).values[:,mask]
X_test_final_2=PolynomialFeatures(degree=4).fit_transform(X_test_final)
y_pred=reg.predict(X_test_final)
test_data['yield']=y_pred
test_data[['id', 'yield']].to_csv('hello.csv', index=False)
152/119:
orig_preds=pd.read_csv('output.csv')
new_preds=pd.read_csv('hello.csv')
mean_absolute_error(orig_preds['yield'], new_preds['yield'])
   1: %history -g -f second_file.py
